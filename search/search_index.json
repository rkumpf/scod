{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"KubeCon + CloudNativeCon North America 2020 Virtual The HPE primary storage team is participating at KubeCon November 17-20th 2020 Visit the primary storage KubeCon mini site for exclusive content! Attend the KubeCon tutorial: \" Introduction to Using the Container Storage Interface (CSI) Primitives \" presented by HPE. Find us in the #HPE channel on the official Slack community of CNCF . HPE Storage Container Orchestrator Documentation \u00b6 This is an umbrella documentation project for all Kubernetes and Docker integrations for HPE primary storage tailored for IT Ops, developers and partners. Including HPE 3PAR and Primera, HPE Cloud Volumes and HPE Nimble Storage. Use the navigation on the left-hand side to explore the different topics. Feel free to contribute to this project but please read the contributing guidelines . Use the navigation to the left. Not sure what you're looking for? \u2192 Get started ! Did you know? SCOD is \"docs\" in reverse?","title":"Home"},{"location":"index.html#hpe_storage_container_orchestrator_documentation","text":"This is an umbrella documentation project for all Kubernetes and Docker integrations for HPE primary storage tailored for IT Ops, developers and partners. Including HPE 3PAR and Primera, HPE Cloud Volumes and HPE Nimble Storage. Use the navigation on the left-hand side to explore the different topics. Feel free to contribute to this project but please read the contributing guidelines . Use the navigation to the left. Not sure what you're looking for? \u2192 Get started ! Did you know? SCOD is \"docs\" in reverse?","title":"HPE Storage Container Orchestrator Documentation"},{"location":"container_storage_provider/index.html","text":"Container Storage Providers \u00b6 HPE Nimble Storage HPE 3PAR and Primera","title":"Container Storage Providers"},{"location":"container_storage_provider/index.html#container_storage_providers","text":"HPE Nimble Storage HPE 3PAR and Primera","title":"Container Storage Providers"},{"location":"container_storage_provider/hpe_3par_primera/index.html","text":"Introduction \u00b6 The HPE 3PAR and Primera Container Storage Provider integrates as part of the HPE CSI Driver for Kubernetes . The CSP abstract the data management capabilities of the array for use by Kubernetes. Introduction Platform requirements SPOCK User role requirements StorageClass example StorageClass parameters Common parameters for provisioning Primera Data Reduction volumes Pod inline volume parameters (Local Ephemeral Volumes) Import volume Cloning parameters Snapshotting a volume Applying QOS Rules (qosName) Remote Copy with Peer Persistence synchronous replication Target Portal IPs VolumeSnapshotClass parameters Import Snapshot Support Note For help getting started with deploying the HPE CSI Driver using HPE Primera or 3PAR Storage, check out the tutorial over at HPE DEV . Platform requirements \u00b6 Always check the corresponding CSI driver version in compatibility and support and SPOCK for latest support matrix for the HPE 3PAR and Primera Container Storage Provider. CSI Linux OS OpenShift Kubernetes 3PAR and Primera OS v1.3.0 - CentOS: 7.7 - RHEL: 7.6, 7.7 / RHCOS OpenShift 4.2/4.3 with RHEL 7.6 or 7.7 or RHCOS as worker nodes K8s 1.16-1.18 - 3PAR OS: 3.3.1 (FC & iSCSI) - Primera OS: 4.0.0, 4.1.0 (FC only), 4.2.0 Important \u2022 Minimum 2 iSCSI IP ports should be in ready state \u2022 FC array should be in ready state and zoned with initiator hosts \u2022 FC supported only on Bare metal and Fabric SAN SPOCK \u00b6 Refer to Hewlett Packard Enterprise Single Point of Connectivity Knowledge (SPOCK) for HPE storage products for specific platform details (requires a HPE Passport account). 3PAR Primera Tip The documentation reflected here always corresponds to the latest supported version and may contain references to future features and capabilities. User role requirements \u00b6 The CSP requires access to a user with either edit or the super role. It's recommended to use the edit role for least privilege practices. StorageClass example \u00b6 A StorageClass is used to provision an HPE 3PAR or Primera Storage-backed persistent volume. Please see using the HPE CSI Driver for additional base StorageClass examples like CSI snapshots and clones. Here is an example of a StorageClass using the HPE 3PAR and Primera CSP. Please see common parameters for additional parameter options. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: 3par-sc provisioner: csi.hpe.com allowVolumeExpansion: true parameters: csi.storage.k8s.io/fstype: ext4 csi.storage.k8s.io/provisioner-secret-name: primera3par-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: primera3par-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: primera3par-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: primera3par-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/controller-expand-secret-name: primera3par-secret csi.storage.k8s.io/controller-expand-secret-namespace: kube-system cpg: FC_r6 provisioning_type: tpvv accessProtocol: iscsi StorageClass parameters \u00b6 All parameters enumerated reflects the current version and may contain unannounced features and capabilities. Common parameters for provisioning \u00b6 These parameters are used for volume provisioning and supported platforms. Parameter Option Description 3PAR Primera accessProtocol (required) fc The access protocol to use when accessing the persistent volume. X X iscsi The access protocol to use when accessing the persistent volume. Requires Primera OS 4.2+ X X cpg Text The name of existing CPG to be used for volume provisioning. If the cpg parameter is not specified, the CSP will automatically set cpg parameter based upon a CPG available to 3PAR or Primera array. X X snap_cpg Text The name of the snapshot CPG to be used for volume provisioning. Defaults to value of cpg if not specified. X X compression Boolean Indicates that the volume should be compressed. X provisioning_type tpvv Indicates Thin provisioned volume type. Default: tpvv X X full Indicates Full provisioned volume type. X dedup Indicates Thin Deduplication volume type. X reduce Indicates Thin Deduplication/Compression volume type. X importVol Text Name of the volume to import. X X importVolAsClone Text Name of the volume to clone and import. X X cloneOf Text Name of the PersistentVolumeClaim to clone. X X virtualCopyOf Text Name of the PersistentVolumeClaim to snapshot. X X qosName Text Name of the volume set which has QoS rules applied. X X remoteCopyGroup Text Name of a new or existing remote copy group on HPE Primera/3PAR array. X X replicationDevices Text Indicates name of custom resource of type hpereplicationdeviceinfos . X X iscsiPortalIps Text Comma separated list of HPE Primera/3PAR iSCSI port IPs. X X Important The HPE CSI Driver allows the PersistentVolumeClaim to override the StorageClass parameters by annotating the PersistentVolumeClaim . Please see Using PVC Overrides for more details. Primera Data Reduction volumes \u00b6 These parameters are used to create Primera Data Reduction (Thinly Provisioned with Deduplication/Compression enabled) volumes. Please see Common Parameters for more details. Parameter Option Description accessProtocol fc The access protocol to use when accessing the persistent volume. cpg Text The name of existing CPG to be used for volume provisioning. snap_cpg Text The name of the snapshot CPG to be used for volume provisioning. Defaults to value of cpg if not specified. provisioning_type reduce Required Example StorageClass for provisioning Primera Data Reduction volumes apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: primera-reduce-sc provisioner: csi.hpe.com allowVolumeExpansion: true parameters: csi.storage.k8s.io/fstype: ext4 csi.storage.k8s.io/provisioner-secret-name: primera3par-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: primera3par-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: primera3par-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: primera3par-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/controller-expand-secret-name: primera3par-secret csi.storage.k8s.io/controller-expand-secret-namespace: kube-system cpg: SSD_r6 provisioning_type: reduce accessProtocol: fc Pod inline volume parameters (Local Ephemeral Volumes) \u00b6 These parameters are applicable only for Pod inline volumes and to be specified within Pod spec. Parameter String Description csi.storage.k8s.io/ephemeral Boolean Indicates that the request is for ephemeral inline volume. This is a mandatory parameter and must be set to \"true\". inline-volume-secret-name Text A reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodePublishVolume call. inline-volume-secret-namespace Text The namespace of inline-volume-secret-name for ephemeral inline volume. size Text The size of ephemeral volume specified in MiB or GiB. If unspecified, a default value will be used. accessProtocol Text Storage access protocol to use, \"iscsi\" or \"fc\". Important All parameters are required for inline ephemeral volumes. Import volume \u00b6 During the import volume process, any legacy (non-container volumes) or existing docker/k8s volume defined in the ImportVol parameter, within a StorageClass , will be renamed to match the PersistentVolumeClaim that leverages the StorageClass . The new volumes will be exposed through the HPE CSI Driver and made available to the Kubernetes cluster. Note: All previous Access Control Records and Initiator Groups will be removed from the volume when it is imported. Parameter Option Description accessProtocol fc or iscsi The access protocol to use when accessing the persistent volume. importVol Text The name of the HPE Primera or 3PAR volume to import. Important \u2022 No other parameters are required in the StorageClass when importing a volume outside of those parameters listed in the table above. \u2022 Support for importVol is available from HPE CSI Driver 1.2.0. Cloning parameters \u00b6 Cloning supports two modes of cloning. Either use cloneOf and reference a PersistentVolumeClaim in the current namespace to clone or use importVolAsClone and reference a HPE Primera or 3PAR volume name to clone and import to Kubernetes. Parameter Option Description cloneOf Text The name of the PersistentVolumeClaim to be cloned. cloneOf and importVolAsClone are mutually exclusive. importVolAsClone Text The name of the HPE Primera or 3PAR volume to clone and import. importVolAsClone and cloneOf are mutually exclusive. accessProtocol fc or iscsi The access protocol to use when accessing the persistent volume. Important \u2022 No other parameters are required in the StorageClass while cloning outside of those parameters listed in the table above. \u2022 Cloning using above parameters is independent of snapshot CRD availability on Kubernetes and it can be performed on any supported Kubernetes version. \u2022 Support for importVolAsClone and cloneOf is available from HPE CSI Driver 1.3.0. Snapshotting a volume \u00b6 During snapshotting process, any existing PersistentVolumeClaim defined in the virtualCopyOf parameter within a StorageClass , will be snapped as PersistentVolumeClaim and exposed through the HPE CSI Driver and made available to the Kubernetes cluster. Parameter Option Description accessProtocol fc or iscsi The access protocol to use when accessing the persistent volume. virtualCopyOf Text The name of existing PersistentVolumeClaim to be snapped Important \u2022 No other parameters are required in the StorageClass when snapshotting a volume outside of those parameters listed in the table above. \u2022 Snapshotting using virtualCopyOf is independent of snapshot CRD availability on Kubernetes and it can be performed on any supported Kubernetes version. \u2022 Support for virtualCopyOf is available from HPE CSI Driver 1.3.0. Applying QOS Rules (qosName) \u00b6 In the HPE Primera or 3PAR Storage system, the QoS rules are applied to a volume set. To apply QoS rules to a PersistentVolumeClaim , create a volume set in the HPE Primera or 3PAR, apply QoS rules to the volume set and set the qosName parameter within a StorageClass to the name of the HPE Primera or 3PAR volume set. Parameter Option Description qosName Text Name of the HPE Primera or 3PAR volume set which has QoS rules. This parameter is optional. If specified, the PersistentVolumeClaim will be associated with the HPE Primera or 3PAR volume set, for purposes of applying the QoS rules. Remote Copy with Peer Persistence synchronous replication \u00b6 To enable replication within the HPE CSI Driver, the following steps must be completed: Create Secrets for both primary and target HPE Primera or 3PAR arrays. Refer to Adding additional backends . Create replication custom resource. Create replication enabled StorageClass . For a tutorial on how to enable replication, check out the blog Enabling Remote Copy using the HPE CSI Driver for Kubernetes on HPE Primera A Custom Resource Definition (CRD) of type hpereplicationdeviceinfos.storage.hpe.com must be created to define the target array information. The CRD object name will be used to define the StorageClass parameter replicationDevices . apiVersion: storage.hpe.com/v1 kind: HPEReplicationDeviceInfo metadata: name: r1 spec: target_array_details: - targetCpg: <cpg_name> targetSnapCpg: <snapcpg_name> #optional targetName: <target_array_name> targetSecret: <target_secret_name> targetSecretNamespace: kube-system Important \u2022 targetCpg, targetName, targetSecret and targetSecretNamespace are mandatory for HPEReplicationDeviceInfo CRD. \u2022 Currently, the HPE CSI Driver only supports Remote Copy Peer Persistence mode. Async support will be added in a future release. These parameters are applicable only for replication. Both parameters are mandatory. If the remote copy volume group (RCG) name, as defined within the StorageClass , does not exist on the HPE Primera or 3PAR array, then a new RCG will be created. Parameter Option Description remoteCopyGroup Text Name of new or existing remote copy group on the HPE Primera/3PAR array. replicationDevices Text Indicates name of hpereplicationdeviceinfos Custom Resource Definition (CRD). Target Portal IPs \u00b6 This parameter allows the ability to specify a subset of HPE Primera/3PAR iSCSI ports for iSCSI sessions. By default, the HPE CSI Driver uses all available iSCSI ports. Parameter Option Description iscsiPortalIps Text Comma separated list of target portal IPs. VolumeSnapshotClass parameters \u00b6 These parameters are for VolumeSnapshotClass objects when using CSI snapshots. The external snapshotter needs to be deployed on the Kubernetes cluster and is usually performed by the Kubernetes vendor. Check enabling CSI snapshots for more information. How to use VolumeSnapshotClass and VolumeSnapshot objects is elaborated on in using CSI snapshots . Parameter String Description read_only Boolean Indicates if the snapshot is writable on the 3PAR or Primera array. Import Snapshot \u00b6 During the import snapshot process, any legacy (non-container snapshot) or an existing docker/k8s snapshot defined in the ImportVol parameter, within a VolumeSnapshotClass , will be renamed with the prefix \"snapshot-\". The new snapshot will be exposed through the HPE CSI Driver and made available to the Kubernetes cluster. Note: All previous Access Control Records and Initiator Groups will be removed from the snapshot when it is imported. Parameter Option Description importVol Text The name of the 3PAR or Primera snapshot to import. Support \u00b6 Please refer to the HPE 3PAR and Primera Container Storage Provider support statement .","title":"HPE 3PAR and Primera"},{"location":"container_storage_provider/hpe_3par_primera/index.html#introduction","text":"The HPE 3PAR and Primera Container Storage Provider integrates as part of the HPE CSI Driver for Kubernetes . The CSP abstract the data management capabilities of the array for use by Kubernetes. Introduction Platform requirements SPOCK User role requirements StorageClass example StorageClass parameters Common parameters for provisioning Primera Data Reduction volumes Pod inline volume parameters (Local Ephemeral Volumes) Import volume Cloning parameters Snapshotting a volume Applying QOS Rules (qosName) Remote Copy with Peer Persistence synchronous replication Target Portal IPs VolumeSnapshotClass parameters Import Snapshot Support Note For help getting started with deploying the HPE CSI Driver using HPE Primera or 3PAR Storage, check out the tutorial over at HPE DEV .","title":"Introduction"},{"location":"container_storage_provider/hpe_3par_primera/index.html#platform_requirements","text":"Always check the corresponding CSI driver version in compatibility and support and SPOCK for latest support matrix for the HPE 3PAR and Primera Container Storage Provider. CSI Linux OS OpenShift Kubernetes 3PAR and Primera OS v1.3.0 - CentOS: 7.7 - RHEL: 7.6, 7.7 / RHCOS OpenShift 4.2/4.3 with RHEL 7.6 or 7.7 or RHCOS as worker nodes K8s 1.16-1.18 - 3PAR OS: 3.3.1 (FC & iSCSI) - Primera OS: 4.0.0, 4.1.0 (FC only), 4.2.0 Important \u2022 Minimum 2 iSCSI IP ports should be in ready state \u2022 FC array should be in ready state and zoned with initiator hosts \u2022 FC supported only on Bare metal and Fabric SAN","title":"Platform requirements"},{"location":"container_storage_provider/hpe_3par_primera/index.html#spock","text":"Refer to Hewlett Packard Enterprise Single Point of Connectivity Knowledge (SPOCK) for HPE storage products for specific platform details (requires a HPE Passport account). 3PAR Primera Tip The documentation reflected here always corresponds to the latest supported version and may contain references to future features and capabilities.","title":"SPOCK"},{"location":"container_storage_provider/hpe_3par_primera/index.html#user_role_requirements","text":"The CSP requires access to a user with either edit or the super role. It's recommended to use the edit role for least privilege practices.","title":"User role requirements"},{"location":"container_storage_provider/hpe_3par_primera/index.html#storageclass_example","text":"A StorageClass is used to provision an HPE 3PAR or Primera Storage-backed persistent volume. Please see using the HPE CSI Driver for additional base StorageClass examples like CSI snapshots and clones. Here is an example of a StorageClass using the HPE 3PAR and Primera CSP. Please see common parameters for additional parameter options. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: 3par-sc provisioner: csi.hpe.com allowVolumeExpansion: true parameters: csi.storage.k8s.io/fstype: ext4 csi.storage.k8s.io/provisioner-secret-name: primera3par-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: primera3par-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: primera3par-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: primera3par-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/controller-expand-secret-name: primera3par-secret csi.storage.k8s.io/controller-expand-secret-namespace: kube-system cpg: FC_r6 provisioning_type: tpvv accessProtocol: iscsi","title":"StorageClass example"},{"location":"container_storage_provider/hpe_3par_primera/index.html#storageclass_parameters","text":"All parameters enumerated reflects the current version and may contain unannounced features and capabilities.","title":"StorageClass parameters"},{"location":"container_storage_provider/hpe_3par_primera/index.html#common_parameters_for_provisioning","text":"These parameters are used for volume provisioning and supported platforms. Parameter Option Description 3PAR Primera accessProtocol (required) fc The access protocol to use when accessing the persistent volume. X X iscsi The access protocol to use when accessing the persistent volume. Requires Primera OS 4.2+ X X cpg Text The name of existing CPG to be used for volume provisioning. If the cpg parameter is not specified, the CSP will automatically set cpg parameter based upon a CPG available to 3PAR or Primera array. X X snap_cpg Text The name of the snapshot CPG to be used for volume provisioning. Defaults to value of cpg if not specified. X X compression Boolean Indicates that the volume should be compressed. X provisioning_type tpvv Indicates Thin provisioned volume type. Default: tpvv X X full Indicates Full provisioned volume type. X dedup Indicates Thin Deduplication volume type. X reduce Indicates Thin Deduplication/Compression volume type. X importVol Text Name of the volume to import. X X importVolAsClone Text Name of the volume to clone and import. X X cloneOf Text Name of the PersistentVolumeClaim to clone. X X virtualCopyOf Text Name of the PersistentVolumeClaim to snapshot. X X qosName Text Name of the volume set which has QoS rules applied. X X remoteCopyGroup Text Name of a new or existing remote copy group on HPE Primera/3PAR array. X X replicationDevices Text Indicates name of custom resource of type hpereplicationdeviceinfos . X X iscsiPortalIps Text Comma separated list of HPE Primera/3PAR iSCSI port IPs. X X Important The HPE CSI Driver allows the PersistentVolumeClaim to override the StorageClass parameters by annotating the PersistentVolumeClaim . Please see Using PVC Overrides for more details.","title":"Common parameters for provisioning"},{"location":"container_storage_provider/hpe_3par_primera/index.html#primera_data_reduction_volumes","text":"These parameters are used to create Primera Data Reduction (Thinly Provisioned with Deduplication/Compression enabled) volumes. Please see Common Parameters for more details. Parameter Option Description accessProtocol fc The access protocol to use when accessing the persistent volume. cpg Text The name of existing CPG to be used for volume provisioning. snap_cpg Text The name of the snapshot CPG to be used for volume provisioning. Defaults to value of cpg if not specified. provisioning_type reduce Required Example StorageClass for provisioning Primera Data Reduction volumes apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: primera-reduce-sc provisioner: csi.hpe.com allowVolumeExpansion: true parameters: csi.storage.k8s.io/fstype: ext4 csi.storage.k8s.io/provisioner-secret-name: primera3par-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: primera3par-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: primera3par-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: primera3par-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/controller-expand-secret-name: primera3par-secret csi.storage.k8s.io/controller-expand-secret-namespace: kube-system cpg: SSD_r6 provisioning_type: reduce accessProtocol: fc","title":"Primera Data Reduction volumes"},{"location":"container_storage_provider/hpe_3par_primera/index.html#pod_inline_volume_parameters_local_ephemeral_volumes","text":"These parameters are applicable only for Pod inline volumes and to be specified within Pod spec. Parameter String Description csi.storage.k8s.io/ephemeral Boolean Indicates that the request is for ephemeral inline volume. This is a mandatory parameter and must be set to \"true\". inline-volume-secret-name Text A reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodePublishVolume call. inline-volume-secret-namespace Text The namespace of inline-volume-secret-name for ephemeral inline volume. size Text The size of ephemeral volume specified in MiB or GiB. If unspecified, a default value will be used. accessProtocol Text Storage access protocol to use, \"iscsi\" or \"fc\". Important All parameters are required for inline ephemeral volumes.","title":"Pod inline volume parameters (Local Ephemeral Volumes)"},{"location":"container_storage_provider/hpe_3par_primera/index.html#import_volume","text":"During the import volume process, any legacy (non-container volumes) or existing docker/k8s volume defined in the ImportVol parameter, within a StorageClass , will be renamed to match the PersistentVolumeClaim that leverages the StorageClass . The new volumes will be exposed through the HPE CSI Driver and made available to the Kubernetes cluster. Note: All previous Access Control Records and Initiator Groups will be removed from the volume when it is imported. Parameter Option Description accessProtocol fc or iscsi The access protocol to use when accessing the persistent volume. importVol Text The name of the HPE Primera or 3PAR volume to import. Important \u2022 No other parameters are required in the StorageClass when importing a volume outside of those parameters listed in the table above. \u2022 Support for importVol is available from HPE CSI Driver 1.2.0.","title":"Import volume"},{"location":"container_storage_provider/hpe_3par_primera/index.html#cloning_parameters","text":"Cloning supports two modes of cloning. Either use cloneOf and reference a PersistentVolumeClaim in the current namespace to clone or use importVolAsClone and reference a HPE Primera or 3PAR volume name to clone and import to Kubernetes. Parameter Option Description cloneOf Text The name of the PersistentVolumeClaim to be cloned. cloneOf and importVolAsClone are mutually exclusive. importVolAsClone Text The name of the HPE Primera or 3PAR volume to clone and import. importVolAsClone and cloneOf are mutually exclusive. accessProtocol fc or iscsi The access protocol to use when accessing the persistent volume. Important \u2022 No other parameters are required in the StorageClass while cloning outside of those parameters listed in the table above. \u2022 Cloning using above parameters is independent of snapshot CRD availability on Kubernetes and it can be performed on any supported Kubernetes version. \u2022 Support for importVolAsClone and cloneOf is available from HPE CSI Driver 1.3.0.","title":"Cloning parameters"},{"location":"container_storage_provider/hpe_3par_primera/index.html#snapshotting_a_volume","text":"During snapshotting process, any existing PersistentVolumeClaim defined in the virtualCopyOf parameter within a StorageClass , will be snapped as PersistentVolumeClaim and exposed through the HPE CSI Driver and made available to the Kubernetes cluster. Parameter Option Description accessProtocol fc or iscsi The access protocol to use when accessing the persistent volume. virtualCopyOf Text The name of existing PersistentVolumeClaim to be snapped Important \u2022 No other parameters are required in the StorageClass when snapshotting a volume outside of those parameters listed in the table above. \u2022 Snapshotting using virtualCopyOf is independent of snapshot CRD availability on Kubernetes and it can be performed on any supported Kubernetes version. \u2022 Support for virtualCopyOf is available from HPE CSI Driver 1.3.0.","title":"Snapshotting a volume"},{"location":"container_storage_provider/hpe_3par_primera/index.html#applying_qos_rules_qosname","text":"In the HPE Primera or 3PAR Storage system, the QoS rules are applied to a volume set. To apply QoS rules to a PersistentVolumeClaim , create a volume set in the HPE Primera or 3PAR, apply QoS rules to the volume set and set the qosName parameter within a StorageClass to the name of the HPE Primera or 3PAR volume set. Parameter Option Description qosName Text Name of the HPE Primera or 3PAR volume set which has QoS rules. This parameter is optional. If specified, the PersistentVolumeClaim will be associated with the HPE Primera or 3PAR volume set, for purposes of applying the QoS rules.","title":"Applying QOS Rules (qosName)"},{"location":"container_storage_provider/hpe_3par_primera/index.html#remote_copy_with_peer_persistence_synchronous_replication","text":"To enable replication within the HPE CSI Driver, the following steps must be completed: Create Secrets for both primary and target HPE Primera or 3PAR arrays. Refer to Adding additional backends . Create replication custom resource. Create replication enabled StorageClass . For a tutorial on how to enable replication, check out the blog Enabling Remote Copy using the HPE CSI Driver for Kubernetes on HPE Primera A Custom Resource Definition (CRD) of type hpereplicationdeviceinfos.storage.hpe.com must be created to define the target array information. The CRD object name will be used to define the StorageClass parameter replicationDevices . apiVersion: storage.hpe.com/v1 kind: HPEReplicationDeviceInfo metadata: name: r1 spec: target_array_details: - targetCpg: <cpg_name> targetSnapCpg: <snapcpg_name> #optional targetName: <target_array_name> targetSecret: <target_secret_name> targetSecretNamespace: kube-system Important \u2022 targetCpg, targetName, targetSecret and targetSecretNamespace are mandatory for HPEReplicationDeviceInfo CRD. \u2022 Currently, the HPE CSI Driver only supports Remote Copy Peer Persistence mode. Async support will be added in a future release. These parameters are applicable only for replication. Both parameters are mandatory. If the remote copy volume group (RCG) name, as defined within the StorageClass , does not exist on the HPE Primera or 3PAR array, then a new RCG will be created. Parameter Option Description remoteCopyGroup Text Name of new or existing remote copy group on the HPE Primera/3PAR array. replicationDevices Text Indicates name of hpereplicationdeviceinfos Custom Resource Definition (CRD).","title":"Remote Copy with Peer Persistence synchronous replication"},{"location":"container_storage_provider/hpe_3par_primera/index.html#target_portal_ips","text":"This parameter allows the ability to specify a subset of HPE Primera/3PAR iSCSI ports for iSCSI sessions. By default, the HPE CSI Driver uses all available iSCSI ports. Parameter Option Description iscsiPortalIps Text Comma separated list of target portal IPs.","title":"Target Portal IPs"},{"location":"container_storage_provider/hpe_3par_primera/index.html#volumesnapshotclass_parameters","text":"These parameters are for VolumeSnapshotClass objects when using CSI snapshots. The external snapshotter needs to be deployed on the Kubernetes cluster and is usually performed by the Kubernetes vendor. Check enabling CSI snapshots for more information. How to use VolumeSnapshotClass and VolumeSnapshot objects is elaborated on in using CSI snapshots . Parameter String Description read_only Boolean Indicates if the snapshot is writable on the 3PAR or Primera array.","title":"VolumeSnapshotClass parameters"},{"location":"container_storage_provider/hpe_3par_primera/index.html#import_snapshot","text":"During the import snapshot process, any legacy (non-container snapshot) or an existing docker/k8s snapshot defined in the ImportVol parameter, within a VolumeSnapshotClass , will be renamed with the prefix \"snapshot-\". The new snapshot will be exposed through the HPE CSI Driver and made available to the Kubernetes cluster. Note: All previous Access Control Records and Initiator Groups will be removed from the snapshot when it is imported. Parameter Option Description importVol Text The name of the 3PAR or Primera snapshot to import.","title":"Import Snapshot"},{"location":"container_storage_provider/hpe_3par_primera/index.html#support","text":"Please refer to the HPE 3PAR and Primera Container Storage Provider support statement .","title":"Support"},{"location":"container_storage_provider/hpe_nimble_storage/index.html","text":"Introduction \u00b6 The HPE Nimble Storage CSP is the reference implementation for the HPE CSI Driver for Kubernetes. The CSP abstracts the data management capabilities of the array for use by Kubernetes. The documentation found herein is mainly geared towards day two operations and reference documentation for the StorageClass and VolumeSnapshotClass parameters but also contains important Nimble array setup requirements. Important For a successful deployment, it's important to understand the Nimble platform requirements found within the CSI driver (worker host OS and Kubernetes versions) and the CSP. Introduction Platform requirements Setting up the Nimble array Single tenant deployment StorageClass parameters Common parameters for provisioning and cloning Provisioning parameters Pod inline volume parameters (Local Ephemeral Volumes) Cloning parameters Import parameters VolumeSnapshotClass parameters Seealso There's a brief introduction on how to use HPE Nimble Storage with the HPE CSI Driver in the Video Gallery. Platform requirements \u00b6 Always check the corresponding CSI driver version in compatibility and support for the required NimbleOS version for a particular release of the driver. If a certain feature is gated against a certain version of NimbleOS it will be called out where applicable. Tip The documentation reflected here always corresponds to the latest supported version and may contain references to future features and capabilities. Setting up the Nimble array \u00b6 How to deploy an HPE Nimble Storage array is beyond the scope of this document. Please refer to HPE InfoSight for further reading. Single tenant deployment \u00b6 The CSP requires access to a user with either poweruser or the administrator role. It's recommended to use the poweruser role for least privilege practices. StorageClass parameters \u00b6 A StorageClass is used to provision or clone an HPE Nimble Storage-backed persistent volume. It can also be used to import an existing HPE Nimble Storage volume or clone of a snapshot into the Kubernetes cluster. The parameters are grouped below by those same workflows. Common parameters for provisioning and cloning Provisioning parameters Pod inline volume parameters (Local Ephemeral Volumes) Cloning parameters Import parameters VolumeSnapshotClass parameters Backward compatibility with the HPE Nimble Storage FlexVolume driver is being honored to a certain degree. StorageClass API objects needs be rewritten and parameters need to be updated regardless. Please see using the HPE CSI Driver for base StorageClass examples. All parameters enumerated reflects the current version and may contain unannounced features and capabilities. Note These are optional parameters unless specified. Common parameters for provisioning and cloning \u00b6 These parameters are mutable between a parent volume and creating a clone from a snapshot. Parameter String Description accessProtocol 1 Text The access protocol to use when accessing the persistent volume (\"fc\" or \"iscsi\"). Defaults to \"iscsi\" when unspecified. destroyOnDelete Boolean Indicates the backing Nimble volume (including snapshots) should be destroyed when the PVC is deleted. limitIops Integer The IOPS limit of the volume. The IOPS limit should be in the range 256 to 4294967294, or -1 for unlimited (default). limitMbps Integer The MB/s throughput limit for the volume. description Text Text to be added to the volume's description on the Nimble array. performancePolicy 2 Text The name of the performance policy to assign to the volume. Default example performance policies include \"Backup Repository\", \"Exchange 2003 data store\", \"Exchange 2007 data store\", \"Exchange 2010 data store\", \"Exchange log\", \"Oracle OLTP\", \"Other Workloads\", \"SharePoint\", \"SQL Server\", \"SQL Server 2012\", \"SQL Server Logs\". protectionTemplate 1 Text The name of the protection template to assign to the volume. Default examples of protection templates include \"Retain-30Daily\", \"Retain-48Hourly-30aily-52Weekly\", and \"Retain-90Daily\". folder Text The name of the Nimble folder in which to place the volume. thick Boolean Indicates that the volume should be thick provisioned. dedupeEnabled 3 Boolean Indicates that the volume should enable deduplication. syncOnDetach Boolean Indicates that a snapshot of the volume should be synced to the replication partner each time it is detached from a node. Restrictions applicable when using the CSI volume mutator : 1 = Parameter is immutable and can't be altered after provisioning/cloning. 2 = Performance policies may only be mutated between performance polices with the same block size. 3 = Deduplication may only be mutated within the same performance policy application category and block size. Note Performance Policies, Folders and Protection Templates are Nimble specific constructs that can be created on the Nimble array itself to address particular requirements or workloads. Please consult with the storage admin or read the admin guide found on HPE InfoSight . Provisioning parameters \u00b6 These parameters are immutable for both volumes and clones once created, clones will inherit parent attributes. Parameter String Description encrypted Boolean Indicates that the volume should be encrypted. pool Text The name of the pool in which to place the volume. Pod inline volume parameters (Local Ephemeral Volumes) \u00b6 These parameters are applicable only for Pod inline volumes and to be specified within Pod spec. Parameter String Description csi.storage.k8s.io/ephemeral Boolean Indicates that the request is for ephemeral inline volume. This is a mandatory parameter and must be set to \"true\". inline-volume-secret-name Text A reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodePublishVolume call. inline-volume-secret-namespace Text The namespace of inline-volume-secret-name for ephemeral inline volume. size Text The size of ephemeral volume specified in MiB or GiB. If unspecified, a default value will be used. accessProtocol Text Storage access protocol to use, \"iscsi\" or \"fc\". Important All parameters are required for inline ephemeral volumes. Cloning parameters \u00b6 Cloning supports two modes of cloning. Either use cloneOf and reference a PVC in the current namespace or use importVolAsClone and reference a Nimble volume name to clone and import to Kubernetes. Parameter String Description cloneOf Text The name of the PV to be cloned. cloneOf and importVolAsClone are mutually exclusive. importVolAsClone Text The name of the Nimble volume to clone and import. importVolAsClone and cloneOf are mutually exclusive. snapshot Text The name of the snapshot to base the clone on. This is optional. If not specified, a new snapshot is created. createSnapshot Boolean Indicates that a new snapshot of the volume should be taken matching the name provided in the snapshot parameter. If the snapshot parameter is not specified, a default name will be created. Import parameters \u00b6 Importing volumes to Kubernetes requires the source Nimble volume to be offline. In case of reverse replication, the upstream volume should be in offline state. All previous Access Control Records and Initiator Groups will be stripped from the volume when put under control of the HPE CSI Driver. Parameter String Description importVolumeName Text The name of the Nimble volume to import. snapshot Text The name of the Nimble snapshot to restore the imported volume to after takeover. If not specified, the volume will not be restored. takeover Boolean Indicates the current group will takeover ownership of the Nimble volume and volume collection. This should be performed against a downstream replica. reverseReplication Boolean Reverses the replication direction so that writes to the Nimble volume are replicated back to the group where it was replicated from. forceImport Boolean Forces the import of a volume that is not owned by the group and is not part of a volume collection. If the volume is part of a volume collection, use takeover instead. VolumeSnapshotClass parameters \u00b6 These parametes are for VolumeSnapshotClass objects when using CSI snapshots. The external snapshotter needs to be deployed on the Kubernetes cluster and is usually performed by the Kubernetes vendor. Check enabling CSI snapshots for more information. How to use VolumeSnapshotClass and VolumeSnapshot objects is elaborated on in using CSI snapshots . Parameter String Description description Text Text to be added to the snapshot's description on the Nimble array. writable Boolean Indicates if the snapshot is writable on the Nimble array. online Boolean Indicates if the snapshot is set to online on the Nimble array.","title":"HPE Nimble Storage"},{"location":"container_storage_provider/hpe_nimble_storage/index.html#introduction","text":"The HPE Nimble Storage CSP is the reference implementation for the HPE CSI Driver for Kubernetes. The CSP abstracts the data management capabilities of the array for use by Kubernetes. The documentation found herein is mainly geared towards day two operations and reference documentation for the StorageClass and VolumeSnapshotClass parameters but also contains important Nimble array setup requirements. Important For a successful deployment, it's important to understand the Nimble platform requirements found within the CSI driver (worker host OS and Kubernetes versions) and the CSP. Introduction Platform requirements Setting up the Nimble array Single tenant deployment StorageClass parameters Common parameters for provisioning and cloning Provisioning parameters Pod inline volume parameters (Local Ephemeral Volumes) Cloning parameters Import parameters VolumeSnapshotClass parameters Seealso There's a brief introduction on how to use HPE Nimble Storage with the HPE CSI Driver in the Video Gallery.","title":"Introduction"},{"location":"container_storage_provider/hpe_nimble_storage/index.html#platform_requirements","text":"Always check the corresponding CSI driver version in compatibility and support for the required NimbleOS version for a particular release of the driver. If a certain feature is gated against a certain version of NimbleOS it will be called out where applicable. Tip The documentation reflected here always corresponds to the latest supported version and may contain references to future features and capabilities.","title":"Platform requirements"},{"location":"container_storage_provider/hpe_nimble_storage/index.html#setting_up_the_nimble_array","text":"How to deploy an HPE Nimble Storage array is beyond the scope of this document. Please refer to HPE InfoSight for further reading.","title":"Setting up the Nimble array"},{"location":"container_storage_provider/hpe_nimble_storage/index.html#single_tenant_deployment","text":"The CSP requires access to a user with either poweruser or the administrator role. It's recommended to use the poweruser role for least privilege practices.","title":"Single tenant deployment"},{"location":"container_storage_provider/hpe_nimble_storage/index.html#storageclass_parameters","text":"A StorageClass is used to provision or clone an HPE Nimble Storage-backed persistent volume. It can also be used to import an existing HPE Nimble Storage volume or clone of a snapshot into the Kubernetes cluster. The parameters are grouped below by those same workflows. Common parameters for provisioning and cloning Provisioning parameters Pod inline volume parameters (Local Ephemeral Volumes) Cloning parameters Import parameters VolumeSnapshotClass parameters Backward compatibility with the HPE Nimble Storage FlexVolume driver is being honored to a certain degree. StorageClass API objects needs be rewritten and parameters need to be updated regardless. Please see using the HPE CSI Driver for base StorageClass examples. All parameters enumerated reflects the current version and may contain unannounced features and capabilities. Note These are optional parameters unless specified.","title":"StorageClass parameters"},{"location":"container_storage_provider/hpe_nimble_storage/index.html#common_parameters_for_provisioning_and_cloning","text":"These parameters are mutable between a parent volume and creating a clone from a snapshot. Parameter String Description accessProtocol 1 Text The access protocol to use when accessing the persistent volume (\"fc\" or \"iscsi\"). Defaults to \"iscsi\" when unspecified. destroyOnDelete Boolean Indicates the backing Nimble volume (including snapshots) should be destroyed when the PVC is deleted. limitIops Integer The IOPS limit of the volume. The IOPS limit should be in the range 256 to 4294967294, or -1 for unlimited (default). limitMbps Integer The MB/s throughput limit for the volume. description Text Text to be added to the volume's description on the Nimble array. performancePolicy 2 Text The name of the performance policy to assign to the volume. Default example performance policies include \"Backup Repository\", \"Exchange 2003 data store\", \"Exchange 2007 data store\", \"Exchange 2010 data store\", \"Exchange log\", \"Oracle OLTP\", \"Other Workloads\", \"SharePoint\", \"SQL Server\", \"SQL Server 2012\", \"SQL Server Logs\". protectionTemplate 1 Text The name of the protection template to assign to the volume. Default examples of protection templates include \"Retain-30Daily\", \"Retain-48Hourly-30aily-52Weekly\", and \"Retain-90Daily\". folder Text The name of the Nimble folder in which to place the volume. thick Boolean Indicates that the volume should be thick provisioned. dedupeEnabled 3 Boolean Indicates that the volume should enable deduplication. syncOnDetach Boolean Indicates that a snapshot of the volume should be synced to the replication partner each time it is detached from a node. Restrictions applicable when using the CSI volume mutator : 1 = Parameter is immutable and can't be altered after provisioning/cloning. 2 = Performance policies may only be mutated between performance polices with the same block size. 3 = Deduplication may only be mutated within the same performance policy application category and block size. Note Performance Policies, Folders and Protection Templates are Nimble specific constructs that can be created on the Nimble array itself to address particular requirements or workloads. Please consult with the storage admin or read the admin guide found on HPE InfoSight .","title":"Common parameters for provisioning and cloning"},{"location":"container_storage_provider/hpe_nimble_storage/index.html#provisioning_parameters","text":"These parameters are immutable for both volumes and clones once created, clones will inherit parent attributes. Parameter String Description encrypted Boolean Indicates that the volume should be encrypted. pool Text The name of the pool in which to place the volume.","title":"Provisioning parameters"},{"location":"container_storage_provider/hpe_nimble_storage/index.html#pod_inline_volume_parameters_local_ephemeral_volumes","text":"These parameters are applicable only for Pod inline volumes and to be specified within Pod spec. Parameter String Description csi.storage.k8s.io/ephemeral Boolean Indicates that the request is for ephemeral inline volume. This is a mandatory parameter and must be set to \"true\". inline-volume-secret-name Text A reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodePublishVolume call. inline-volume-secret-namespace Text The namespace of inline-volume-secret-name for ephemeral inline volume. size Text The size of ephemeral volume specified in MiB or GiB. If unspecified, a default value will be used. accessProtocol Text Storage access protocol to use, \"iscsi\" or \"fc\". Important All parameters are required for inline ephemeral volumes.","title":"Pod inline volume parameters (Local Ephemeral Volumes)"},{"location":"container_storage_provider/hpe_nimble_storage/index.html#cloning_parameters","text":"Cloning supports two modes of cloning. Either use cloneOf and reference a PVC in the current namespace or use importVolAsClone and reference a Nimble volume name to clone and import to Kubernetes. Parameter String Description cloneOf Text The name of the PV to be cloned. cloneOf and importVolAsClone are mutually exclusive. importVolAsClone Text The name of the Nimble volume to clone and import. importVolAsClone and cloneOf are mutually exclusive. snapshot Text The name of the snapshot to base the clone on. This is optional. If not specified, a new snapshot is created. createSnapshot Boolean Indicates that a new snapshot of the volume should be taken matching the name provided in the snapshot parameter. If the snapshot parameter is not specified, a default name will be created.","title":"Cloning parameters"},{"location":"container_storage_provider/hpe_nimble_storage/index.html#import_parameters","text":"Importing volumes to Kubernetes requires the source Nimble volume to be offline. In case of reverse replication, the upstream volume should be in offline state. All previous Access Control Records and Initiator Groups will be stripped from the volume when put under control of the HPE CSI Driver. Parameter String Description importVolumeName Text The name of the Nimble volume to import. snapshot Text The name of the Nimble snapshot to restore the imported volume to after takeover. If not specified, the volume will not be restored. takeover Boolean Indicates the current group will takeover ownership of the Nimble volume and volume collection. This should be performed against a downstream replica. reverseReplication Boolean Reverses the replication direction so that writes to the Nimble volume are replicated back to the group where it was replicated from. forceImport Boolean Forces the import of a volume that is not owned by the group and is not part of a volume collection. If the volume is part of a volume collection, use takeover instead.","title":"Import parameters"},{"location":"container_storage_provider/hpe_nimble_storage/index.html#volumesnapshotclass_parameters","text":"These parametes are for VolumeSnapshotClass objects when using CSI snapshots. The external snapshotter needs to be deployed on the Kubernetes cluster and is usually performed by the Kubernetes vendor. Check enabling CSI snapshots for more information. How to use VolumeSnapshotClass and VolumeSnapshot objects is elaborated on in using CSI snapshots . Parameter String Description description Text Text to be added to the snapshot's description on the Nimble array. writable Boolean Indicates if the snapshot is writable on the Nimble array. online Boolean Indicates if the snapshot is set to online on the Nimble array.","title":"VolumeSnapshotClass parameters"},{"location":"csi_driver/index.html","text":"Introduction \u00b6 A Container Storage Interface ( CSI ) Driver for Kubernetes. The HPE CSI Driver for Kubernetes allows you to use a Container Storage Provider (CSP) to perform data management operations on storage resources. The architecture of the CSI driver allows block storage vendors to implement a CSP that follows the specification (a browser friendly version ). The CSI driver architecture allows a complete separation of concerns between upstream Kubernetes core, SIG Storage (CSI owners), CSI driver author (HPE) and the backend CSP developer. Tip The HPE CSI Driver for Kubernetes is vendor agnostic. Any entity may leverage the driver and provide their own Container Storage Provider. Table of Contents \u00b6 Introduction Table of Contents Features and capabilities Compatibility and support HPE CSI Driver for Kubernetes 1.3.0 HPE CSI Driver for Kubernetes 1.2.0 HPE CSI Driver for Kubernetes 1.1.1 Release archive Known limitations iSCSI CHAP considerations CSI driver 1.2.1 and below CSI driver 1.3.0 and above Kubernetes feature gates Kubernetes 1.13 Kubernetes 1.14 Kubernetes 1.15 Kubernetes 1.19 Features and capabilities \u00b6 CSI gradually mature features and capabilities in the specification at the pace of the community. HPE keep a close watch on differentiating features the primary storage family of products may be suitable for implementing in CSI and Kubernetes. HPE experiment early and often. That's why it's sometimes possible to observe a certain feature being available in the CSI driver although it hasn't been announced or isn't documented. Below is the official table for CSI features we track and deem readily available for use after we've officially tested and validated it in the platform matrix . Feature K8s maturity Since K8s version HPE CSI Driver Dynamic Provisioning GA 1.13 1.0.0 Raw Block Volume GA 1.18 1.2.0 Volume Expansion Beta 1.16 1.1.0 PVC Data Source GA 1.18 1.1.0 Inline Ephemeral Volumes Beta 1.16 1.2.0 Volume Snapshots Beta 1.17 1.1.0 Volume Limits GA 1.17 1.2.0 Volume Mutator 1 N/A N/A 1.3.0 Generic Ephemeral Volumes Alpha 1.19 1.3.0 Topology GA 1.17 Future 1 = HPE CSI Driver for Kubernetes specific feature. CSP support may vary. Depending on the CSP, it may support a number of different snapshotting, cloning and restoring operations by taking advantage of StorageClass parameter overloading. Please see the respective CSP for additional functionality. Refer to the official table of feature gates in the Kubernetes docs to find availability of beta and alpha features. HPE provide limited support on non-GA CSI features. Please file any issues, questions or feature requests here . You may also join our Slack community to chat with HPE folks close to this project. We hang out in #NimbleStorage , #3par-primera and #Kubernetes , sign up at slack.hpedev.io and login at hpedev.slack.com . Tip Familiarize yourself with the basic requirements below for running the CSI driver on your Kubernetes cluster. It's then highly recommended to continue installing the CSI driver with either a Helm chart or an Operator . Compatibility and support \u00b6 These are the combinations HPE has tested and can provide official support services around for each of the CSI driver releases. Each Container Storage Provider has it's own requirements in terms of storage platform OS and may have other constraints not listed here. Note For Kubernetes 1.12 and earlier please see legacy FlexVolume drivers . HPE CSI Driver for Kubernetes 1.3.0 \u00b6 Release highlights: Kubernetes CSI Sidecar: Volume Mutator Broader ecosystem support Native iSCSI CHAP configuration Kubernetes 1.15-1.18 1 Worker OS CentOS 7.6, RHEL 7.6, RHCOS 4.3-4.4, Ubuntu 18.04, Ubuntu 20.04 Data protocol Fibre Channel, iSCSI Platforms NimbleOS 5.0.10.x, 5.1.4.200-x, 5.2.1.x, 5.3.0.x 3PAR OS 3.3.1 Primera OS 4.0.0, 4.1.0, 4.2.0 2 Release notes v1.3.0 on GitHub Blogs Around The Storage Block (release) HPE DEV (Remote copy peer persistence tutorial) HPE DEV (Introducing the volume mutator) 1 = For HPE Ezmeral Container Platform and Rancher; Kubernetes clusters must be deployed within the currently supported range of \"Worker OS\" platforms listed in the above table. See partner ecosystems for other variations. 2 = Only FC is supported on Primera OS prior to 4.2.0. HPE CSI Driver for Kubernetes 1.2.0 \u00b6 Release highlights: Support for raw block volumes and inline ephemeral volumes. NFS Server Provisioner in Tech Preview (beta). Kubernetes 1.14-1.18 Worker OS CentOS 7.6, RHEL 7.6, RHCOS 4.2-4.3, Ubuntu 16.04, Ubuntu 18.04 Data protocol Fibre Channel, iSCSI Platforms NimbleOS 5.0.10.x, 5.1.3.1000-x, 5.1.4.200-x, 5.2.1.x 3PAR OS 3.3.1 Primera OS 4.0.0, 4.1.0 (FC only) Release notes v1.2.0 on GitHub Blogs Around The Storage Block (release) HPE DEV (tutorial for raw block and inline volumes) Around The Storage Block (NFS Server Provisioner) HPE DEV (tutorial for NFS) HPE CSI Driver for Kubernetes 1.1.1 \u00b6 Release highlights: Support for HPE 3PAR and Primera Container Storage Provider. Kubernetes 1.13-1.17 Worker OS CentOS 7.6, RHEL 7.6, RHCOS 4.2-4.3, Ubuntu 16.04, Ubuntu 18.04 Data protocol Fibre Channel, iSCSI Platforms NimbleOS 5.0.8.x, 5.1.3.x, 5.1.4.x 3PAR OS 3.3.1 Primera OS 4.0.0, 4.1.0 (FC only) Release notes N/A Blogs HPE Storage Tech Insiders (release), HPE DEV (tutorial for \"primera3par\" CSP) Release archive \u00b6 HPE currently supports up to three minor releases of the HPE CSI Driver for Kubernetes. Unsupported releases Known limitations \u00b6 Always check with the Kubernetes vendor distribution which CSI features are available for use and supported by the vendor. iSCSI CHAP considerations \u00b6 If iSCSI CHAP is being used in the environment, consider the following. CSI driver 1.2.1 and below \u00b6 In version 1.2.1 and below, the CSI driver did not support CHAP natively. CHAP must be enabled manually on the worker nodes before deploying the CSI driver on the cluster. This also needs to be applied to new worker nodes before they join the cluster. CSI driver 1.3.0 and above \u00b6 CHAP is now an optional part of the initial deployment of the driver with parameters passed to Helm or the Operator. For object definitions, the CHAP_USER and CHAP_PASSWORD needs to be supplied to the csi-node-driver . The CHAP username and secret is picked up in the hpenodeinfo Custom Resource Definition (CRD). The CSP is under contract to create the user if it doesn't exist on the backend. CHAP is a good measure to prevent unauthorized access to iSCSI targets, it does not encrypt data on the wire. CHAP secrets should be at least twelve charcters in length. Kubernetes feature gates \u00b6 Different features mature at different rates. Refer to the official table of feature gates in the Kubernetes docs. The following guidelines appliy to which feature gates got introduced as alphas for the corresponding version of Kubernetes. For example, ExpandCSIVolumes got introduced in 1.14 but is still an alpha in 1.15, hence you need to enable that feature gate in 1.15 as well if you want to use it. Kubernetes 1.13 \u00b6 --allow-privileged flag must be set to true for the API server Kubernetes 1.14 \u00b6 --allow-privileged flag must be set to true for the API server --feature-gates=ExpandCSIVolumes=true,ExpandInUsePersistentVolumes=true feature gate flags must be set to true for both the API server and kubelet for resize support Kubernetes 1.15 \u00b6 --allow-privileged flag must be set to true for the API server --feature-gates=ExpandCSIVolumes=true,ExpandInUsePersistentVolumes=true feature gate flags must be set to true for both the API server and kubelet for resize support --feature-gates=CSIInlineVolume=true feature gate flag must be set to true for both the API server and kubelet for pod inline volumes (Ephemeral Local Volumes) support --feature-gates=VolumePVCDataSource=true feature gate flag must be set to true for both the API server and kubelet for Volume cloning support Kubernetes 1.19 \u00b6 --feature-gates=GenericEphemeralVolume=true feature gate flags needs to be passed to api-server, scheduler, controller-manager and kubelet to enable Generic Ephemeral Volumes","title":"Overview"},{"location":"csi_driver/index.html#introduction","text":"A Container Storage Interface ( CSI ) Driver for Kubernetes. The HPE CSI Driver for Kubernetes allows you to use a Container Storage Provider (CSP) to perform data management operations on storage resources. The architecture of the CSI driver allows block storage vendors to implement a CSP that follows the specification (a browser friendly version ). The CSI driver architecture allows a complete separation of concerns between upstream Kubernetes core, SIG Storage (CSI owners), CSI driver author (HPE) and the backend CSP developer. Tip The HPE CSI Driver for Kubernetes is vendor agnostic. Any entity may leverage the driver and provide their own Container Storage Provider.","title":"Introduction"},{"location":"csi_driver/index.html#table_of_contents","text":"Introduction Table of Contents Features and capabilities Compatibility and support HPE CSI Driver for Kubernetes 1.3.0 HPE CSI Driver for Kubernetes 1.2.0 HPE CSI Driver for Kubernetes 1.1.1 Release archive Known limitations iSCSI CHAP considerations CSI driver 1.2.1 and below CSI driver 1.3.0 and above Kubernetes feature gates Kubernetes 1.13 Kubernetes 1.14 Kubernetes 1.15 Kubernetes 1.19","title":"Table of Contents"},{"location":"csi_driver/index.html#features_and_capabilities","text":"CSI gradually mature features and capabilities in the specification at the pace of the community. HPE keep a close watch on differentiating features the primary storage family of products may be suitable for implementing in CSI and Kubernetes. HPE experiment early and often. That's why it's sometimes possible to observe a certain feature being available in the CSI driver although it hasn't been announced or isn't documented. Below is the official table for CSI features we track and deem readily available for use after we've officially tested and validated it in the platform matrix . Feature K8s maturity Since K8s version HPE CSI Driver Dynamic Provisioning GA 1.13 1.0.0 Raw Block Volume GA 1.18 1.2.0 Volume Expansion Beta 1.16 1.1.0 PVC Data Source GA 1.18 1.1.0 Inline Ephemeral Volumes Beta 1.16 1.2.0 Volume Snapshots Beta 1.17 1.1.0 Volume Limits GA 1.17 1.2.0 Volume Mutator 1 N/A N/A 1.3.0 Generic Ephemeral Volumes Alpha 1.19 1.3.0 Topology GA 1.17 Future 1 = HPE CSI Driver for Kubernetes specific feature. CSP support may vary. Depending on the CSP, it may support a number of different snapshotting, cloning and restoring operations by taking advantage of StorageClass parameter overloading. Please see the respective CSP for additional functionality. Refer to the official table of feature gates in the Kubernetes docs to find availability of beta and alpha features. HPE provide limited support on non-GA CSI features. Please file any issues, questions or feature requests here . You may also join our Slack community to chat with HPE folks close to this project. We hang out in #NimbleStorage , #3par-primera and #Kubernetes , sign up at slack.hpedev.io and login at hpedev.slack.com . Tip Familiarize yourself with the basic requirements below for running the CSI driver on your Kubernetes cluster. It's then highly recommended to continue installing the CSI driver with either a Helm chart or an Operator .","title":"Features and capabilities"},{"location":"csi_driver/index.html#compatibility_and_support","text":"These are the combinations HPE has tested and can provide official support services around for each of the CSI driver releases. Each Container Storage Provider has it's own requirements in terms of storage platform OS and may have other constraints not listed here. Note For Kubernetes 1.12 and earlier please see legacy FlexVolume drivers .","title":"Compatibility and support"},{"location":"csi_driver/index.html#hpe_csi_driver_for_kubernetes_130","text":"Release highlights: Kubernetes CSI Sidecar: Volume Mutator Broader ecosystem support Native iSCSI CHAP configuration Kubernetes 1.15-1.18 1 Worker OS CentOS 7.6, RHEL 7.6, RHCOS 4.3-4.4, Ubuntu 18.04, Ubuntu 20.04 Data protocol Fibre Channel, iSCSI Platforms NimbleOS 5.0.10.x, 5.1.4.200-x, 5.2.1.x, 5.3.0.x 3PAR OS 3.3.1 Primera OS 4.0.0, 4.1.0, 4.2.0 2 Release notes v1.3.0 on GitHub Blogs Around The Storage Block (release) HPE DEV (Remote copy peer persistence tutorial) HPE DEV (Introducing the volume mutator) 1 = For HPE Ezmeral Container Platform and Rancher; Kubernetes clusters must be deployed within the currently supported range of \"Worker OS\" platforms listed in the above table. See partner ecosystems for other variations. 2 = Only FC is supported on Primera OS prior to 4.2.0.","title":"HPE CSI Driver for Kubernetes 1.3.0"},{"location":"csi_driver/index.html#hpe_csi_driver_for_kubernetes_120","text":"Release highlights: Support for raw block volumes and inline ephemeral volumes. NFS Server Provisioner in Tech Preview (beta). Kubernetes 1.14-1.18 Worker OS CentOS 7.6, RHEL 7.6, RHCOS 4.2-4.3, Ubuntu 16.04, Ubuntu 18.04 Data protocol Fibre Channel, iSCSI Platforms NimbleOS 5.0.10.x, 5.1.3.1000-x, 5.1.4.200-x, 5.2.1.x 3PAR OS 3.3.1 Primera OS 4.0.0, 4.1.0 (FC only) Release notes v1.2.0 on GitHub Blogs Around The Storage Block (release) HPE DEV (tutorial for raw block and inline volumes) Around The Storage Block (NFS Server Provisioner) HPE DEV (tutorial for NFS)","title":"HPE CSI Driver for Kubernetes 1.2.0"},{"location":"csi_driver/index.html#hpe_csi_driver_for_kubernetes_111","text":"Release highlights: Support for HPE 3PAR and Primera Container Storage Provider. Kubernetes 1.13-1.17 Worker OS CentOS 7.6, RHEL 7.6, RHCOS 4.2-4.3, Ubuntu 16.04, Ubuntu 18.04 Data protocol Fibre Channel, iSCSI Platforms NimbleOS 5.0.8.x, 5.1.3.x, 5.1.4.x 3PAR OS 3.3.1 Primera OS 4.0.0, 4.1.0 (FC only) Release notes N/A Blogs HPE Storage Tech Insiders (release), HPE DEV (tutorial for \"primera3par\" CSP)","title":"HPE CSI Driver for Kubernetes 1.1.1"},{"location":"csi_driver/index.html#release_archive","text":"HPE currently supports up to three minor releases of the HPE CSI Driver for Kubernetes. Unsupported releases","title":"Release archive"},{"location":"csi_driver/index.html#known_limitations","text":"Always check with the Kubernetes vendor distribution which CSI features are available for use and supported by the vendor.","title":"Known limitations"},{"location":"csi_driver/index.html#iscsi_chap_considerations","text":"If iSCSI CHAP is being used in the environment, consider the following.","title":"iSCSI CHAP considerations"},{"location":"csi_driver/index.html#csi_driver_121_and_below","text":"In version 1.2.1 and below, the CSI driver did not support CHAP natively. CHAP must be enabled manually on the worker nodes before deploying the CSI driver on the cluster. This also needs to be applied to new worker nodes before they join the cluster.","title":"CSI driver 1.2.1 and below"},{"location":"csi_driver/index.html#csi_driver_130_and_above","text":"CHAP is now an optional part of the initial deployment of the driver with parameters passed to Helm or the Operator. For object definitions, the CHAP_USER and CHAP_PASSWORD needs to be supplied to the csi-node-driver . The CHAP username and secret is picked up in the hpenodeinfo Custom Resource Definition (CRD). The CSP is under contract to create the user if it doesn't exist on the backend. CHAP is a good measure to prevent unauthorized access to iSCSI targets, it does not encrypt data on the wire. CHAP secrets should be at least twelve charcters in length.","title":"CSI driver 1.3.0 and above"},{"location":"csi_driver/index.html#kubernetes_feature_gates","text":"Different features mature at different rates. Refer to the official table of feature gates in the Kubernetes docs. The following guidelines appliy to which feature gates got introduced as alphas for the corresponding version of Kubernetes. For example, ExpandCSIVolumes got introduced in 1.14 but is still an alpha in 1.15, hence you need to enable that feature gate in 1.15 as well if you want to use it.","title":"Kubernetes feature gates"},{"location":"csi_driver/index.html#kubernetes_113","text":"--allow-privileged flag must be set to true for the API server","title":"Kubernetes 1.13"},{"location":"csi_driver/index.html#kubernetes_114","text":"--allow-privileged flag must be set to true for the API server --feature-gates=ExpandCSIVolumes=true,ExpandInUsePersistentVolumes=true feature gate flags must be set to true for both the API server and kubelet for resize support","title":"Kubernetes 1.14"},{"location":"csi_driver/index.html#kubernetes_115","text":"--allow-privileged flag must be set to true for the API server --feature-gates=ExpandCSIVolumes=true,ExpandInUsePersistentVolumes=true feature gate flags must be set to true for both the API server and kubelet for resize support --feature-gates=CSIInlineVolume=true feature gate flag must be set to true for both the API server and kubelet for pod inline volumes (Ephemeral Local Volumes) support --feature-gates=VolumePVCDataSource=true feature gate flag must be set to true for both the API server and kubelet for Volume cloning support","title":"Kubernetes 1.15"},{"location":"csi_driver/index.html#kubernetes_119","text":"--feature-gates=GenericEphemeralVolume=true feature gate flags needs to be passed to api-server, scheduler, controller-manager and kubelet to enable Generic Ephemeral Volumes","title":"Kubernetes 1.19"},{"location":"csi_driver/archive.html","text":"Unsupported releases \u00b6 HPE supports up to three minor releases. These release are kept here for historic purposes. Unsupported releases HPE CSI Driver for Kubernetes 1.1.0 HPE CSI Driver for Kubernetes 1.0.0 HPE CSI Driver for Kubernetes 1.1.0 \u00b6 Release highlights: Broader ecosystem support, official support for CSI snapshots and volume resize. Kubernetes 1.13-1.17 Worker OS CentOS 7.6, RHEL 7.6, RHCOS 4.2-4.3, Ubuntu 16.04, Ubuntu 18.04 Data protocol Fibre Channel, iSCSI Platforms NimbleOS 5.0.8.x, 5.1.3.x, 5.1.4.x Release notes v1.1.0 on GitHub Blogs HPE Storage Tech Insiders (release), HPE DEV (snapshots, clones, resize) HPE CSI Driver for Kubernetes 1.0.0 \u00b6 Release highlights: Initial GA release with support for Dynamic Provisioning. Kubernetes 1.13-1.17 Worker OS CentOS 7.6, RHEL 7.6, Ubuntu 16.04, Ubuntu 18.04 Data protocol Fibre Channel, iSCSI Platforms NimbleOS 5.0.8.x, 5.1.3.x, 5.1.4.x Release notes v1.0.0 on GitHub Blogs HPE Storage Tech Insiders (release), HPE DEV (architecture and introduction)","title":"Unsupported releases"},{"location":"csi_driver/archive.html#unsupported_releases","text":"HPE supports up to three minor releases. These release are kept here for historic purposes. Unsupported releases HPE CSI Driver for Kubernetes 1.1.0 HPE CSI Driver for Kubernetes 1.0.0","title":"Unsupported releases"},{"location":"csi_driver/archive.html#hpe_csi_driver_for_kubernetes_110","text":"Release highlights: Broader ecosystem support, official support for CSI snapshots and volume resize. Kubernetes 1.13-1.17 Worker OS CentOS 7.6, RHEL 7.6, RHCOS 4.2-4.3, Ubuntu 16.04, Ubuntu 18.04 Data protocol Fibre Channel, iSCSI Platforms NimbleOS 5.0.8.x, 5.1.3.x, 5.1.4.x Release notes v1.1.0 on GitHub Blogs HPE Storage Tech Insiders (release), HPE DEV (snapshots, clones, resize)","title":"HPE CSI Driver for Kubernetes 1.1.0"},{"location":"csi_driver/archive.html#hpe_csi_driver_for_kubernetes_100","text":"Release highlights: Initial GA release with support for Dynamic Provisioning. Kubernetes 1.13-1.17 Worker OS CentOS 7.6, RHEL 7.6, Ubuntu 16.04, Ubuntu 18.04 Data protocol Fibre Channel, iSCSI Platforms NimbleOS 5.0.8.x, 5.1.3.x, 5.1.4.x Release notes v1.0.0 on GitHub Blogs HPE Storage Tech Insiders (release), HPE DEV (architecture and introduction)","title":"HPE CSI Driver for Kubernetes 1.0.0"},{"location":"csi_driver/deployment.html","text":"Overview \u00b6 The HPE CSI Driver is deployed by using industry standard means, either a Helm chart or an Operator. An \"advanced install\" from object configuration files is provided as reference for partners, OEMs and users wanting to perform customizations and their own packaging or deployment methodologies. Overview Delivery vehicles Need help deciding? Helm Operator Red Hat OpenShift Container Platform Upstream Kubernetes and others Add a HPE storage backend Secret parameters Adding additional backends Create a StorageClass with the custom Secret Advanced install Manual CSI driver install Common Kubernetes 1.18 Kubernetes 1.17 Kubernetes 1.16 Kubernetes 1.15 Legacy versions Kubernetes 1.14 Kubernetes 1.13 Delivery vehicles \u00b6 As different methods of installation are provided, it might not be too obvious which delivery vehicle is the right one. Need help deciding? \u00b6 I have a... Then you need... Vanilla upstream Kubernetes cluster on a supported host OS. The Helm chart Red Hat OpenShift 4.x cluster. The certified CSI operator for OpenShift Supported environment with multiple backends. Helm chart with additional Secrets and StorageClasses HPE Ezmeral Container Platform environment. The Helm chart Operator Life-cycle Manager (OLM) environment. The CSI operator Unsupported host OS/Kubernetes cluster and like to tinker. The advanced install Undecided? If it's not clear what you should use for your environment, the Helm chart is most likely the correct answer. Helm \u00b6 Helm is the package manager for Kubernetes. Software is being delivered in a format designated as a \"chart\". Helm is a standalone CLI that interacts with the Kubernetes API server using your KUBECONFIG file. The official Helm chart for the HPE CSI Driver for Kubernetes is hosted on Artifact Hub . The chart only supports Helm 3 from version 1.3.0 of the HPE CSI Driver. In an effort to avoid duplicate documentation, please see the chart for instructions on how to deploy the CSI driver using Helm. Go to the chart on Artifact Hub . Operator \u00b6 The Operator pattern is based on the idea that software should be instantiated and run with a set of custom controllers in Kubernetes. It creates a native experience for any software running in Kubernetes. The official HPE CSI Operator for Kubernetes is hosted on OperatorHub.io . The CSI Operator images are hosted both on docker.io and officially certified containers on Red Hat Ecosystem Catalog. Red Hat OpenShift Container Platform \u00b6 The HPE CSI Operator for Kubernetes is a fully certified Operator for OpenShift. There are a few tweaks needed and there's a separate section for OpenShift. See Red Hat OpenShift in the partner ecosystem section Upstream Kubernetes and others \u00b6 Follow the documentation from the respective upstream distributions on how to deploy an Operator. In most cases, the Operator Lifecyle Manager (OLM) needs to be installed separately. As an example, we'll deploy version 0.14.1 of the OLM to be able to manage the HPE CSI Operator. Familiarize yourself while is the latest stable release on the OLM GitHub project's release page . curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/0.14.1/install.sh | bash -s 0.14.1 Install the HPE CSI Operator. kubectl create -f https://operatorhub.io/install/hpe-csi-operator.yaml The Operator will be installed in my-hpe-csi-operator namespace. Watch it come up by inspecting the ClusterServiceVersion (CSV). kubectl get csv -n my-hpe-csi-operator Next, a HPECSIDriver object needs to be instantiated. Create a file named hpe-csi-operator.yaml and populate it according to which CSP is being deployed. HPE Nimble Storage apiVersion: storage.hpe.com/v1 kind: HPECSIDriver metadata: name: csi-driver spec: backendType: nimble imagePullPolicy: IfNotPresent logLevel: info disableNodeConformance: false secret: backend: 192.168.1.1 create: true password: admin servicePort: '8080' username: admin storageClass: allowVolumeExpansion: true create: true defaultClass: false name: hpe-standard parameters: accessProtocol: iscsi fsType: xfs volumeDescription: Volume created by the HPE CSI Driver for Kubernetes HPE Primera and 3PAR apiVersion: storage.hpe.com/v1 kind: HPECSIDriver metadata: name: csi-driver spec: backendType: primera3par imagePullPolicy: IfNotPresent logLevel: info disableNodeConformance: false secret: backend: 10.10.0.1 create: true password: 3pardata servicePort: '8080' username: 3paradm storageClass: allowVolumeExpansion: true create: true defaultClass: false name: hpe-standard parameters: accessProtocol: iscsi fsType: xfs volumeDescription: Volume created by the HPE CSI Driver for Kubernetes Create a HPECSIDriver with the manifest. kubectl create -f hpe-csi-operator.yaml The CSI driver is now ready for use. Proceed to the next section to learn about using the driver. Add a HPE storage backend \u00b6 Once the CSI driver is deployed, two additional objects needs to be created to get started with dynamic provisioning of persistent storage, a Secret and a StorageClass . Tip Naming the Secret and StorageClass is entirely up to the user, however, to keep up with the examples on SCOD, it's highly recommended to use the names illustrated here. Secret parameters \u00b6 All parameters are mandatory and described below. Parameter Description serviceName This hostname or IP address where the Container Storage Provider (CSP) is running, usually a Kubernetes Service , such as \"nimble-csp-svc\" or \"primera3par-csp-svc\" servicePort This is port the serviceName is listening to. backend This is the management hostname or IP address of the actual backend storage system, such as a Nimble or 3PAR array. username Backend storage system username with the correct privileges to perform storage management. password Backend storage system password. Example: HPE Nimble Storage apiVersion: v1 kind: Secret metadata: name: hpe-backend namespace: kube-system stringData: serviceName: nimble-csp-svc servicePort: \"8080\" backend: 192.168.1.2 username: admin password: admin HPE Primera apiVersion: v1 kind: Secret metadata: name: hpe-backend namespace: kube-system stringData: serviceName: primera3par-csp-svc servicePort: \"8080\" backend: 10.10.0.2 username: 3paradm password: 3pardata Create the Secret using kubectl : kubectl create -f secret.yaml Tip In a real world scenario it's more practical to name the Secret something that makes sense for the organization. It could be the hostname of the backend or the role it carries, i.e \"hpe-nimble-sanjose-prod\". Next step involves creating a default StorageClass . Adding additional backends \u00b6 It's not uncommon to have multiple HPE primary storage systems within the same environment, either the same family or different ones. This section walks through the scenario of managing multiple StorageClass and Secret API objects to represent an environment with multiple systems. There's a brief tutorial available in the Video Gallery that walks through these steps. Note Make note of the Kubernetes Namespace or OpenShift project name used during the deployment. In the following examples, we will be using the \"kube-system\" Namespace . To view the current Secrets in the \"kube-system\" Namespace (assuming default names): kubectl -n kube-system get secret/hpe-backend NAME TYPE DATA AGE hpe-backend Opaque 5 2m This Secret is used by the CSI sidecars in the StorageClass to authenticate to a specific backend for CSI operations. In order to add a new Secret or manage access to multiple backends, additional Secrets will need to be created per backend. Secret Requirements Each Secret name must be unique. servicePort should be set to 8080 . To create a new Secret , specify the name, Namespace , backend username, backend password and the backend IP address to be used by the CSP and save it as custom-secret.yaml (a detailed description of the parameters are available above ). HPE Nimble Storage apiVersion: v1 kind: Secret metadata: name: custom-secret namespace: kube-system stringData: serviceName: nimble-csp-svc servicePort: \"8080\" backend: 192.168.1.2 username: admin password: admin HPE Primera apiVersion: v1 kind: Secret metadata: name: custom-secret namespace: kube-system stringData: serviceName: primera3par-csp-svc servicePort: \"8080\" backend: 10.10.0.2 username: 3paradm password: 3pardata Create the Secret using kubectl : kubectl create -f custom-secret.yaml You should now see the Secret in the \"kube-system\" Namespace : kubectl -n kube-system get secret/custom-secret NAME TYPE DATA AGE custom-secret Opaque 5 1m Create a StorageClass with the custom Secret \u00b6 To use the new Secret \"custom-secret\", create a new StorageClass using the Secret and the necessary StorageClass parameters. Please see the requirements section of the respective CSP . K8s 1.15+ apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-custom provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/controller-expand-secret-name: custom-secret csi.storage.k8s.io/controller-expand-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: custom-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: custom-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: custom-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: custom-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by using a custom Secret with the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete allowVolumeExpansion: true K8s 1.14 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-custom provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/resizer-secret-name: custom-secret csi.storage.k8s.io/resizer-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: custom-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: custom-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: custom-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: custom-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by using a custom Secret with the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete allowVolumeExpansion: true K8s 1.13 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-custom provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/controller-publish-secret-name: custom-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: custom-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: custom-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: custom-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by using a custom Secret with the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete Note Don't forget to call out the StorageClass explictly when creating PVCs from non-default StorageClasses . Next, Create a PersistentVolumeClaim from a StorageClass . Advanced install \u00b6 This guide is primarily written to accommodate a highly manual installation on upstream Kubernetes or partner OEMs engaged with HPE to bundle the HPE CSI Driver in a custom distribution. Installation steps may vary for different vendors and flavors of Kubernetes. The following example walks through deployment of the latest CSI driver. Critical It's highly recommended to use either the Helm chart or Operator to install the HPE CSI Driver for Kubernetes and the associated Container Storage Providers. Only venture down manual installation if your requirements can't be met by the Helm chart or Operator . Manual CSI driver install \u00b6 Deploy the CSI driver and sidecars for the relevant Kubernetes version. Common \u00b6 These object configuration files are common for all versions of Kubernetes. Worker node IO settings: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/hpe-linux-config.yaml Container Storage Provider: HPE Nimble Storage kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/nimble-csp.yaml HPE 3PAR and Primera kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/3par-primera-csp.yaml Important The above instructions assumes you have an array with a supported platform OS installed. Please see the requirements section of the respective CSP . After deploying the CSI driver for the particular version of Kubernetes being used below, add a HPE storage backend . Kubernetes 1.18 \u00b6 kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/hpe-csi-k8s-1.18.yaml Kubernetes 1.17 \u00b6 kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/hpe-csi-k8s-1.17.yaml Kubernetes 1.16 \u00b6 kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/hpe-csi-k8s-1.16.yaml Kubernetes 1.15 \u00b6 kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/hpe-csi-k8s-1.15.yaml Legacy versions \u00b6 Older versions of the HPE CSI Driver for Kubernetes are kept here for reference. Check the CSI driver GitHub repo for the appropriate YAML files to declare on the cluster for the respective version of Kubernetes. Kubernetes 1.14 \u00b6 Object definitons for HPE CSI Driver for Kubernetes v1.2.0 Note Latest supported CSI driver version is 1.2.0 for Kubernetes 1.14. Kubernetes 1.13 \u00b6 Object definitons for HPE CSI Driver for Kubernetes v1.1.0 Note Latest supported CSI driver version is 1.1.0 for Kubernetes 1.13. Depending on which version being deployed, different API objects gets created.","title":"Deployment"},{"location":"csi_driver/deployment.html#overview","text":"The HPE CSI Driver is deployed by using industry standard means, either a Helm chart or an Operator. An \"advanced install\" from object configuration files is provided as reference for partners, OEMs and users wanting to perform customizations and their own packaging or deployment methodologies. Overview Delivery vehicles Need help deciding? Helm Operator Red Hat OpenShift Container Platform Upstream Kubernetes and others Add a HPE storage backend Secret parameters Adding additional backends Create a StorageClass with the custom Secret Advanced install Manual CSI driver install Common Kubernetes 1.18 Kubernetes 1.17 Kubernetes 1.16 Kubernetes 1.15 Legacy versions Kubernetes 1.14 Kubernetes 1.13","title":"Overview"},{"location":"csi_driver/deployment.html#delivery_vehicles","text":"As different methods of installation are provided, it might not be too obvious which delivery vehicle is the right one.","title":"Delivery vehicles"},{"location":"csi_driver/deployment.html#need_help_deciding","text":"I have a... Then you need... Vanilla upstream Kubernetes cluster on a supported host OS. The Helm chart Red Hat OpenShift 4.x cluster. The certified CSI operator for OpenShift Supported environment with multiple backends. Helm chart with additional Secrets and StorageClasses HPE Ezmeral Container Platform environment. The Helm chart Operator Life-cycle Manager (OLM) environment. The CSI operator Unsupported host OS/Kubernetes cluster and like to tinker. The advanced install Undecided? If it's not clear what you should use for your environment, the Helm chart is most likely the correct answer.","title":"Need help deciding?"},{"location":"csi_driver/deployment.html#helm","text":"Helm is the package manager for Kubernetes. Software is being delivered in a format designated as a \"chart\". Helm is a standalone CLI that interacts with the Kubernetes API server using your KUBECONFIG file. The official Helm chart for the HPE CSI Driver for Kubernetes is hosted on Artifact Hub . The chart only supports Helm 3 from version 1.3.0 of the HPE CSI Driver. In an effort to avoid duplicate documentation, please see the chart for instructions on how to deploy the CSI driver using Helm. Go to the chart on Artifact Hub .","title":"Helm"},{"location":"csi_driver/deployment.html#operator","text":"The Operator pattern is based on the idea that software should be instantiated and run with a set of custom controllers in Kubernetes. It creates a native experience for any software running in Kubernetes. The official HPE CSI Operator for Kubernetes is hosted on OperatorHub.io . The CSI Operator images are hosted both on docker.io and officially certified containers on Red Hat Ecosystem Catalog.","title":"Operator"},{"location":"csi_driver/deployment.html#red_hat_openshift_container_platform","text":"The HPE CSI Operator for Kubernetes is a fully certified Operator for OpenShift. There are a few tweaks needed and there's a separate section for OpenShift. See Red Hat OpenShift in the partner ecosystem section","title":"Red Hat OpenShift Container Platform"},{"location":"csi_driver/deployment.html#upstream_kubernetes_and_others","text":"Follow the documentation from the respective upstream distributions on how to deploy an Operator. In most cases, the Operator Lifecyle Manager (OLM) needs to be installed separately. As an example, we'll deploy version 0.14.1 of the OLM to be able to manage the HPE CSI Operator. Familiarize yourself while is the latest stable release on the OLM GitHub project's release page . curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/0.14.1/install.sh | bash -s 0.14.1 Install the HPE CSI Operator. kubectl create -f https://operatorhub.io/install/hpe-csi-operator.yaml The Operator will be installed in my-hpe-csi-operator namespace. Watch it come up by inspecting the ClusterServiceVersion (CSV). kubectl get csv -n my-hpe-csi-operator Next, a HPECSIDriver object needs to be instantiated. Create a file named hpe-csi-operator.yaml and populate it according to which CSP is being deployed. HPE Nimble Storage apiVersion: storage.hpe.com/v1 kind: HPECSIDriver metadata: name: csi-driver spec: backendType: nimble imagePullPolicy: IfNotPresent logLevel: info disableNodeConformance: false secret: backend: 192.168.1.1 create: true password: admin servicePort: '8080' username: admin storageClass: allowVolumeExpansion: true create: true defaultClass: false name: hpe-standard parameters: accessProtocol: iscsi fsType: xfs volumeDescription: Volume created by the HPE CSI Driver for Kubernetes HPE Primera and 3PAR apiVersion: storage.hpe.com/v1 kind: HPECSIDriver metadata: name: csi-driver spec: backendType: primera3par imagePullPolicy: IfNotPresent logLevel: info disableNodeConformance: false secret: backend: 10.10.0.1 create: true password: 3pardata servicePort: '8080' username: 3paradm storageClass: allowVolumeExpansion: true create: true defaultClass: false name: hpe-standard parameters: accessProtocol: iscsi fsType: xfs volumeDescription: Volume created by the HPE CSI Driver for Kubernetes Create a HPECSIDriver with the manifest. kubectl create -f hpe-csi-operator.yaml The CSI driver is now ready for use. Proceed to the next section to learn about using the driver.","title":"Upstream Kubernetes and others"},{"location":"csi_driver/deployment.html#add_a_hpe_storage_backend","text":"Once the CSI driver is deployed, two additional objects needs to be created to get started with dynamic provisioning of persistent storage, a Secret and a StorageClass . Tip Naming the Secret and StorageClass is entirely up to the user, however, to keep up with the examples on SCOD, it's highly recommended to use the names illustrated here.","title":"Add a HPE storage backend"},{"location":"csi_driver/deployment.html#secret_parameters","text":"All parameters are mandatory and described below. Parameter Description serviceName This hostname or IP address where the Container Storage Provider (CSP) is running, usually a Kubernetes Service , such as \"nimble-csp-svc\" or \"primera3par-csp-svc\" servicePort This is port the serviceName is listening to. backend This is the management hostname or IP address of the actual backend storage system, such as a Nimble or 3PAR array. username Backend storage system username with the correct privileges to perform storage management. password Backend storage system password. Example: HPE Nimble Storage apiVersion: v1 kind: Secret metadata: name: hpe-backend namespace: kube-system stringData: serviceName: nimble-csp-svc servicePort: \"8080\" backend: 192.168.1.2 username: admin password: admin HPE Primera apiVersion: v1 kind: Secret metadata: name: hpe-backend namespace: kube-system stringData: serviceName: primera3par-csp-svc servicePort: \"8080\" backend: 10.10.0.2 username: 3paradm password: 3pardata Create the Secret using kubectl : kubectl create -f secret.yaml Tip In a real world scenario it's more practical to name the Secret something that makes sense for the organization. It could be the hostname of the backend or the role it carries, i.e \"hpe-nimble-sanjose-prod\". Next step involves creating a default StorageClass .","title":"Secret parameters"},{"location":"csi_driver/deployment.html#adding_additional_backends","text":"It's not uncommon to have multiple HPE primary storage systems within the same environment, either the same family or different ones. This section walks through the scenario of managing multiple StorageClass and Secret API objects to represent an environment with multiple systems. There's a brief tutorial available in the Video Gallery that walks through these steps. Note Make note of the Kubernetes Namespace or OpenShift project name used during the deployment. In the following examples, we will be using the \"kube-system\" Namespace . To view the current Secrets in the \"kube-system\" Namespace (assuming default names): kubectl -n kube-system get secret/hpe-backend NAME TYPE DATA AGE hpe-backend Opaque 5 2m This Secret is used by the CSI sidecars in the StorageClass to authenticate to a specific backend for CSI operations. In order to add a new Secret or manage access to multiple backends, additional Secrets will need to be created per backend. Secret Requirements Each Secret name must be unique. servicePort should be set to 8080 . To create a new Secret , specify the name, Namespace , backend username, backend password and the backend IP address to be used by the CSP and save it as custom-secret.yaml (a detailed description of the parameters are available above ). HPE Nimble Storage apiVersion: v1 kind: Secret metadata: name: custom-secret namespace: kube-system stringData: serviceName: nimble-csp-svc servicePort: \"8080\" backend: 192.168.1.2 username: admin password: admin HPE Primera apiVersion: v1 kind: Secret metadata: name: custom-secret namespace: kube-system stringData: serviceName: primera3par-csp-svc servicePort: \"8080\" backend: 10.10.0.2 username: 3paradm password: 3pardata Create the Secret using kubectl : kubectl create -f custom-secret.yaml You should now see the Secret in the \"kube-system\" Namespace : kubectl -n kube-system get secret/custom-secret NAME TYPE DATA AGE custom-secret Opaque 5 1m","title":"Adding additional backends"},{"location":"csi_driver/deployment.html#create_a_storageclass_with_the_custom_secret","text":"To use the new Secret \"custom-secret\", create a new StorageClass using the Secret and the necessary StorageClass parameters. Please see the requirements section of the respective CSP . K8s 1.15+ apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-custom provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/controller-expand-secret-name: custom-secret csi.storage.k8s.io/controller-expand-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: custom-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: custom-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: custom-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: custom-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by using a custom Secret with the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete allowVolumeExpansion: true K8s 1.14 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-custom provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/resizer-secret-name: custom-secret csi.storage.k8s.io/resizer-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: custom-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: custom-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: custom-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: custom-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by using a custom Secret with the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete allowVolumeExpansion: true K8s 1.13 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-custom provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/controller-publish-secret-name: custom-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: custom-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: custom-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: custom-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by using a custom Secret with the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete Note Don't forget to call out the StorageClass explictly when creating PVCs from non-default StorageClasses . Next, Create a PersistentVolumeClaim from a StorageClass .","title":"Create a StorageClass with the custom Secret"},{"location":"csi_driver/deployment.html#advanced_install","text":"This guide is primarily written to accommodate a highly manual installation on upstream Kubernetes or partner OEMs engaged with HPE to bundle the HPE CSI Driver in a custom distribution. Installation steps may vary for different vendors and flavors of Kubernetes. The following example walks through deployment of the latest CSI driver. Critical It's highly recommended to use either the Helm chart or Operator to install the HPE CSI Driver for Kubernetes and the associated Container Storage Providers. Only venture down manual installation if your requirements can't be met by the Helm chart or Operator .","title":"Advanced install"},{"location":"csi_driver/deployment.html#manual_csi_driver_install","text":"Deploy the CSI driver and sidecars for the relevant Kubernetes version.","title":"Manual CSI driver install"},{"location":"csi_driver/deployment.html#common","text":"These object configuration files are common for all versions of Kubernetes. Worker node IO settings: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/hpe-linux-config.yaml Container Storage Provider: HPE Nimble Storage kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/nimble-csp.yaml HPE 3PAR and Primera kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/3par-primera-csp.yaml Important The above instructions assumes you have an array with a supported platform OS installed. Please see the requirements section of the respective CSP . After deploying the CSI driver for the particular version of Kubernetes being used below, add a HPE storage backend .","title":"Common"},{"location":"csi_driver/deployment.html#kubernetes_118","text":"kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/hpe-csi-k8s-1.18.yaml","title":"Kubernetes 1.18"},{"location":"csi_driver/deployment.html#kubernetes_117","text":"kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/hpe-csi-k8s-1.17.yaml","title":"Kubernetes 1.17"},{"location":"csi_driver/deployment.html#kubernetes_116","text":"kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/hpe-csi-k8s-1.16.yaml","title":"Kubernetes 1.16"},{"location":"csi_driver/deployment.html#kubernetes_115","text":"kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/csi-driver/v1.3.0/hpe-csi-k8s-1.15.yaml","title":"Kubernetes 1.15"},{"location":"csi_driver/deployment.html#legacy_versions","text":"Older versions of the HPE CSI Driver for Kubernetes are kept here for reference. Check the CSI driver GitHub repo for the appropriate YAML files to declare on the cluster for the respective version of Kubernetes.","title":"Legacy versions"},{"location":"csi_driver/deployment.html#kubernetes_114","text":"Object definitons for HPE CSI Driver for Kubernetes v1.2.0 Note Latest supported CSI driver version is 1.2.0 for Kubernetes 1.14.","title":"Kubernetes 1.14"},{"location":"csi_driver/deployment.html#kubernetes_113","text":"Object definitons for HPE CSI Driver for Kubernetes v1.1.0 Note Latest supported CSI driver version is 1.1.0 for Kubernetes 1.13. Depending on which version being deployed, different API objects gets created.","title":"Kubernetes 1.13"},{"location":"csi_driver/diagnostics.html","text":"Introduction \u00b6 It's recommended to familiarize yourself with inspecting workloads on Kubernetes. This particular cheat sheet is very useful to have readily available. Sanity checks \u00b6 Once the CSI driver has been deployed either through object configuration files, Helm or an Operator. This view should be representative of what a healthy system should look like after install. If any of the workload deployments lists anything but Running , proceed to inspect the logs of the problematic workload. HPE Nimble Storage kubectl get pods --all-namespaces -l 'app in (nimble-csp, hpe-csi-node, hpe-csi-controller)' NAMESPACE NAME READY STATUS RESTARTS AGE kube-system hpe-csi-controller-7d9cd6b855-zzmd9 5/5 Running 0 15s kube-system hpe-csi-node-dk5t4 2/2 Running 0 15s kube-system hpe-csi-node-pwq2d 2/2 Running 0 15s kube-system nimble-csp-546c9c4dd4-5lsdt 1/1 Running 0 15s HPE 3PAR and Primera kubectl get pods --all-namespaces -l 'app in (primera3par-csp, hpe-csi-node, hpe-csi-controller)' NAMESPACE NAME READY STATUS RESTARTS AGE kube-system hpe-csi-controller-7d9cd6b855-fqppd 5/5 Running 0 14s kube-system hpe-csi-node-86kh6 2/2 Running 0 14s kube-system hpe-csi-node-k8p4p 2/2 Running 0 14s kube-system hpe-csi-node-r2mg8 2/2 Running 0 14s kube-system hpe-csi-node-vwb5r 2/2 Running 0 14s kube-system primera3par-csp-546c9c4dd4-bcwc6 1/1 Running 0 14s A Custom Resource Definition (CRD) named hpenodeinfos.storage.hpe.com holds important network and host initiator information. Retrieve list of nodes. kubectl get hpenodeinfos $ kubectl get hpenodeinfos NAME AGE tme-lnx-worker1 57m tme-lnx-worker3 57m tme-lnx-worker2 57m tme-lnx-worker4 57m Inspect a node. kubectl get hpenodeinfos/tme-lnx-worker1 -o yaml apiVersion: storage.hpe.com/v1 kind: HPENodeInfo metadata: creationTimestamp: \"2020-08-24T23:50:09Z\" generation: 1 managedFields: - apiVersion: storage.hpe.com/v1 fieldsType: FieldsV1 fieldsV1: f:spec: .: {} f:chap_password: {} f:chap_user: {} f:iqns: {} f:networks: {} f:uuid: {} manager: csi-driver operation: Update time: \"2020-08-24T23:50:09Z\" name: tme-lnx-worker1 resourceVersion: \"30337986\" selfLink: /apis/storage.hpe.com/v1/hpenodeinfos/tme-lnx-worker1 uid: 3984752b-29ac-48de-8ca0-8381532cbf06 spec: chap_password: RGlkIHlvdSByZWFsbHkgZGVjb2RlIHRoaXM/ chap_user: chap-user iqns: - iqn.1994-05.com.redhat:828e7a4eef40 networks: - 10.2.2.2/16 - 172.16.6.115/24 - 172.16.8.115/24 - 172.17.0.1/16 - 10.1.1.0/12 uuid: 0242f811-3995-746d-652d-6c6e78352d77 NFS Server Provisioner resources \u00b6 The NFS Server Provisioner consists of a number of Kubernetes resources per PVC. The default Namespace where the resources are deployed is \"hpe-nfs\" but is configurable in the StorageClass . See base StorageClass parameters for more details. Object Name Purpose ConfigMap hpe-nfs-config This ConfigMap holds the configuration file for the NFS server. Local tweaks may be wanted. Please see the config file reference for more details. Deployment hpe-nfs-<UUID> The Deployment that is running the NFS Pod . Service hpe-nfs-<UUID> Pod Service the NFS clients perform mounts against. PVC hpe-nfs-<UUID> The RWO claim serving the NFS workload. Tip The <UUID> stems from the user request RWX claim UUID for easy tracking. Logging \u00b6 Log files associated with the HPE CSI Driver logs data to the standard output stream. If the logs need to be retained for long term, use a standard logging solution for Kubernetes such as Fluentd. Some of the logs on the host are persisted which follow standard logrotate policies. CSI driver logs \u00b6 Node driver kubectl logs -f daemonset.apps/hpe-csi-node hpe-csi-driver -n kube-system Controller driver kubectl logs -f deployment.apps/hpe-csi-controller hpe-csi-driver -n kube-system Tip The logs for both node and controller drivers are persisted at /var/log/hpe-csi.log Log level \u00b6 Log levels for both CSI Controller and Node driver can be controlled using LOG_LEVEL environment variable. Possible values are info , warn , error , debug , and trace . Apply the changes using kubectl apply -f <yaml> command after adding this to CSI controller and node container spec as below. For Helm charts this is controlled through logLevel variable in values.yaml . env: - name: LOG_LEVEL value: trace CSP logs \u00b6 CSP logs can be accessed from their respective services. HPE Nimble Storage kubectl logs -f svc/nimble-csp-svc -n kube-system HPE 3PAR and Primera kubectl logs -f svc/primera3par-csp-svc -n kube-system Log collector \u00b6 Log collector script hpe-logcollector.sh can be used to collect the logs from any node which has kubectl access to the cluster. curl -O https://raw.githubusercontent.com/hpe-storage/csi-driver/master/hpe-logcollector.sh chmod 555 hpe-logcollector.sh Usage: ./hpe-logcollector.sh -h Diagnostic Script to collect HPE Storage logs using kubectl Usage: hpe-logcollector.sh [-h|--help][--node-name NODE_NAME][-n|--namespace NAMESPACE][-a|--all] Where -h|--help Print the Usage text --node-name NODE_NAME where NODE_NAME is kubernetes Node Name needed to collect the hpe diagnostic logs of the Node -n|--namespace NAMESPACE where NAMESPACE is namespace of the pod deployment. default is kube-system -a|--all collect diagnostic logs of all the nodes.If nothing is specified logs would be collected from all the nodes Tuning \u00b6 HPE provides a set of well tested defaults for the CSI driver and all the supported CSPs. In certain case it may be necessary to fine tune the CSI driver to accommodate a certain workload or behavior. Data path configuration \u00b6 The HPE CSI Driver for Kubernetes automatically configures Linux iSCSI/multipath settings based on config.json . In order to tune these values, edit the config map with kubectl edit configmap hpe-linux-config -n kube-system and restart node plugin using kubectl delete pod -l app=hpe-csi-node to apply. Important HPE provide a set of general purpose default values for the IO paths, tuning is only required if prescribed by HPE.","title":"Diagnostics"},{"location":"csi_driver/diagnostics.html#introduction","text":"It's recommended to familiarize yourself with inspecting workloads on Kubernetes. This particular cheat sheet is very useful to have readily available.","title":"Introduction"},{"location":"csi_driver/diagnostics.html#sanity_checks","text":"Once the CSI driver has been deployed either through object configuration files, Helm or an Operator. This view should be representative of what a healthy system should look like after install. If any of the workload deployments lists anything but Running , proceed to inspect the logs of the problematic workload. HPE Nimble Storage kubectl get pods --all-namespaces -l 'app in (nimble-csp, hpe-csi-node, hpe-csi-controller)' NAMESPACE NAME READY STATUS RESTARTS AGE kube-system hpe-csi-controller-7d9cd6b855-zzmd9 5/5 Running 0 15s kube-system hpe-csi-node-dk5t4 2/2 Running 0 15s kube-system hpe-csi-node-pwq2d 2/2 Running 0 15s kube-system nimble-csp-546c9c4dd4-5lsdt 1/1 Running 0 15s HPE 3PAR and Primera kubectl get pods --all-namespaces -l 'app in (primera3par-csp, hpe-csi-node, hpe-csi-controller)' NAMESPACE NAME READY STATUS RESTARTS AGE kube-system hpe-csi-controller-7d9cd6b855-fqppd 5/5 Running 0 14s kube-system hpe-csi-node-86kh6 2/2 Running 0 14s kube-system hpe-csi-node-k8p4p 2/2 Running 0 14s kube-system hpe-csi-node-r2mg8 2/2 Running 0 14s kube-system hpe-csi-node-vwb5r 2/2 Running 0 14s kube-system primera3par-csp-546c9c4dd4-bcwc6 1/1 Running 0 14s A Custom Resource Definition (CRD) named hpenodeinfos.storage.hpe.com holds important network and host initiator information. Retrieve list of nodes. kubectl get hpenodeinfos $ kubectl get hpenodeinfos NAME AGE tme-lnx-worker1 57m tme-lnx-worker3 57m tme-lnx-worker2 57m tme-lnx-worker4 57m Inspect a node. kubectl get hpenodeinfos/tme-lnx-worker1 -o yaml apiVersion: storage.hpe.com/v1 kind: HPENodeInfo metadata: creationTimestamp: \"2020-08-24T23:50:09Z\" generation: 1 managedFields: - apiVersion: storage.hpe.com/v1 fieldsType: FieldsV1 fieldsV1: f:spec: .: {} f:chap_password: {} f:chap_user: {} f:iqns: {} f:networks: {} f:uuid: {} manager: csi-driver operation: Update time: \"2020-08-24T23:50:09Z\" name: tme-lnx-worker1 resourceVersion: \"30337986\" selfLink: /apis/storage.hpe.com/v1/hpenodeinfos/tme-lnx-worker1 uid: 3984752b-29ac-48de-8ca0-8381532cbf06 spec: chap_password: RGlkIHlvdSByZWFsbHkgZGVjb2RlIHRoaXM/ chap_user: chap-user iqns: - iqn.1994-05.com.redhat:828e7a4eef40 networks: - 10.2.2.2/16 - 172.16.6.115/24 - 172.16.8.115/24 - 172.17.0.1/16 - 10.1.1.0/12 uuid: 0242f811-3995-746d-652d-6c6e78352d77","title":"Sanity checks"},{"location":"csi_driver/diagnostics.html#nfs_server_provisioner_resources","text":"The NFS Server Provisioner consists of a number of Kubernetes resources per PVC. The default Namespace where the resources are deployed is \"hpe-nfs\" but is configurable in the StorageClass . See base StorageClass parameters for more details. Object Name Purpose ConfigMap hpe-nfs-config This ConfigMap holds the configuration file for the NFS server. Local tweaks may be wanted. Please see the config file reference for more details. Deployment hpe-nfs-<UUID> The Deployment that is running the NFS Pod . Service hpe-nfs-<UUID> Pod Service the NFS clients perform mounts against. PVC hpe-nfs-<UUID> The RWO claim serving the NFS workload. Tip The <UUID> stems from the user request RWX claim UUID for easy tracking.","title":"NFS Server Provisioner resources"},{"location":"csi_driver/diagnostics.html#logging","text":"Log files associated with the HPE CSI Driver logs data to the standard output stream. If the logs need to be retained for long term, use a standard logging solution for Kubernetes such as Fluentd. Some of the logs on the host are persisted which follow standard logrotate policies.","title":"Logging"},{"location":"csi_driver/diagnostics.html#csi_driver_logs","text":"Node driver kubectl logs -f daemonset.apps/hpe-csi-node hpe-csi-driver -n kube-system Controller driver kubectl logs -f deployment.apps/hpe-csi-controller hpe-csi-driver -n kube-system Tip The logs for both node and controller drivers are persisted at /var/log/hpe-csi.log","title":"CSI driver logs"},{"location":"csi_driver/diagnostics.html#log_level","text":"Log levels for both CSI Controller and Node driver can be controlled using LOG_LEVEL environment variable. Possible values are info , warn , error , debug , and trace . Apply the changes using kubectl apply -f <yaml> command after adding this to CSI controller and node container spec as below. For Helm charts this is controlled through logLevel variable in values.yaml . env: - name: LOG_LEVEL value: trace","title":"Log level"},{"location":"csi_driver/diagnostics.html#csp_logs","text":"CSP logs can be accessed from their respective services. HPE Nimble Storage kubectl logs -f svc/nimble-csp-svc -n kube-system HPE 3PAR and Primera kubectl logs -f svc/primera3par-csp-svc -n kube-system","title":"CSP logs"},{"location":"csi_driver/diagnostics.html#log_collector","text":"Log collector script hpe-logcollector.sh can be used to collect the logs from any node which has kubectl access to the cluster. curl -O https://raw.githubusercontent.com/hpe-storage/csi-driver/master/hpe-logcollector.sh chmod 555 hpe-logcollector.sh Usage: ./hpe-logcollector.sh -h Diagnostic Script to collect HPE Storage logs using kubectl Usage: hpe-logcollector.sh [-h|--help][--node-name NODE_NAME][-n|--namespace NAMESPACE][-a|--all] Where -h|--help Print the Usage text --node-name NODE_NAME where NODE_NAME is kubernetes Node Name needed to collect the hpe diagnostic logs of the Node -n|--namespace NAMESPACE where NAMESPACE is namespace of the pod deployment. default is kube-system -a|--all collect diagnostic logs of all the nodes.If nothing is specified logs would be collected from all the nodes","title":"Log collector"},{"location":"csi_driver/diagnostics.html#tuning","text":"HPE provides a set of well tested defaults for the CSI driver and all the supported CSPs. In certain case it may be necessary to fine tune the CSI driver to accommodate a certain workload or behavior.","title":"Tuning"},{"location":"csi_driver/diagnostics.html#data_path_configuration","text":"The HPE CSI Driver for Kubernetes automatically configures Linux iSCSI/multipath settings based on config.json . In order to tune these values, edit the config map with kubectl edit configmap hpe-linux-config -n kube-system and restart node plugin using kubectl delete pod -l app=hpe-csi-node to apply. Important HPE provide a set of general purpose default values for the IO paths, tuning is only required if prescribed by HPE.","title":"Data path configuration"},{"location":"csi_driver/monitor.html","text":"Introduction \u00b6 The HPE CSI Driver for Kubernetes includes a Kubernetes Pod Monitor. Specifically it looks for Pods with the label monitored-by: hpe-csi and has NodeLost status set on them. This usually occurs if a node becomes unresponsive or partioned due to a network outage. The Pod Monitor will delete the affected Pod and associated HPE CSI Driver VolumeAttachment to allow Kubernetes to reschedule the workload on a healthy node. The Pod Monitor is mandatory and automatically applied for the RWX server Deployment managed by the HPE CSI Driver. It may be used for any Pods on the Kubernetes cluster to perform a more graceful automatic recovery rather than performing a manual intervention to resurrect stuck Pods . CSI driver parameters \u00b6 The Pod Monitor is part of the \"hpe-csi-controller\" Deployment served by the \"hpe-csi-driver\" container. It's by default enabled and the Pod Monitor interval is set to 30 seconds. Edit the CSI driver deployment to change the interval or disable the Pod Monitor. kubectl edit -n kube-system deploy/hpe-csi-controller The parameters that control the \"hpe-csi-driver\" are the following: - --pod-monitor - --pod-monitor-interval=30 Pod inclusion \u00b6 Enable the Pod Monitor for a single replica Deployment by labeling the Pod (assumes an existing PVC name \"my-pvc\" exists). --- apiVersion: apps/v1 kind: Deployment metadata: name: my-app labels: app: my-app spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: my-app template: metadata: labels: monitored-by: hpe-csi app: my-app spec: containers: - image: busybox name: busybox command: - \"sleep\" - \"4800\" volumeMounts: - mountPath: /data name: my-vol volumes: - name: my-vol persistentVolumeClaim: claimName: my-pvc Tech Preview The Pod Monitor is currently in beta and should only be used for testing on non-production workloads.","title":"Pod Monitor"},{"location":"csi_driver/monitor.html#introduction","text":"The HPE CSI Driver for Kubernetes includes a Kubernetes Pod Monitor. Specifically it looks for Pods with the label monitored-by: hpe-csi and has NodeLost status set on them. This usually occurs if a node becomes unresponsive or partioned due to a network outage. The Pod Monitor will delete the affected Pod and associated HPE CSI Driver VolumeAttachment to allow Kubernetes to reschedule the workload on a healthy node. The Pod Monitor is mandatory and automatically applied for the RWX server Deployment managed by the HPE CSI Driver. It may be used for any Pods on the Kubernetes cluster to perform a more graceful automatic recovery rather than performing a manual intervention to resurrect stuck Pods .","title":"Introduction"},{"location":"csi_driver/monitor.html#csi_driver_parameters","text":"The Pod Monitor is part of the \"hpe-csi-controller\" Deployment served by the \"hpe-csi-driver\" container. It's by default enabled and the Pod Monitor interval is set to 30 seconds. Edit the CSI driver deployment to change the interval or disable the Pod Monitor. kubectl edit -n kube-system deploy/hpe-csi-controller The parameters that control the \"hpe-csi-driver\" are the following: - --pod-monitor - --pod-monitor-interval=30","title":"CSI driver parameters"},{"location":"csi_driver/monitor.html#pod_inclusion","text":"Enable the Pod Monitor for a single replica Deployment by labeling the Pod (assumes an existing PVC name \"my-pvc\" exists). --- apiVersion: apps/v1 kind: Deployment metadata: name: my-app labels: app: my-app spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: my-app template: metadata: labels: monitored-by: hpe-csi app: my-app spec: containers: - image: busybox name: busybox command: - \"sleep\" - \"4800\" volumeMounts: - mountPath: /data name: my-vol volumes: - name: my-vol persistentVolumeClaim: claimName: my-pvc Tech Preview The Pod Monitor is currently in beta and should only be used for testing on non-production workloads.","title":"Pod inclusion"},{"location":"csi_driver/using.html","text":"Overview \u00b6 At this point the CSI driver and CSP should be installed and configured. Important Most examples below assumes there's a Secret named \"hpe-backend\" in the \"kube-system\" Namespace . Learn how to add Secrets in the Deployment section . Overview PVC access modes Enabling CSI snapshots Base StorageClass parameters Provisioning concepts Create a PersistentVolumeClaim from a StorageClass Ephemeral inline volume Raw block volume Using CSI snapshots Expanding PVCs Using PVC Overrides Using volume mutations Using the NFS Server Provisioner Limitations and considerations for the NFS Server Provisioner Further reading Tip If you're familiar with the basic concepts of persistent storage on Kubernetes and are looking for an overview of example YAML declarations for different object types supported by the HPE CSI driver, visit the source code repo on GitHub. PVC access modes \u00b6 The HPE CSI Driver for Kubernetes is primarily a ReadWriteOnce (RWO) CSI implementation for block based storage. The CSI driver also supports ReadWriteMany (RWX) and ReadOnlyMany (ROX) using a NFS Server Provisioner. It's enabled by transparently deploying a NFS server for each Persistent Volume Claim (PVC) against a StorageClass where it's enabled, that in turn is backed by a traditional RWO claim. Most of the examples featured on SCOD are illustrated as RWO using block based storage, but many of the examples apply in the majority of use cases. Access Mode Abbreviation Use Case ReadWriteOnce RWO For high performance Pods where access to the PVC is exclusive to one Pod at a time. May use either block based storage or the NFS Server Provisioner where connectivity to the data fabric is limited to a few worker nodes in the Kubernetes cluster ReadWriteMany RWX For shared filesystems where multiple Pods in the same Namespace need simultaneous access to a PVC. ReadOnlyMany ROX Read-only representation of RWX. The NFS Server Provisioner is not enabled by the default StorageClass and needs a custom StorageClass . The following sections are tailored to help understand the NFS Server Provisioner capabilities. Using the NFS Server Provisioner NFS Server Provisioner StorageClass parameters Diagnosing the NFS Server Provisioner issues Caution The NFS Server Provisioner is currently in \"Tech Preview\" and should be considered beta software for use with non-production workloads. Enabling CSI snapshots \u00b6 Support for VolumeSnapshotClasses and VolumeSnapshots is available from Kubernetes 1.17+. The snapshot beta CRDs and the common snapshot controller needs to be installed manually. As per Kubernetes SIG Storage, these should not be installed as part of a CSI driver and should be deployed by the Kubernetes cluster vendor or user. Install snapshot beta CRDs and common snapshot controller (once per Kubernetes cluster, independent of any CSI drivers). HPE CSI Driver v1.3.0 git clone https://github.com/kubernetes-csi/external-snapshotter cd external-snapshotter git checkout release-2.0 kubectl apply -f config/crd -f deploy/kubernetes/snapshot-controller HPE CSI Driver v1.4.0-beta git clone https://github.com/kubernetes-csi/external-snapshotter cd external-snapshotter git checkout release-3.0 kubectl apply -f client/config/crd -f deploy/kubernetes/snapshot-controller Tip The provisioning section contains examples on how to create VolumeSnapshotClass and VolumeSnapshot objects. Base StorageClass parameters \u00b6 Each CSP has its own set of unique parameters to control the provisioning behavior. These examples serve as a base StorageClass example for each version of Kubernetes. See the respective CSP for more elaborate examples. K8s 1.15+ # Renamed csi.storage.k8s.io/resizer-secret-name to # csi.storage.k8s.io/controller-expand-secret-name # # Alpha features: PVC cloning and Pod inline ephemeral volumes # apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"true\" name: hpe-standard provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/controller-expand-secret-name: hpe-backend csi.storage.k8s.io/controller-expand-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete allowVolumeExpansion: true K8s 1.14 # Alpha feature: volume expansion apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"true\" name: hpe-standard provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/resizer-secret-name: hpe-backend csi.storage.k8s.io/resizer-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete # Required to allow volume expansion allowVolumeExpansion: true K8s 1.13 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"true\" name: hpe-standard provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete Important Replace \"hpe-backend\" with a Secret relevant to the backend being referenced. The example StorageClass does not work with the primera3par CSP version 1.0.0, use the example from the CSP instead. Common HPE CSI Driver StorageClass parameters across CSPs. Parameter String Description accessProtocol Text The access protocol to use when accessing the persistent volume (\"fc\" or \"iscsi\"). Default: \"iscsi\" description 1 Text Text to be added to the volume PV metadata on the backend CSP. Default: \"\" fsOwner userId:groupId The user id and group id that should own the root directory of the filesystem. fsMode Octal digits 1 to 4 octal digits that represent the file mode to be applied to the root directory of the filesystem. fsCreateOptions Text A string to be passed to the mkfs command. These flags are opaque to CSI and are therefore not validated. To protect the node, only the following characters are allowed: [a-zA-Z0-9=, \\-] . nfsResources Boolean When set to \"true\", requests against the StorageClass will create resources for the NFS Server Provisioner ( Deployment , RWO PVC and Service ). Required parameter for ReadWriteMany and ReadOnlyMany accessModes. Default: \"false\" nfsNamespace Text Resources are by default created in the \"hpe-nfs\" Namespace . If CSI VolumeSnapshotClass and dataSource functionality is required on the requesting claim, requesting and backing PVC need to exist in the requesting Namespace . nfsMountOptions Text Customize NFS mount options for the Pods to the server Deployment . Default: \"nolock, hard,vers=4\" nfsProvisionerImage Text Customize provisioner image for the server Deployment . Default: Official build from \"hpestorage/nfs-provisioner\" repo nfsResourceLimitsCpuM Text Specify CPU limits for the server Deployment in milli CPU. Default: no limits applied. Example: \"500m\" nfsResourceLimitsMemoryMi Text Specify memory limits (in megabytes) for the server Deployment . Default: no limits applied. Example: \"500Mi\" 1 = Parameter is mutable using the CSI Volume Mutator . Note All common HPE CSI Driver parameters are optional. Provisioning concepts \u00b6 These instructions are provided as an example on how to use the HPE CSI Driver with a CSP supported by HPE. Create a PersistentVolumeClaim from a StorageClass Ephemeral inline volume Raw block volume Using CSI snapshots Expanding PVCs Using PVC overrides Using volume mutations Using the NFS Server Provisioner New to Kubernetes? There's a basic tutorial of how dynamic provisioning of persistent storage on Kubernetes works in the Video Gallery . Create a PersistentVolumeClaim from a StorageClass \u00b6 The below YAML declarations are meant to be created with kubectl create . Either copy the content to a file on the host where kubectl is being executed, or copy & paste into the terminal, like this: kubectl create -f- < paste the YAML > ^D (CTRL + D) To get started, create a StorageClass API object referencing the CSI driver Secret relevant to the backend. These examples are for Kubernetes 1.15+ apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-scod provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/controller-expand-secret-name: hpe-backend csi.storage.k8s.io/controller-expand-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by the HPE CSI Driver for Kubernetes\" accessProtocol: iscsi reclaimPolicy: Delete allowVolumeExpansion: true Create a PersistentVolumeClaim . This object declaration ensures a PersistentVolume is created and provisioned on your behalf, make sure to reference the correct .spec.storageClassName : apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-first-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 32Gi storageClassName: hpe-scod Note In most environments, there is a default StorageClass declared on the cluster. In such a scenario, the .spec.storageClassName can be omitted. The default StorageClass is controlled by an annotation: .metadata.annotations.storageclass.kubernetes.io/is-default-class set to either \"true\" or \"false\" . After the PersistentVolumeClaim has been declared, check that a new PersistentVolume is created based on your claim: kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS AGE pvc-13336da3-7... 32Gi RWO Delete Bound default/my-first-pvc hpe-scod 3s The above output means that the HPE CSI Driver successfully provisioned a new volume. The volume is not attached to any node yet. It will only be attached to a node if a scheduled workload requests the PersistentVolumeClaim . Now, let us create a Pod that refers to the above volume. When the Pod is created, the volume will be attached, formatted and mounted according to the specification. kind: Pod apiVersion: v1 metadata: name: my-pod spec: containers: - name: pod-datelog-1 image: nginx command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: export1 mountPath: /data - name: pod-datelog-2 image: debian command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: export1 mountPath: /data volumes: - name: export1 persistentVolumeClaim: claimName: my-first-pvc Check if the Pod is running successfully. kubectl get pod my-pod NAME READY STATUS RESTARTS AGE my-pod 2/2 Running 0 2m29s Tip A simple Pod does not provide any automatic recovery if the node the Pod is scheduled on crashes or become unresponsive. Please see the official Kubernetes documentation for different workload types that provide automatic recovery. A shortlist of recommended workload types that are suitable for persistent storage is available in this blog post and best practices are outlined in this blog post . Ephemeral inline volume \u00b6 It's possible to declare a volume \"inline\" a Pod specification. The volume is ephemeral and only persists as long as the Pod is running. If the Pod gets rescheduled, deleted or upgraded, the volume is deleted and a new volume gets provisioned if it gets restarted. Ephemeral inline volumes are not associated with a StorageClass , hence a Secret needs to be provided inline with the volume. Warning Allowing user Pods to access the CSP Secret gives them the same privileges on the backend system as the HPE CSI Driver. There are two ways to declare the Secret with ephemeral inline volumes, either the Secret is in the same Namespace as the workload being declared or it resides in a foreign Namespace . Local Secret : apiVersion: v1 kind: Pod metadata: name: my-pod-inline-mount-1 spec: containers: - name: pod-datelog-1 image: nginx command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: my-volume-1 mountPath: /data volumes: - name: my-volume-1 csi: driver: csi.hpe.com nodePublishSecretRef: name: hpe-backend fsType: ext3 volumeAttributes: csi.storage.k8s.io/ephemeral: \"true\" accessProtocol: \"iscsi\" size: \"5Gi\" Foreign Secret : apiVersion: v1 kind: Pod metadata: name: my-pod-inline-mount-2 spec: containers: - name: pod-datelog-1 image: nginx command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: my-volume-1 mountPath: /data volumes: - name: my-volume-1 csi: driver: csi.hpe.com fsType: ext3 volumeAttributes: csi.storage.k8s.io/ephemeral: \"true\" inline-volume-secret-name: hpe-backend inline-volume-secret-namespace: kube-system accessProtocol: \"iscsi\" size: \"7Gi\" The parameters used in the examples are the bare minimum required parameters. Any parameters supported by the HPE CSI Driver and backend CSP may be used for ephemeral inline volumes. See the base StorageClass parameters or the respective CSP being used. Seealso For more elaborate use cases around ephemeral inline volumes, check out the tutorial on HPE DEV: Using Ephemeral Inline Volumes on Kubernetes Raw block volume \u00b6 The default volumeMode for a PersistentVolumeClaim is Filesystem . If a raw block volume is desired, volumeMode needs to be set to Block . No filesystem will be created. Example: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-block spec: accessModes: - ReadWriteOnce resources: requests: storage: 32Gi storageClassName: hpe-scod volumeMode: Block Mapping the device in a Pod specification is slightly different than using regular filesystems as a volumeDevices section is added instead of a volumeMounts stanza: apiVersion: v1 kind: Pod metadata: name: my-pod-block spec: containers: - name: my-null-pod image: fedora:31 command: [\"/bin/sh\", \"-c\"] args: [ \"tail -f /dev/null\" ] volumeDevices: - name: data devicePath: /dev/xvda volumes: - name: data persistentVolumeClaim: claimName: my-pvc-block Seealso There's an in-depth tutorial available on HPE DEV that covers raw block volumes: Using Raw Block Volumes on Kubernetes Using CSI snapshots \u00b6 CSI introduces snapshots as native objects in Kubernetes that allows end-users to provision VolumeSnapshot objects from an existing PersistentVolumeClaim . New PVCs may then be created using the snapshot as a source. Tip Ensure CSI snapshots are enabled . There's a tutorial in the Video Gallery on how to use CSI snapshots and clones. Start by creating a VolumeSnapshotClass referencing the Secret and defining additional snapshot parameters. Kubernetes 1.17+ (CSI snapshots in beta) HPE CSI Driver v1.3.0 apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: hpe-snapshot annotations: snapshot.storage.kubernetes.io/is-default-class: \"true\" driver: csi.hpe.com deletionPolicy: Delete parameters: description: \"Snapshot created by the HPE CSI Driver\" csi.storage.k8s.io/snapshotter-secret-name: hpe-backend csi.storage.k8s.io/snapshotter-secret-namespace: kube-system HPE CSI Driver v1.4.0-beta apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: hpe-snapshot annotations: snapshot.storage.kubernetes.io/is-default-class: \"true\" driver: csi.hpe.com deletionPolicy: Delete parameters: description: \"Snapshot created by the HPE CSI Driver\" csi.storage.k8s.io/snapshotter-secret-name: hpe-backend csi.storage.k8s.io/snapshotter-secret-namespace: kube-system csi.storage.k8s.io/snapshotter-list-secret-name: hpe-backend csi.storage.k8s.io/snapshotter-list-secret-namespace: kube-system Create a VolumeSnapshot . This will create a new snapshot of the volume. apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: my-snapshot spec: source: persistentVolumeClaimName: my-pvc Tip If a specific VolumeSnapshotClass is desired, use .spec.snapshotClassName to call it out. Check that a new VolumeSnapshot is created based on your claim: kubectl describe volumesnapshot my-snapshot Name: my-snapshot Namespace: default ... Status: Creation Time: 2019-05-22T15:51:28Z Ready: true Restore Size: 32Gi It's now possible to create a new PersistentVolumeClaim from the VolumeSnapshot . --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-from-snapshot spec: dataSource: name: my-snapshot kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 32Gi Important The size in .spec.resources.requests.storage must match the .spec.dataSource size. The .data.dataSource attribute may also clone PersistentVolumeClaim directly, without creating a VolumeSnapshot . apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-from-pvc spec: dataSource: name: my-pvc kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 32Gi Again, the size in .spec.resources.requests.storage must match the source PersistentVolumeClaim . This can get sticky from an automation perspective as volume expansion is being used on the source volume. It's recommended to inspect the source PersistentVolumeClaim or VolumeSnapshot size prior to creating a clone. Learn more For a more comprehensive tutorial on how to use snapshots and clones with CSI on Kubernetes 1.17, see HPE CSI Driver for Kubernetes: Snapshots, Clones and Volume Expansion on HPE DEV. Expanding PVCs \u00b6 To perform expansion operations on Kubernetes 1.14+, you must enhance your StorageClass with some additional attributes. Please see base StorageClass parameters . Then, a volume provisioned by a StorageClass with expansion attributes may have its PersistentVolumeClaim expanded by altering the .spec.resources.requests.storage key of the PersistentVolumeClaim . This may be done by the kubectl patch command. kubectl patch pvc/my-pvc --patch '{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"64Gi\"}}}}' persistentvolumeclaim/my-pvc patched The new PersistentVolumeClaim size may be observed with kubectl get pvc/my-pvc after a few moments. Using PVC Overrides \u00b6 The HPE CSI Driver allows the PersistentVolumeClaim to override the StorageClass parameters by annotating the PersistentVolumeClaim . Define the parameters allowed to be overridden in the StorageClass by setting the allowOverrides parameter: apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-scod-override provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system description: \"Volume provisioned by the HPE CSI Driver\" accessProtocol: iscsi allowOverrides: description,accessProtocol The end-user may now control those parameters (the StorageClass provides the default values). apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-override annotations: csi.hpe.com/description: \"This is my custom description\" csi.hpe.com/accessProtocol: fc spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: hpe-scod-override Using volume mutations \u00b6 The HPE CSI Driver (version 1.3.0 and later) allows the CSP backend volume to be mutated by annotating the PersistentVolumeClaim . Define the parameters allowed to be mutated in the StorageClass by setting the allowMutations parameter. In the StorageClass , also make sure that the csi.storage.k8s.io/controller-provisioner-secret-name and csi.storage.k8s.io/controller-provisioner-secret-namespace are set, as those are used by the csi-extensions and csi-volume-mutator sidecars. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-scod-mutation provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system description: \"Volume provisioned by the HPE CSI Driver\" allowMutations: description Note The allowMutations parameter is a comma separated list of values defined by each of the CSPs parameters, except the description parameter, which is common across all CSPs. See the documentation for each CSP on what parameters are mutable. The end-user may now control those parameters by editing or patching the PersistentVolumeClaim . apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-mutation annotations: csi.hpe.com/description: \"My description needs to change\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: hpe-scod-mutation Good to know As the .spec.csi.volumeAttributes on the PersistentVolume are immutable, the mutations performed on the backend volume are also annotated on the PersistentVolume object. Using the NFS Server Provisioner \u00b6 Enabling the NFS Server Provisioner to allow RWX and ROX access mode for a PVC is straightforward. Create a new StorageClass and set .parameters.nfsResources to \"true\" . Any subsequent claim to the StorageClass will create a NFS server Deployment on the cluster with the associated objects running on top of a RWO PVC. Any RWO claim made against the StorageClass will also create a NFS server Deployment . This allows diverse connectivity options among the Kubernetes worker nodes as the HPE CSI Driver will look for nodes labelled csi.hpe.com/hpe-nfs=true before submitting the workload for scheduling. This allows dedicated NFS worker nodes without user workloads using taints and tolerations. By default, the NFS Server Provisioner deploy resources in the \"hpe-nfs\" Namespace . This makes it easy to manage and diagnose. However, to use CSI data management capabilities on the PVCs, the NFS resources need to be deployed in the same Namespace as the RWX/ROX requesting PVC. This is controlled by the nfsNamespace StorageClass parameter. See base StorageClass parameters for more information. Tip A comprehensive tutorial is available on HPE DEV on how to get started with the NFS Server Provisioner and the HPE CSI Driver for Kubernetes. Example use of accessModes : ReadWriteOnce apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-rwo-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 32Gi storageClassName: hpe-nfs ReadWriteMany apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-rwx-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 32Gi storageClassName: hpe-nfs ReadOnlyMany apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-rox-pvc spec: accessModes: - ReadOnlyMany resources: requests: storage: 32Gi storageClassName: hpe-nfs In the case of declaring a ROX PVC, the requesting Pod specification needs to request the PVC as read-only. Example: apiVersion: v1 kind: Pod metadata: name: pod-rox spec: containers: - image: busybox name: busybox command: - \"sleep\" - \"300\" volumeMounts: - mountPath: /data name: my-vol readOnly: true volumes: - name: my-vol persistentVolumeClaim: claimName: my-rox-pvc readOnly: true Requesting an empty read-only volume might not seem practical. The primary use case is to source existing datasets into immutable applications, using either a backend CSP cloning capability or CSI data management feature such as snapshots or existing PVCs . Good to know The NFS Server Provisioner is currently in beta. More elaborate deployment architectures, documentation and examples will become available in time for General Availability (GA). Limitations and considerations for the NFS Server Provisioner \u00b6 The current hardcoded limit for the NFS Server Provisioner is 20 NFS servers per Kubernetes worker node. The NFS server Deployment is currently setup in a completely unfettered resource mode where it will consume as much memory and CPU as it requests. The two StorageClass parameters nfsResourceLimitsCpuM and nfsResourceLimitsMemoryMi control how much CPU and memory it may consume. Tests show that the NFS server consume about 150MiB at instantiation. These parameters will have defaults ready for GA. The HPE CSI Driver now also incorporates a Pod Monitor to delete Pods that have become unavailable due to the Pod status changing to NodeLost or a node becoming unreachable that the Pod runs on. Be default the Pod Monitor only watches the NFS Server Provisioner Deployments . It may be used for any Deployment . See Pod Monitor on how to use it. See diagnosing NFS Server Provisioner issues for further details. Further reading \u00b6 The official Kubernetes documentation contains comprehensive documentation on how to markup PersistentVolumeClaim and StorageClass API objects to tweak certain behaviors. Each CSP has a set of unique StorageClass parameters that may be tweaked to accommodate a wide variety of use cases. Please see the documentation of the respective CSP for more details .","title":"Using"},{"location":"csi_driver/using.html#overview","text":"At this point the CSI driver and CSP should be installed and configured. Important Most examples below assumes there's a Secret named \"hpe-backend\" in the \"kube-system\" Namespace . Learn how to add Secrets in the Deployment section . Overview PVC access modes Enabling CSI snapshots Base StorageClass parameters Provisioning concepts Create a PersistentVolumeClaim from a StorageClass Ephemeral inline volume Raw block volume Using CSI snapshots Expanding PVCs Using PVC Overrides Using volume mutations Using the NFS Server Provisioner Limitations and considerations for the NFS Server Provisioner Further reading Tip If you're familiar with the basic concepts of persistent storage on Kubernetes and are looking for an overview of example YAML declarations for different object types supported by the HPE CSI driver, visit the source code repo on GitHub.","title":"Overview"},{"location":"csi_driver/using.html#pvc_access_modes","text":"The HPE CSI Driver for Kubernetes is primarily a ReadWriteOnce (RWO) CSI implementation for block based storage. The CSI driver also supports ReadWriteMany (RWX) and ReadOnlyMany (ROX) using a NFS Server Provisioner. It's enabled by transparently deploying a NFS server for each Persistent Volume Claim (PVC) against a StorageClass where it's enabled, that in turn is backed by a traditional RWO claim. Most of the examples featured on SCOD are illustrated as RWO using block based storage, but many of the examples apply in the majority of use cases. Access Mode Abbreviation Use Case ReadWriteOnce RWO For high performance Pods where access to the PVC is exclusive to one Pod at a time. May use either block based storage or the NFS Server Provisioner where connectivity to the data fabric is limited to a few worker nodes in the Kubernetes cluster ReadWriteMany RWX For shared filesystems where multiple Pods in the same Namespace need simultaneous access to a PVC. ReadOnlyMany ROX Read-only representation of RWX. The NFS Server Provisioner is not enabled by the default StorageClass and needs a custom StorageClass . The following sections are tailored to help understand the NFS Server Provisioner capabilities. Using the NFS Server Provisioner NFS Server Provisioner StorageClass parameters Diagnosing the NFS Server Provisioner issues Caution The NFS Server Provisioner is currently in \"Tech Preview\" and should be considered beta software for use with non-production workloads.","title":"PVC access modes"},{"location":"csi_driver/using.html#enabling_csi_snapshots","text":"Support for VolumeSnapshotClasses and VolumeSnapshots is available from Kubernetes 1.17+. The snapshot beta CRDs and the common snapshot controller needs to be installed manually. As per Kubernetes SIG Storage, these should not be installed as part of a CSI driver and should be deployed by the Kubernetes cluster vendor or user. Install snapshot beta CRDs and common snapshot controller (once per Kubernetes cluster, independent of any CSI drivers). HPE CSI Driver v1.3.0 git clone https://github.com/kubernetes-csi/external-snapshotter cd external-snapshotter git checkout release-2.0 kubectl apply -f config/crd -f deploy/kubernetes/snapshot-controller HPE CSI Driver v1.4.0-beta git clone https://github.com/kubernetes-csi/external-snapshotter cd external-snapshotter git checkout release-3.0 kubectl apply -f client/config/crd -f deploy/kubernetes/snapshot-controller Tip The provisioning section contains examples on how to create VolumeSnapshotClass and VolumeSnapshot objects.","title":"Enabling CSI snapshots"},{"location":"csi_driver/using.html#base_storageclass_parameters","text":"Each CSP has its own set of unique parameters to control the provisioning behavior. These examples serve as a base StorageClass example for each version of Kubernetes. See the respective CSP for more elaborate examples. K8s 1.15+ # Renamed csi.storage.k8s.io/resizer-secret-name to # csi.storage.k8s.io/controller-expand-secret-name # # Alpha features: PVC cloning and Pod inline ephemeral volumes # apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"true\" name: hpe-standard provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/controller-expand-secret-name: hpe-backend csi.storage.k8s.io/controller-expand-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete allowVolumeExpansion: true K8s 1.14 # Alpha feature: volume expansion apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"true\" name: hpe-standard provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/resizer-secret-name: hpe-backend csi.storage.k8s.io/resizer-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete # Required to allow volume expansion allowVolumeExpansion: true K8s 1.13 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"true\" name: hpe-standard provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by the HPE CSI Driver for Kubernetes\" reclaimPolicy: Delete Important Replace \"hpe-backend\" with a Secret relevant to the backend being referenced. The example StorageClass does not work with the primera3par CSP version 1.0.0, use the example from the CSP instead. Common HPE CSI Driver StorageClass parameters across CSPs. Parameter String Description accessProtocol Text The access protocol to use when accessing the persistent volume (\"fc\" or \"iscsi\"). Default: \"iscsi\" description 1 Text Text to be added to the volume PV metadata on the backend CSP. Default: \"\" fsOwner userId:groupId The user id and group id that should own the root directory of the filesystem. fsMode Octal digits 1 to 4 octal digits that represent the file mode to be applied to the root directory of the filesystem. fsCreateOptions Text A string to be passed to the mkfs command. These flags are opaque to CSI and are therefore not validated. To protect the node, only the following characters are allowed: [a-zA-Z0-9=, \\-] . nfsResources Boolean When set to \"true\", requests against the StorageClass will create resources for the NFS Server Provisioner ( Deployment , RWO PVC and Service ). Required parameter for ReadWriteMany and ReadOnlyMany accessModes. Default: \"false\" nfsNamespace Text Resources are by default created in the \"hpe-nfs\" Namespace . If CSI VolumeSnapshotClass and dataSource functionality is required on the requesting claim, requesting and backing PVC need to exist in the requesting Namespace . nfsMountOptions Text Customize NFS mount options for the Pods to the server Deployment . Default: \"nolock, hard,vers=4\" nfsProvisionerImage Text Customize provisioner image for the server Deployment . Default: Official build from \"hpestorage/nfs-provisioner\" repo nfsResourceLimitsCpuM Text Specify CPU limits for the server Deployment in milli CPU. Default: no limits applied. Example: \"500m\" nfsResourceLimitsMemoryMi Text Specify memory limits (in megabytes) for the server Deployment . Default: no limits applied. Example: \"500Mi\" 1 = Parameter is mutable using the CSI Volume Mutator . Note All common HPE CSI Driver parameters are optional.","title":"Base StorageClass parameters"},{"location":"csi_driver/using.html#provisioning_concepts","text":"These instructions are provided as an example on how to use the HPE CSI Driver with a CSP supported by HPE. Create a PersistentVolumeClaim from a StorageClass Ephemeral inline volume Raw block volume Using CSI snapshots Expanding PVCs Using PVC overrides Using volume mutations Using the NFS Server Provisioner New to Kubernetes? There's a basic tutorial of how dynamic provisioning of persistent storage on Kubernetes works in the Video Gallery .","title":"Provisioning concepts"},{"location":"csi_driver/using.html#create_a_persistentvolumeclaim_from_a_storageclass","text":"The below YAML declarations are meant to be created with kubectl create . Either copy the content to a file on the host where kubectl is being executed, or copy & paste into the terminal, like this: kubectl create -f- < paste the YAML > ^D (CTRL + D) To get started, create a StorageClass API object referencing the CSI driver Secret relevant to the backend. These examples are for Kubernetes 1.15+ apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-scod provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/controller-expand-secret-name: hpe-backend csi.storage.k8s.io/controller-expand-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system description: \"Volume created by the HPE CSI Driver for Kubernetes\" accessProtocol: iscsi reclaimPolicy: Delete allowVolumeExpansion: true Create a PersistentVolumeClaim . This object declaration ensures a PersistentVolume is created and provisioned on your behalf, make sure to reference the correct .spec.storageClassName : apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-first-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 32Gi storageClassName: hpe-scod Note In most environments, there is a default StorageClass declared on the cluster. In such a scenario, the .spec.storageClassName can be omitted. The default StorageClass is controlled by an annotation: .metadata.annotations.storageclass.kubernetes.io/is-default-class set to either \"true\" or \"false\" . After the PersistentVolumeClaim has been declared, check that a new PersistentVolume is created based on your claim: kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS AGE pvc-13336da3-7... 32Gi RWO Delete Bound default/my-first-pvc hpe-scod 3s The above output means that the HPE CSI Driver successfully provisioned a new volume. The volume is not attached to any node yet. It will only be attached to a node if a scheduled workload requests the PersistentVolumeClaim . Now, let us create a Pod that refers to the above volume. When the Pod is created, the volume will be attached, formatted and mounted according to the specification. kind: Pod apiVersion: v1 metadata: name: my-pod spec: containers: - name: pod-datelog-1 image: nginx command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: export1 mountPath: /data - name: pod-datelog-2 image: debian command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: export1 mountPath: /data volumes: - name: export1 persistentVolumeClaim: claimName: my-first-pvc Check if the Pod is running successfully. kubectl get pod my-pod NAME READY STATUS RESTARTS AGE my-pod 2/2 Running 0 2m29s Tip A simple Pod does not provide any automatic recovery if the node the Pod is scheduled on crashes or become unresponsive. Please see the official Kubernetes documentation for different workload types that provide automatic recovery. A shortlist of recommended workload types that are suitable for persistent storage is available in this blog post and best practices are outlined in this blog post .","title":"Create a PersistentVolumeClaim from a StorageClass"},{"location":"csi_driver/using.html#ephemeral_inline_volume","text":"It's possible to declare a volume \"inline\" a Pod specification. The volume is ephemeral and only persists as long as the Pod is running. If the Pod gets rescheduled, deleted or upgraded, the volume is deleted and a new volume gets provisioned if it gets restarted. Ephemeral inline volumes are not associated with a StorageClass , hence a Secret needs to be provided inline with the volume. Warning Allowing user Pods to access the CSP Secret gives them the same privileges on the backend system as the HPE CSI Driver. There are two ways to declare the Secret with ephemeral inline volumes, either the Secret is in the same Namespace as the workload being declared or it resides in a foreign Namespace . Local Secret : apiVersion: v1 kind: Pod metadata: name: my-pod-inline-mount-1 spec: containers: - name: pod-datelog-1 image: nginx command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: my-volume-1 mountPath: /data volumes: - name: my-volume-1 csi: driver: csi.hpe.com nodePublishSecretRef: name: hpe-backend fsType: ext3 volumeAttributes: csi.storage.k8s.io/ephemeral: \"true\" accessProtocol: \"iscsi\" size: \"5Gi\" Foreign Secret : apiVersion: v1 kind: Pod metadata: name: my-pod-inline-mount-2 spec: containers: - name: pod-datelog-1 image: nginx command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: my-volume-1 mountPath: /data volumes: - name: my-volume-1 csi: driver: csi.hpe.com fsType: ext3 volumeAttributes: csi.storage.k8s.io/ephemeral: \"true\" inline-volume-secret-name: hpe-backend inline-volume-secret-namespace: kube-system accessProtocol: \"iscsi\" size: \"7Gi\" The parameters used in the examples are the bare minimum required parameters. Any parameters supported by the HPE CSI Driver and backend CSP may be used for ephemeral inline volumes. See the base StorageClass parameters or the respective CSP being used. Seealso For more elaborate use cases around ephemeral inline volumes, check out the tutorial on HPE DEV: Using Ephemeral Inline Volumes on Kubernetes","title":"Ephemeral inline volume"},{"location":"csi_driver/using.html#raw_block_volume","text":"The default volumeMode for a PersistentVolumeClaim is Filesystem . If a raw block volume is desired, volumeMode needs to be set to Block . No filesystem will be created. Example: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-block spec: accessModes: - ReadWriteOnce resources: requests: storage: 32Gi storageClassName: hpe-scod volumeMode: Block Mapping the device in a Pod specification is slightly different than using regular filesystems as a volumeDevices section is added instead of a volumeMounts stanza: apiVersion: v1 kind: Pod metadata: name: my-pod-block spec: containers: - name: my-null-pod image: fedora:31 command: [\"/bin/sh\", \"-c\"] args: [ \"tail -f /dev/null\" ] volumeDevices: - name: data devicePath: /dev/xvda volumes: - name: data persistentVolumeClaim: claimName: my-pvc-block Seealso There's an in-depth tutorial available on HPE DEV that covers raw block volumes: Using Raw Block Volumes on Kubernetes","title":"Raw block volume"},{"location":"csi_driver/using.html#using_csi_snapshots","text":"CSI introduces snapshots as native objects in Kubernetes that allows end-users to provision VolumeSnapshot objects from an existing PersistentVolumeClaim . New PVCs may then be created using the snapshot as a source. Tip Ensure CSI snapshots are enabled . There's a tutorial in the Video Gallery on how to use CSI snapshots and clones. Start by creating a VolumeSnapshotClass referencing the Secret and defining additional snapshot parameters. Kubernetes 1.17+ (CSI snapshots in beta) HPE CSI Driver v1.3.0 apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: hpe-snapshot annotations: snapshot.storage.kubernetes.io/is-default-class: \"true\" driver: csi.hpe.com deletionPolicy: Delete parameters: description: \"Snapshot created by the HPE CSI Driver\" csi.storage.k8s.io/snapshotter-secret-name: hpe-backend csi.storage.k8s.io/snapshotter-secret-namespace: kube-system HPE CSI Driver v1.4.0-beta apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: hpe-snapshot annotations: snapshot.storage.kubernetes.io/is-default-class: \"true\" driver: csi.hpe.com deletionPolicy: Delete parameters: description: \"Snapshot created by the HPE CSI Driver\" csi.storage.k8s.io/snapshotter-secret-name: hpe-backend csi.storage.k8s.io/snapshotter-secret-namespace: kube-system csi.storage.k8s.io/snapshotter-list-secret-name: hpe-backend csi.storage.k8s.io/snapshotter-list-secret-namespace: kube-system Create a VolumeSnapshot . This will create a new snapshot of the volume. apiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: my-snapshot spec: source: persistentVolumeClaimName: my-pvc Tip If a specific VolumeSnapshotClass is desired, use .spec.snapshotClassName to call it out. Check that a new VolumeSnapshot is created based on your claim: kubectl describe volumesnapshot my-snapshot Name: my-snapshot Namespace: default ... Status: Creation Time: 2019-05-22T15:51:28Z Ready: true Restore Size: 32Gi It's now possible to create a new PersistentVolumeClaim from the VolumeSnapshot . --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-from-snapshot spec: dataSource: name: my-snapshot kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 32Gi Important The size in .spec.resources.requests.storage must match the .spec.dataSource size. The .data.dataSource attribute may also clone PersistentVolumeClaim directly, without creating a VolumeSnapshot . apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-from-pvc spec: dataSource: name: my-pvc kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 32Gi Again, the size in .spec.resources.requests.storage must match the source PersistentVolumeClaim . This can get sticky from an automation perspective as volume expansion is being used on the source volume. It's recommended to inspect the source PersistentVolumeClaim or VolumeSnapshot size prior to creating a clone. Learn more For a more comprehensive tutorial on how to use snapshots and clones with CSI on Kubernetes 1.17, see HPE CSI Driver for Kubernetes: Snapshots, Clones and Volume Expansion on HPE DEV.","title":"Using CSI snapshots"},{"location":"csi_driver/using.html#expanding_pvcs","text":"To perform expansion operations on Kubernetes 1.14+, you must enhance your StorageClass with some additional attributes. Please see base StorageClass parameters . Then, a volume provisioned by a StorageClass with expansion attributes may have its PersistentVolumeClaim expanded by altering the .spec.resources.requests.storage key of the PersistentVolumeClaim . This may be done by the kubectl patch command. kubectl patch pvc/my-pvc --patch '{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"64Gi\"}}}}' persistentvolumeclaim/my-pvc patched The new PersistentVolumeClaim size may be observed with kubectl get pvc/my-pvc after a few moments.","title":"Expanding PVCs"},{"location":"csi_driver/using.html#using_pvc_overrides","text":"The HPE CSI Driver allows the PersistentVolumeClaim to override the StorageClass parameters by annotating the PersistentVolumeClaim . Define the parameters allowed to be overridden in the StorageClass by setting the allowOverrides parameter: apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-scod-override provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system description: \"Volume provisioned by the HPE CSI Driver\" accessProtocol: iscsi allowOverrides: description,accessProtocol The end-user may now control those parameters (the StorageClass provides the default values). apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-override annotations: csi.hpe.com/description: \"This is my custom description\" csi.hpe.com/accessProtocol: fc spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: hpe-scod-override","title":"Using PVC Overrides"},{"location":"csi_driver/using.html#using_volume_mutations","text":"The HPE CSI Driver (version 1.3.0 and later) allows the CSP backend volume to be mutated by annotating the PersistentVolumeClaim . Define the parameters allowed to be mutated in the StorageClass by setting the allowMutations parameter. In the StorageClass , also make sure that the csi.storage.k8s.io/controller-provisioner-secret-name and csi.storage.k8s.io/controller-provisioner-secret-namespace are set, as those are used by the csi-extensions and csi-volume-mutator sidecars. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-scod-mutation provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/provisioner-secret-name: hpe-backend csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: hpe-backend csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: hpe-backend csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: hpe-backend csi.storage.k8s.io/node-publish-secret-namespace: kube-system description: \"Volume provisioned by the HPE CSI Driver\" allowMutations: description Note The allowMutations parameter is a comma separated list of values defined by each of the CSPs parameters, except the description parameter, which is common across all CSPs. See the documentation for each CSP on what parameters are mutable. The end-user may now control those parameters by editing or patching the PersistentVolumeClaim . apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-mutation annotations: csi.hpe.com/description: \"My description needs to change\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: hpe-scod-mutation Good to know As the .spec.csi.volumeAttributes on the PersistentVolume are immutable, the mutations performed on the backend volume are also annotated on the PersistentVolume object.","title":"Using volume mutations"},{"location":"csi_driver/using.html#using_the_nfs_server_provisioner","text":"Enabling the NFS Server Provisioner to allow RWX and ROX access mode for a PVC is straightforward. Create a new StorageClass and set .parameters.nfsResources to \"true\" . Any subsequent claim to the StorageClass will create a NFS server Deployment on the cluster with the associated objects running on top of a RWO PVC. Any RWO claim made against the StorageClass will also create a NFS server Deployment . This allows diverse connectivity options among the Kubernetes worker nodes as the HPE CSI Driver will look for nodes labelled csi.hpe.com/hpe-nfs=true before submitting the workload for scheduling. This allows dedicated NFS worker nodes without user workloads using taints and tolerations. By default, the NFS Server Provisioner deploy resources in the \"hpe-nfs\" Namespace . This makes it easy to manage and diagnose. However, to use CSI data management capabilities on the PVCs, the NFS resources need to be deployed in the same Namespace as the RWX/ROX requesting PVC. This is controlled by the nfsNamespace StorageClass parameter. See base StorageClass parameters for more information. Tip A comprehensive tutorial is available on HPE DEV on how to get started with the NFS Server Provisioner and the HPE CSI Driver for Kubernetes. Example use of accessModes : ReadWriteOnce apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-rwo-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 32Gi storageClassName: hpe-nfs ReadWriteMany apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-rwx-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 32Gi storageClassName: hpe-nfs ReadOnlyMany apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-rox-pvc spec: accessModes: - ReadOnlyMany resources: requests: storage: 32Gi storageClassName: hpe-nfs In the case of declaring a ROX PVC, the requesting Pod specification needs to request the PVC as read-only. Example: apiVersion: v1 kind: Pod metadata: name: pod-rox spec: containers: - image: busybox name: busybox command: - \"sleep\" - \"300\" volumeMounts: - mountPath: /data name: my-vol readOnly: true volumes: - name: my-vol persistentVolumeClaim: claimName: my-rox-pvc readOnly: true Requesting an empty read-only volume might not seem practical. The primary use case is to source existing datasets into immutable applications, using either a backend CSP cloning capability or CSI data management feature such as snapshots or existing PVCs . Good to know The NFS Server Provisioner is currently in beta. More elaborate deployment architectures, documentation and examples will become available in time for General Availability (GA).","title":"Using the NFS Server Provisioner"},{"location":"csi_driver/using.html#limitations_and_considerations_for_the_nfs_server_provisioner","text":"The current hardcoded limit for the NFS Server Provisioner is 20 NFS servers per Kubernetes worker node. The NFS server Deployment is currently setup in a completely unfettered resource mode where it will consume as much memory and CPU as it requests. The two StorageClass parameters nfsResourceLimitsCpuM and nfsResourceLimitsMemoryMi control how much CPU and memory it may consume. Tests show that the NFS server consume about 150MiB at instantiation. These parameters will have defaults ready for GA. The HPE CSI Driver now also incorporates a Pod Monitor to delete Pods that have become unavailable due to the Pod status changing to NodeLost or a node becoming unreachable that the Pod runs on. Be default the Pod Monitor only watches the NFS Server Provisioner Deployments . It may be used for any Deployment . See Pod Monitor on how to use it. See diagnosing NFS Server Provisioner issues for further details.","title":"Limitations and considerations for the NFS Server Provisioner"},{"location":"csi_driver/using.html#further_reading","text":"The official Kubernetes documentation contains comprehensive documentation on how to markup PersistentVolumeClaim and StorageClass API objects to tweak certain behaviors. Each CSP has a set of unique StorageClass parameters that may be tweaked to accommodate a wide variety of use cases. Please see the documentation of the respective CSP for more details .","title":"Further reading"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html","text":"Introduction \u00b6 This is the documentation for HPE Cloud Volumes Plugin for Docker . It allows dynamic provisioning of Docker Volumes on standalone Docker Engine or Docker Swarm nodes. Introduction Requirements Limitations Installation Plugin privileges Host configuration and installation Making changes Configuration files and options Node fencing Usage Create a Docker Volume Clone a Docker Volume Provisioning Docker Volumes Import a Volume to Docker Import a volume snapshot to Docker Restore an offline Docker Volume with specified snapshot List volumes Remove a Docker Volume Uninstall Troubleshooting Log file location Requirements \u00b6 Docker Engine 17.09 or greater If using Docker Enterprise Edition 2.x, the plugin is only supported in swarmmode Recent Red Hat, Debian or Ubuntu-based Linux distribution US regions only Plugin Release Notes 3.1.0 v3.1.0 Note Docker does not support certified and managed Docker Volume plugins with Docker EE Kubernetes. If you want to use Kubernetes on Docker EE with HPE Nimble Storage, please use the HPE Volume Driver for Kubernetes FlexVolume Plugin or the HPE CSI Driver for Kubernetes depending on your situation. Limitations \u00b6 HPE Cloud Volumes provides a Docker certified plugin delivered through the Docker Store. Certain features and capabilities are not available through the managed plugin. Please understand these limitations before deploying either of these plugins. The managed plugin does NOT provide: Support for Docker's release of Kubernetes in Docker Enterprise Edition 2.x Support for Windows Containers The managed plugin does provide a simple way to manage HPE Cloud Volumes integration on your Docker instances using Docker's interface to install and manage the plugin. Installation \u00b6 Plugin privileges \u00b6 In order to create connections, attach devices and mount file systems, the plugin requires more privileges than a standard application container. These privileges are enumerated during installation. These permissions need to be granted for the plugin to operate correctly. Plugin \"cvblock\" is requesting the following privileges: - network: [host] - mount: [/dev] - mount: [/run/lock] - mount: [/sys] - mount: [/etc] - mount: [/var/lib] - mount: [/var/run/docker.sock] - mount: [/sbin/iscsiadm] - mount: [/lib/modules] - mount: [/usr/lib64] - allow-all-devices: [true] - capabilities: [CAP_SYS_ADMIN CAP_SYS_MODULE CAP_MKNOD] Host configuration and installation \u00b6 Setting up the plugin varies between Linux distributions. These procedures requires root privileges on the cloud instance. Red Hat 7.5+, CentOS 7.5+: yum install -y iscsi-initiator-utils device-mapper-multipath docker plugin install --disable --grant-all-permissions --alias cvblock store/hpestorage/cvblock:3.1.0 docker plugin set cv PROVIDER_IP=cloudvolumes.hpe.com PROVIDER_USERNAME=<access_key> PROVIDER_PASSWORD=<access_secret> docker plugin enable cvblock systemctl daemon-reload systemctl enable iscsid multipathd systemctl start iscsid multipathd Ubuntu 16.04 LTS and Ubuntu 18.04 LTS: apt-get install -y open-iscsi multipath-tools xfsprogs modprobe xfs sed -i\"\" -e \"\\$axfs\" /etc/modules docker plugin install --disable --grant-all-permissions --alias cvblock store/hpestorage/cvblock:3.1.0 docker plugin set cv PROVIDER_IP=cloudvolumes.hpe.com PROVIDER_USERNAME=<access_key> PROVIDER_PASSWORD=<access_secret> glibc_libs.source=/lib/x86_64-linux-gnu docker plugin enable cvblock systemctl daemon-reload systemctl restart open-iscsi multipath-tools Debian 9.x (stable): apt-get install -y open-iscsi multipath-tools xfsprogs modprobe xfs sed -i\"\" -e \"\\$axfs\" /etc/modules docker plugin install --disable --grant-all-permissions --alias cvblock store/hpestorage/cvblock:3.1.0 docker plugin set cv PROVIDER_IP=cloudvolumes.hpe.com PROVIDER_USERNAME=<access_key> PROVIDER_PASSWORD=<access_secret> iscsiadm.source=/usr/bin/iscsiadm glibc_libs.source=/lib/x86_64-linux-gnu docker plugin enable cvblock systemctl daemon-reload systemctl restart open-iscsi multipath-tools Making changes \u00b6 The docker plugin set command can only be used on the plugin if it is disabled. To disable the plugin, use the docker plugin disable command. For example: docker plugin disable cvblock List of parameters which are supported to be settable by the plugin Parameter Description Default PROVIDER_IP HPE Cloud Volumes portal \"\" PROVIDER_USERNAME HPE Cloud Volumes username \"\" PROVIDER_PASSWORD HPE Cloud Volumes password \"\" PROVIDER_REMOVE Unassociate Plugin from HPE Cloud Volumes false LOG_LEVEL Log level of the plugin ( info , debug , or trace ) debug SCOPE Scope of the plugin ( global or local ) global In the event of reassociating the plugin with a different HPE Cloud Volumes portal, certain procedures need to be followed: Disable the plugin docker plugin disable cvblock Set new paramters docker plugin set cvblock PROVIDER_REMOVE=true Enable the plugin docker plugin enable cvblock Disable the plugin docker plugin disable cvblock The plugin is now ready for re-configuration docker plugin set cvblock PROVIDER_IP=< New portal address > PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin PROVIDER_REMOVE=false Note The PROVIDER_REMOVE=false parameter must be set if the plugin ever has been unassociated from a HPE Cloud Volumes portal. Configuration files and options \u00b6 The configuration directory for the plugin is /etc/hpe-storage on the host. Files in this directory are preserved between plugin upgrades. The /etc/hpe-storage/volume-driver.json file contains three sections, global , defaults and overrides . The global options are plugin runtime parameters and doesn't have any end-user configurable keys at this time. The defaults map allows the docker host administrator to set default options during volume creation. The docker user may override these default options with their own values for a specific option. The overrides map allows the docker host administrator to enforce a certain option for every volume creation. The docker user may not override the option and any attempt to do so will be silently ignored. Note defaults and overrides are dynamically read during runtime while global changes require a plugin restart. Example config file in /etc/hpe-storage/volume-driver.json : { \"global\": { \"snapPrefix\": \"BaseFor\", \"initiators\": [\"eth0\"], \"automatedConnection\": true, \"existingCloudSubnet\": \"10.1.0.0/24\", \"region\": \"us-east-1\", \"privateCloud\": \"vpc-data\", \"cloudComputeProvider\": \"Amazon AWS\" }, \"defaults\": { \"limitIOPS\": 1000, \"fsOwner\": \"0:0\", \"fsMode\": \"600\", \"description\": \"Volume provisioned by the HPE Volume Driver for Kubernetes FlexVolume Plugin\", \"perfPolicy\": \"Other\", \"protectionTemplate\": \"twicedaily:4\", \"encryption\": true, \"volumeType\": \"PF\", \"destroyOnRm\": true }, \"overrides\": { } } For an exhaustive list of options use the help option from the docker CLI: $ docker volume create -d cvblock -o help Node fencing \u00b6 If you are considering using any Docker clustering technologies for your Docker deployment, it is important to understand the fencing mechanism used to protect data. Attaching the same Docker Volume to multiple containers on the same host is fully supported. Mounting the same volume on multiple hosts is not supported. Docker does not provide a fencing mechanism for nodes that have become disconnected from the Docker Swarm. This results in the isolated nodes continuing to run their containers. When the containers are rescheduled on a surviving node, the Docker Engine will request that the Docker Volume(s) be mounted. In order to prevent data corruption, the Docker Volume Plugin will stop serving the Docker Volume to the original node before mounting it on the newly requested node. During a mount request, the Docker Volume Plugin inspects the ACR (Access Control Record) on the volume. If the ACR does not match the initiator requesting to mount the volume, the ACR is removed and the volume taken offline. The volume is now fenced off and other nodes are unable to access any data in the volume. The volume then receives a new ACR matching the requesting initiator, and it is mounted for the container requesting the volume. This is done because the volumes are formatted with XFS, which is not a clustered filesystem and can be corrupted if the same volume is mounted to multiple hosts. The side effect of a fenced node is that I/O hangs indefinitely, and the initiator is rejected during login. If the fenced node rejoins the Docker Swarm using Docker SwarmKit, the swarm tries to shut down the services that were rescheduled elsewhere to maintain the desired replica set for the service. This operation will also hang indefinitely waiting for I/O. We recommend running a dedicated Docker host that does not host any other critical applications besides the Docker Engine. Doing this supports a safe way to reboot a node after a grace period and have it start cleanly when a hung task is detected. Otherwise, the node can remain in the hung state indefinitely. The following kernel parameters control the system behavior when a hung task is detected: # Reset after these many seconds after a panic kernel.panic = 5 # I do consider hung tasks reason enough to panic kernel.hung_task_panic = 1 # To not panic in vain, I'll wait these many seconds before I declare a hung task kernel.hung_task_timeout_secs = 150 Add these parameters to the /etc/sysctl.d/99-hung_task_timeout.conf file and reboot the system. Important Docker SwarmKit declares a node as failed after five (5) seconds. Services are then rescheduled and up and running again in less than ten (10) seconds. The parameters noted above provide the system a way to manage other tasks that may appear to be hung and avoid a system panic. Usage \u00b6 These are some basic examples on how to use the HPE Cloud Volumes Plugin for Docker. Create a Docker Volume \u00b6 Using docker volume create . Note The plugin applies a set of default options when you create new volumes unless you override them using the volume create -o key=value option flags. Create a Docker volume with a custom description: docker volume create -d cvblock -o description=\"My volume description\" --name myvol1 (Optional) Inspect the new volume: docker volume inspect myvol1 (Optional) Attach the volume to an interactive container. docker run -it --rm -v myvol1:/data bash The volume is mounted inside the container on /data . Clone a Docker Volume \u00b6 Use the docker volume create command with the cloneOf option to clone a Docker volume to a new Docker volume. Clone the Docker volume named myvol1 to a new Docker volume named myvol1-clone . docker volume create -d cvblock -o cloneOf=myvol1 --name=myvol1-clone (Optional) Select a snapshot on which to base the clone. docker volume create -d cvblock -o snapshot=mysnap1 -o cloneOf=myvol1 --name=myvol2-clone Provisioning Docker Volumes \u00b6 There are several ways to provision a Docker volume depending on what tools are used: Docker Engine (CLI) Docker Compose file with either Docker UCP or Docker Engine The Docker Volume plugin leverages the existing Docker CLI and APIs, therefor all native Docker tools may be used to provision a volume. Note The plugin applies a set of default volume create options. Unless you override the default options using the volume option flags, the defaults are applied when you create volumes. For example, the default volume size is 10GiB. Config file volume-driver.json , which is stored at /etc/hpe-storage/volume-driver.json : { \"global\": {}, \"defaults\": { \"sizeInGiB\":\"10\", \"limitIOPS\":\"-1\", \"limitMBPS\":\"-1\", \"perfPolicy\": \"DockerDefault\", }, \"overrides\":{} } Import a Volume to Docker \u00b6 Take the volume you want to import offline before importing it. For information about how to take a volume offline, refer to the HPE Cloud Volumes documentation . Use the create command with the importVol option to import an HPE Cloud Volume to Docker and name it. Import the HPE Cloud Volume named mycloudvol as a Docker volume named myvol3-imported . docker volume create \u2013d cvblock -o importVol=mycloudvol --name=myvol3-imported Import a volume snapshot to Docker \u00b6 Use the create command with the importVolAsClone option to import a HPE Cloud Volume snapshot as a Docker volume. Optionally, specify a particular snapshot on the HPE Cloud Volume using the snapshot option. Import the HPE Cloud Volumes snapshot aSnapshot on the volume importMe as a Docker volume named importedSnap . docker volume create -d cvblock -o importVolAsClone=mycloudvol -o snapshot=mysnap1 --name=myvol4-clone Note If no snapshot is specified, the latest snapshot on the volume is imported. Restore an offline Docker Volume with specified snapshot \u00b6 It's important that the volume to be restored is in an offline state on the array. If the volume snapshot is not specified, the last volume snapshot is used. docker volume create -d cvblock -o importVol=myvol1.docker -o forceImport -o restore -o snapshot=mysnap1 --name=myvol1-restored List volumes \u00b6 List Docker volumes. docker volume ls DRIVER VOLUME NAME cvblock:latest myvol1 cvblock:latest myvol1-clone Remove a Docker Volume \u00b6 When you remove volumes from Docker control they are set to the offline state on the array. Access to the volumes and related snapshots using the Docker Volume plugin can be reestablished. Note To delete volumes from the HPE Cloud Volumes portal using the remove command, the volume should have been created with a -o destroyOnRm flag. Important: Be aware that when this option is set to true, volumes and all related snapshots are deleted from the group, and can no longer be accessed by the Docker Volume plugin. Remove the volume named myvol1 . docker volume rm myvol1 Uninstall \u00b6 The plugin can be removed using the docker plugin rm command. This command will not remove the configuration directory ( /etc/hpe-storage/ ). docker plugin rm cvblock Troubleshooting \u00b6 The config directory is at /etc/hpe-storage/ . When a plugin is installed and enabled, the HPE Cloud Volumes certificates are created in the config directory. ls -l /etc/hpe-storage/ total 16 -r-------- 1 root root 1159 Aug 2 00:20 container_provider_host.cert -r-------- 1 root root 1671 Aug 2 00:20 container_provider_host.key -r-------- 1 root root 1521 Aug 2 00:20 container_provider_server.cert Additionally there is a config file volume-driver.json present at the same location. This file can be edited to set default parameters for create volumes for docker. Log file location \u00b6 The docker plugin logs are located at /var/log/hpe-docker-plugin.log","title":"HPE Cloud Volumes"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#introduction","text":"This is the documentation for HPE Cloud Volumes Plugin for Docker . It allows dynamic provisioning of Docker Volumes on standalone Docker Engine or Docker Swarm nodes. Introduction Requirements Limitations Installation Plugin privileges Host configuration and installation Making changes Configuration files and options Node fencing Usage Create a Docker Volume Clone a Docker Volume Provisioning Docker Volumes Import a Volume to Docker Import a volume snapshot to Docker Restore an offline Docker Volume with specified snapshot List volumes Remove a Docker Volume Uninstall Troubleshooting Log file location","title":"Introduction"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#requirements","text":"Docker Engine 17.09 or greater If using Docker Enterprise Edition 2.x, the plugin is only supported in swarmmode Recent Red Hat, Debian or Ubuntu-based Linux distribution US regions only Plugin Release Notes 3.1.0 v3.1.0 Note Docker does not support certified and managed Docker Volume plugins with Docker EE Kubernetes. If you want to use Kubernetes on Docker EE with HPE Nimble Storage, please use the HPE Volume Driver for Kubernetes FlexVolume Plugin or the HPE CSI Driver for Kubernetes depending on your situation.","title":"Requirements"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#limitations","text":"HPE Cloud Volumes provides a Docker certified plugin delivered through the Docker Store. Certain features and capabilities are not available through the managed plugin. Please understand these limitations before deploying either of these plugins. The managed plugin does NOT provide: Support for Docker's release of Kubernetes in Docker Enterprise Edition 2.x Support for Windows Containers The managed plugin does provide a simple way to manage HPE Cloud Volumes integration on your Docker instances using Docker's interface to install and manage the plugin.","title":"Limitations"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#installation","text":"","title":"Installation"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#plugin_privileges","text":"In order to create connections, attach devices and mount file systems, the plugin requires more privileges than a standard application container. These privileges are enumerated during installation. These permissions need to be granted for the plugin to operate correctly. Plugin \"cvblock\" is requesting the following privileges: - network: [host] - mount: [/dev] - mount: [/run/lock] - mount: [/sys] - mount: [/etc] - mount: [/var/lib] - mount: [/var/run/docker.sock] - mount: [/sbin/iscsiadm] - mount: [/lib/modules] - mount: [/usr/lib64] - allow-all-devices: [true] - capabilities: [CAP_SYS_ADMIN CAP_SYS_MODULE CAP_MKNOD]","title":"Plugin privileges"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#host_configuration_and_installation","text":"Setting up the plugin varies between Linux distributions. These procedures requires root privileges on the cloud instance. Red Hat 7.5+, CentOS 7.5+: yum install -y iscsi-initiator-utils device-mapper-multipath docker plugin install --disable --grant-all-permissions --alias cvblock store/hpestorage/cvblock:3.1.0 docker plugin set cv PROVIDER_IP=cloudvolumes.hpe.com PROVIDER_USERNAME=<access_key> PROVIDER_PASSWORD=<access_secret> docker plugin enable cvblock systemctl daemon-reload systemctl enable iscsid multipathd systemctl start iscsid multipathd Ubuntu 16.04 LTS and Ubuntu 18.04 LTS: apt-get install -y open-iscsi multipath-tools xfsprogs modprobe xfs sed -i\"\" -e \"\\$axfs\" /etc/modules docker plugin install --disable --grant-all-permissions --alias cvblock store/hpestorage/cvblock:3.1.0 docker plugin set cv PROVIDER_IP=cloudvolumes.hpe.com PROVIDER_USERNAME=<access_key> PROVIDER_PASSWORD=<access_secret> glibc_libs.source=/lib/x86_64-linux-gnu docker plugin enable cvblock systemctl daemon-reload systemctl restart open-iscsi multipath-tools Debian 9.x (stable): apt-get install -y open-iscsi multipath-tools xfsprogs modprobe xfs sed -i\"\" -e \"\\$axfs\" /etc/modules docker plugin install --disable --grant-all-permissions --alias cvblock store/hpestorage/cvblock:3.1.0 docker plugin set cv PROVIDER_IP=cloudvolumes.hpe.com PROVIDER_USERNAME=<access_key> PROVIDER_PASSWORD=<access_secret> iscsiadm.source=/usr/bin/iscsiadm glibc_libs.source=/lib/x86_64-linux-gnu docker plugin enable cvblock systemctl daemon-reload systemctl restart open-iscsi multipath-tools","title":"Host configuration and installation"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#making_changes","text":"The docker plugin set command can only be used on the plugin if it is disabled. To disable the plugin, use the docker plugin disable command. For example: docker plugin disable cvblock List of parameters which are supported to be settable by the plugin Parameter Description Default PROVIDER_IP HPE Cloud Volumes portal \"\" PROVIDER_USERNAME HPE Cloud Volumes username \"\" PROVIDER_PASSWORD HPE Cloud Volumes password \"\" PROVIDER_REMOVE Unassociate Plugin from HPE Cloud Volumes false LOG_LEVEL Log level of the plugin ( info , debug , or trace ) debug SCOPE Scope of the plugin ( global or local ) global In the event of reassociating the plugin with a different HPE Cloud Volumes portal, certain procedures need to be followed: Disable the plugin docker plugin disable cvblock Set new paramters docker plugin set cvblock PROVIDER_REMOVE=true Enable the plugin docker plugin enable cvblock Disable the plugin docker plugin disable cvblock The plugin is now ready for re-configuration docker plugin set cvblock PROVIDER_IP=< New portal address > PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin PROVIDER_REMOVE=false Note The PROVIDER_REMOVE=false parameter must be set if the plugin ever has been unassociated from a HPE Cloud Volumes portal.","title":"Making changes"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#configuration_files_and_options","text":"The configuration directory for the plugin is /etc/hpe-storage on the host. Files in this directory are preserved between plugin upgrades. The /etc/hpe-storage/volume-driver.json file contains three sections, global , defaults and overrides . The global options are plugin runtime parameters and doesn't have any end-user configurable keys at this time. The defaults map allows the docker host administrator to set default options during volume creation. The docker user may override these default options with their own values for a specific option. The overrides map allows the docker host administrator to enforce a certain option for every volume creation. The docker user may not override the option and any attempt to do so will be silently ignored. Note defaults and overrides are dynamically read during runtime while global changes require a plugin restart. Example config file in /etc/hpe-storage/volume-driver.json : { \"global\": { \"snapPrefix\": \"BaseFor\", \"initiators\": [\"eth0\"], \"automatedConnection\": true, \"existingCloudSubnet\": \"10.1.0.0/24\", \"region\": \"us-east-1\", \"privateCloud\": \"vpc-data\", \"cloudComputeProvider\": \"Amazon AWS\" }, \"defaults\": { \"limitIOPS\": 1000, \"fsOwner\": \"0:0\", \"fsMode\": \"600\", \"description\": \"Volume provisioned by the HPE Volume Driver for Kubernetes FlexVolume Plugin\", \"perfPolicy\": \"Other\", \"protectionTemplate\": \"twicedaily:4\", \"encryption\": true, \"volumeType\": \"PF\", \"destroyOnRm\": true }, \"overrides\": { } } For an exhaustive list of options use the help option from the docker CLI: $ docker volume create -d cvblock -o help","title":"Configuration files and options"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#node_fencing","text":"If you are considering using any Docker clustering technologies for your Docker deployment, it is important to understand the fencing mechanism used to protect data. Attaching the same Docker Volume to multiple containers on the same host is fully supported. Mounting the same volume on multiple hosts is not supported. Docker does not provide a fencing mechanism for nodes that have become disconnected from the Docker Swarm. This results in the isolated nodes continuing to run their containers. When the containers are rescheduled on a surviving node, the Docker Engine will request that the Docker Volume(s) be mounted. In order to prevent data corruption, the Docker Volume Plugin will stop serving the Docker Volume to the original node before mounting it on the newly requested node. During a mount request, the Docker Volume Plugin inspects the ACR (Access Control Record) on the volume. If the ACR does not match the initiator requesting to mount the volume, the ACR is removed and the volume taken offline. The volume is now fenced off and other nodes are unable to access any data in the volume. The volume then receives a new ACR matching the requesting initiator, and it is mounted for the container requesting the volume. This is done because the volumes are formatted with XFS, which is not a clustered filesystem and can be corrupted if the same volume is mounted to multiple hosts. The side effect of a fenced node is that I/O hangs indefinitely, and the initiator is rejected during login. If the fenced node rejoins the Docker Swarm using Docker SwarmKit, the swarm tries to shut down the services that were rescheduled elsewhere to maintain the desired replica set for the service. This operation will also hang indefinitely waiting for I/O. We recommend running a dedicated Docker host that does not host any other critical applications besides the Docker Engine. Doing this supports a safe way to reboot a node after a grace period and have it start cleanly when a hung task is detected. Otherwise, the node can remain in the hung state indefinitely. The following kernel parameters control the system behavior when a hung task is detected: # Reset after these many seconds after a panic kernel.panic = 5 # I do consider hung tasks reason enough to panic kernel.hung_task_panic = 1 # To not panic in vain, I'll wait these many seconds before I declare a hung task kernel.hung_task_timeout_secs = 150 Add these parameters to the /etc/sysctl.d/99-hung_task_timeout.conf file and reboot the system. Important Docker SwarmKit declares a node as failed after five (5) seconds. Services are then rescheduled and up and running again in less than ten (10) seconds. The parameters noted above provide the system a way to manage other tasks that may appear to be hung and avoid a system panic.","title":"Node fencing"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#usage","text":"These are some basic examples on how to use the HPE Cloud Volumes Plugin for Docker.","title":"Usage"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#create_a_docker_volume","text":"Using docker volume create . Note The plugin applies a set of default options when you create new volumes unless you override them using the volume create -o key=value option flags. Create a Docker volume with a custom description: docker volume create -d cvblock -o description=\"My volume description\" --name myvol1 (Optional) Inspect the new volume: docker volume inspect myvol1 (Optional) Attach the volume to an interactive container. docker run -it --rm -v myvol1:/data bash The volume is mounted inside the container on /data .","title":"Create a Docker Volume"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#clone_a_docker_volume","text":"Use the docker volume create command with the cloneOf option to clone a Docker volume to a new Docker volume. Clone the Docker volume named myvol1 to a new Docker volume named myvol1-clone . docker volume create -d cvblock -o cloneOf=myvol1 --name=myvol1-clone (Optional) Select a snapshot on which to base the clone. docker volume create -d cvblock -o snapshot=mysnap1 -o cloneOf=myvol1 --name=myvol2-clone","title":"Clone a Docker Volume"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#provisioning_docker_volumes","text":"There are several ways to provision a Docker volume depending on what tools are used: Docker Engine (CLI) Docker Compose file with either Docker UCP or Docker Engine The Docker Volume plugin leverages the existing Docker CLI and APIs, therefor all native Docker tools may be used to provision a volume. Note The plugin applies a set of default volume create options. Unless you override the default options using the volume option flags, the defaults are applied when you create volumes. For example, the default volume size is 10GiB. Config file volume-driver.json , which is stored at /etc/hpe-storage/volume-driver.json : { \"global\": {}, \"defaults\": { \"sizeInGiB\":\"10\", \"limitIOPS\":\"-1\", \"limitMBPS\":\"-1\", \"perfPolicy\": \"DockerDefault\", }, \"overrides\":{} }","title":"Provisioning Docker Volumes"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#import_a_volume_to_docker","text":"Take the volume you want to import offline before importing it. For information about how to take a volume offline, refer to the HPE Cloud Volumes documentation . Use the create command with the importVol option to import an HPE Cloud Volume to Docker and name it. Import the HPE Cloud Volume named mycloudvol as a Docker volume named myvol3-imported . docker volume create \u2013d cvblock -o importVol=mycloudvol --name=myvol3-imported","title":"Import a Volume to Docker"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#import_a_volume_snapshot_to_docker","text":"Use the create command with the importVolAsClone option to import a HPE Cloud Volume snapshot as a Docker volume. Optionally, specify a particular snapshot on the HPE Cloud Volume using the snapshot option. Import the HPE Cloud Volumes snapshot aSnapshot on the volume importMe as a Docker volume named importedSnap . docker volume create -d cvblock -o importVolAsClone=mycloudvol -o snapshot=mysnap1 --name=myvol4-clone Note If no snapshot is specified, the latest snapshot on the volume is imported.","title":"Import a volume snapshot to Docker"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#restore_an_offline_docker_volume_with_specified_snapshot","text":"It's important that the volume to be restored is in an offline state on the array. If the volume snapshot is not specified, the last volume snapshot is used. docker volume create -d cvblock -o importVol=myvol1.docker -o forceImport -o restore -o snapshot=mysnap1 --name=myvol1-restored","title":"Restore an offline Docker Volume with specified snapshot"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#list_volumes","text":"List Docker volumes. docker volume ls DRIVER VOLUME NAME cvblock:latest myvol1 cvblock:latest myvol1-clone","title":"List volumes"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#remove_a_docker_volume","text":"When you remove volumes from Docker control they are set to the offline state on the array. Access to the volumes and related snapshots using the Docker Volume plugin can be reestablished. Note To delete volumes from the HPE Cloud Volumes portal using the remove command, the volume should have been created with a -o destroyOnRm flag. Important: Be aware that when this option is set to true, volumes and all related snapshots are deleted from the group, and can no longer be accessed by the Docker Volume plugin. Remove the volume named myvol1 . docker volume rm myvol1","title":"Remove a Docker Volume"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#uninstall","text":"The plugin can be removed using the docker plugin rm command. This command will not remove the configuration directory ( /etc/hpe-storage/ ). docker plugin rm cvblock","title":"Uninstall"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#troubleshooting","text":"The config directory is at /etc/hpe-storage/ . When a plugin is installed and enabled, the HPE Cloud Volumes certificates are created in the config directory. ls -l /etc/hpe-storage/ total 16 -r-------- 1 root root 1159 Aug 2 00:20 container_provider_host.cert -r-------- 1 root root 1671 Aug 2 00:20 container_provider_host.key -r-------- 1 root root 1521 Aug 2 00:20 container_provider_server.cert Additionally there is a config file volume-driver.json present at the same location. This file can be edited to set default parameters for create volumes for docker.","title":"Troubleshooting"},{"location":"docker_volume_plugins/hpe_cloud_volumes/index.html#log_file_location","text":"The docker plugin logs are located at /var/log/hpe-docker-plugin.log","title":"Log file location"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html","text":"Introduction \u00b6 This is the documentation for HPE Nimble Storage Volume Plugin for Docker . It allows dynamic provisioning of Docker Volumes on standalone Docker Engine or Docker Swarm nodes. Introduction Requirements Limitations Installation Plugin privileges Host configuration and installation Making changes Security considerations Configuration files and options Node fencing Usage Create a Docker Volume Clone a Docker Volume Provisioning Docker Volumes Import a volume to Docker Import a volume snapshot to Docker Restore an offline Docker Volume with specified snapshot List volumes Remove a Docker Volume Uninstall Troubleshooting Log file location Upgrade from older plugins Requirements \u00b6 Docker Engine 17.09 or greater If using Docker Enterprise Edition 2.x, the plugin is only supported in swarmmode Recent Red Hat, Debian or Ubuntu-based Linux distribution NimbleOS 5.0.8/5.1.3 or greater Plugin HPE Nimble Storage Version Release Notes 3.0.0 5.0.8.x and 5.1.3.x onwards v3.0.0 3.1.0 5.0.8.x and 5.1.3.x onwards v3.1.0 Note Docker does not support certified and managed Docker Volume plugins with Docker EE Kubernetes. If you want to use Kubernetes on Docker EE with HPE Nimble Storage, please use the HPE Volume Driver for Kubernetes FlexVolume Plugin or the HPE CSI Driver for Kubernetes depending on your situation. Limitations \u00b6 HPE Nimble Storage provides a Docker certified plugin delivered through the Docker Store. HPE Nimble Storage also provides a Docker Volume plugin for Windows Containers, it's available on HPE InfoSight along with its documentation . Certain features and capabilities are not available through the managed plugin. Please understand these limitations before deploying either of these plugins. The managed plugin does NOT provide: Support for Docker's release of Kubernetes in Docker Enterprise Edition 2.x Support for older versions of NimbleOS (all versions below 5.x) Support for Windows Containers The managed plugin does provide a simple way to manage HPE Nimble Storage on your Docker hosts using Docker's interface to install and manage the plugin. Installation \u00b6 Plugin privileges \u00b6 In order to create connections, attach devices and mount file systems, the plugin requires more privileges than a standard application container. These privileges are enumerated during installation. These permissions need to be granted for the plugin to operate correctly. Plugin \"nimble\" is requesting the following privileges: - network: [host] - mount: [/dev] - mount: [/run/lock] - mount: [/sys] - mount: [/etc] - mount: [/var/lib] - mount: [/var/run/docker.sock] - mount: [/sbin/iscsiadm] - mount: [/lib/modules] - mount: [/usr/lib64] - allow-all-devices: [true] - capabilities: [CAP_SYS_ADMIN CAP_SYS_MODULE CAP_MKNOD] Host configuration and installation \u00b6 Setting up the plugin varies between Linux distributions. The following workflows have been tested using a Nimble iSCSI group array at 192.168.171.74 with PROVIDER_USERNAME admin and PROVIDER_PASSWORD admin : These procedures require root privileges. Red Hat 7.5+, CentOS 7.5+: yum install -y iscsi-initiator-utils device-mapper-multipath docker plugin install --disable --grant-all-permissions --alias nimble store/nimblestorage/nimble:3.1.0 docker plugin set nimble PROVIDER_IP=192.168.1.1 PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin docker plugin enable nimble systemctl daemon-reload systemctl enable iscsid multipathd systemctl start iscsid multipathd Ubuntu 16.04 LTS and Ubuntu 18.04 LTS: apt-get install -y open-iscsi multipath-tools xfsprogs modprobe xfs sed -i\"\" -e \"\\$axfs\" /etc/modules docker plugin install --disable --grant-all-permissions --alias nimble store/nimblestorage/nimble:3.1.0 docker plugin set nimble PROVIDER_IP=192.168.1.1 PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin glibc_libs.source=/lib/x86_64-linux-gnu docker plugin enable nimble systemctl daemon-reload systemctl restart open-iscsi multipath-tools Debian 9.x (stable): apt-get install -y open-iscsi multipath-tools xfsprogs modprobe xfs sed -i\"\" -e \"\\$axfs\" /etc/modules docker plugin install --disable --grant-all-permissions --alias nimble store/nimblestorage/nimble:3.1.0 docker plugin set nimble PROVIDER_IP=192.168.1.1 PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin iscsiadm.source=/usr/bin/iscsiadm glibc_libs.source=/lib/x86_64-linux-gnu docker plugin enable nimble systemctl daemon-reload systemctl restart open-iscsi multipath-tools NOTE: To use the plugin on Fibre Channel environments use the PROTOCOL=FC environment variable. Making changes \u00b6 The docker plugin set command can only be used on the plugin if it is disabled. To disable the plugin, use the docker plugin disable command. For example: docker plugin disable nimble List of parameters which are supported to be settable by the plugin. Parameter Description Default PROVIDER_IP HPE Nimble Storage array ip \"\" PROVIDER_USERNAME HPE Nimble Storage array username \"\" PROVIDER_PASSWORD HPE Nimble Storage array password \"\" PROVIDER_REMOVE Unassociate Plugin from HPE Nimble Storage array false LOG_LEVEL Log level of the plugin ( info , debug , or trace ) debug SCOPE Scope of the plugin ( global or local ) global PROTOCOL Scsi protocol supported by the plugin ( iscsi or fc ) iscsi Security considerations \u00b6 The HPE Nimble Storage credentials are visible to any user who can execute docker plugin inspect nimble . To limit credential visibility, the variables should be unset after certificates have been generated. The following set of steps can be used to accomplish this: Add the credentials docker plugin set PROVIDER_IP=192.168.1.1 PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin Start the plugin docker plugin enable nimble Stop the plugin docker plugin disable nimble Remove the credentials docker plugin set nimble PROVIDER_USERNAME=\"true\" PROVIDER_PASSWORD=\"true\" Start the plugin docker plugin enable nimble Note Certificates are stored in /etc/hpe-storage/ on the host and will be preserved across plugin updates. In the event of reassociating the plugin with a different HPE Nimble Storage group, certain procedures need to be followed: Disable the plugin docker plugin disable nimble Set new paramters docker plugin set nimble PROVIDER_REMOVE=true Enable the plugin docker plugin enable nimble Disable the plugin docker plugin disable nimble The plugin is now ready for re-configuration docker plugin set nimble PROVIDER_IP=< New IP address > PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin PROVIDER_REMOVE=false Note: The PROVIDER_REMOVE=false parameter must be set if the plugin ever has been unassociated from a HPE Nimble Storage group. Configuration files and options \u00b6 The configuration directory for the plugin is /etc/hpe-storage on the host. Files in this directory are preserved between plugin upgrades. The /etc/hpe-storage/volume-driver.json file contains three sections, global , defaults and overrides . The global options are plugin runtime parameters and doesn't have any end-user configurable keys at this time. The defaults map allows the docker host administrator to set default options during volume creation. The docker user may override these default options with their own values for a specific option. The overrides map allows the docker host administrator to enforce a certain option for every volume creation. The docker user may not override the option and any attempt to do so will be silently ignored. These maps are essential to discuss with the HPE Nimble Storage administrator. A common pattern is that a default protection template is selected for all volumes to fulfill a certain data protection policy enforced by the business it's serving. Another useful option is to override the volume placement options to allow a single HPE Nimble Storage array to provide multi-tenancy for docker environments. Note: defaults and overrides are dynamically read during runtime while global changes require a plugin restart. Below is an example /etc/hpe-storage/volume-driver.json outlining the above use cases: { \"global\": { \"nameSuffix\": \".docker\" }, \"defaults\": { \"description\": \"Volume provisioned by Docker\", \"protectionTemplate\": \"Retain-90Daily\" }, \"overrides\": { \"folder\": \"docker-prod\" } } For an exhaustive list of options use the help option from the docker CLI: $ docker volume create -d nimble -o help Nimble Storage Docker Volume Driver: Create Help Create or Clone a Nimble Storage backed Docker Volume or Import an existing Nimble Volume or Clone of a Snapshot into Docker. Universal options: -o mountConflictDelay=X X is the number of seconds to delay a mount request when there is a conflict (default is 0) Create options: -o sizeInGiB=X X is the size of volume specified in GiB -o size=X X is the size of volume specified in GiB (short form of sizeInGiB) -o fsOwner=X X is the user id and group id that should own the root directory of the filesystem, in the form of [userId:groupId] -o fsMode=X X is 1 to 4 octal digits that represent the file mode to be applied to the root directory of the filesystem -o description=X X is the text to be added to volume description (optional) -o perfPolicy=X X is the name of the performance policy (optional) Performance Policies: Exchange 2003 data store, Exchange log, Exchange 2007 data store, SQL Server, SharePoint, Exchange 2010 data store, SQL Server Logs, SQL Server 2012, Oracle OLTP, Windows File Server, Other Workloads, DockerDefault, General, MariaDB, Veeam Backup Repository, Backup Repository -o pool=X X is the name of pool in which to place the volume Needed with -o folder (optional) -o folder=X X is the name of folder in which to place the volume Needed with -o pool (optional). -o encryption indicates that the volume should be encrypted (optional, dedupe and encryption are mutually exclusive) -o thick indicates that the volume should be thick provisioned (optional, dedupe and thick are mutually exclusive) -o dedupe indicates that the volume should be deduplicated -o limitIOPS=X X is the IOPS limit of the volume. IOPS limit should be in range [256, 4294967294] or -1 for unlimited. -o limitMBPS=X X is the MB/s throughput limit for this volume. If both limitIOPS and limitMBPS are specified, limitMBPS must not be hit before limitIOPS -o destroyOnRm indicates that the Nimble volume (including snapshots) backing this volume should be destroyed when this volume is deleted -o syncOnUnmount only valid with \"protectionTemplate\", if the protectionTemplate includes a replica destination, unmount calls will snapshot and transfer the last delta to the destination. (optional) -o protectionTemplate=X X is the name of the protection template (optional) Protection Templates: General, Retain-90Daily, Retain-30Daily, Retain-48Hourly-30Daily-52Weekly Clone options: -o cloneOf=X X is the name of Docker Volume to create a clone of -o snapshot=X X is the name of the snapshot to base the clone on (optional, if missing, a new snapshot is created) -o createSnapshot indicates that a new snapshot of the volume should be taken and used for the clone (optional) -o destroyOnRm indicates that the Nimble volume (including snapshots) backing this volume should be destroyed when this volume is deleted -o destroyOnDetach indicates that the Nimble volume (including snapshots) backing this volume should be destroyed when this volume is unmounted or detached Import Volume options: -o importVol=X X is the name of the Nimble Volume to import -o pool=X X is the name of the pool in which the volume to be imported resides (optional) -o folder=X X is the name of the folder in which the volume to be imported resides (optional) -o forceImport forces the import of the volume. Note that overwrites application metadata (optional) -o restore restores the volume to the last snapshot taken on the volume (optional) -o snapshot=X X is the name of the snapshot which the volume will be restored to, only used with -o restore (optional) -o takeover indicates the current group will takeover the ownership of the Nimble volume and volume collection (optional) -o reverseRepl reverses the replication direction so that writes to the Nimble volume are replicated back to the group where it was replicated from (optional) Import Clone of Snapshot options: -o importVolAsClone=X X is the name of the Nimble Volume and Nimble Snapshot to clone and import -o snapshot=X X is the name of the Nimble snapshot to clone and import (optional, if missing, will use the most recent snapshot) -o createSnapshot indicates that a new snapshot of the volume should be taken and used for the clone (optional) -o pool=X X is the name of the pool in which the volume to be imported resides (optional) -o folder=X X is the name of the folder in which the volume to be imported resides (optional) -o destroyOnRm indicates that the Nimble volume (including snapshots) backing this volume should be destroyed when this volume is deleted -o destroyOnDetach indicates that the Nimble volume (including snapshots) backing this volume should be destroyed when this volume is unmounted or detached Node fencing \u00b6 If you are considering using any Docker clustering technologies for your Docker deployment, it is important to understand the fencing mechanism used to protect data. Attaching the same Docker Volume to multiple containers on the same host is fully supported. Mounting the same volume on multiple hosts is not supported. Docker does not provide a fencing mechanism for nodes that have become disconnected from the Docker Swarm. This results in the isolated nodes continuing to run their containers. When the containers are rescheduled on a surviving node, the Docker Engine will request that the Docker Volume(s) be mounted. In order to prevent data corruption, the Docker Volume Plugin will stop serving the Docker Volume to the original node before mounting it on the newly requested node. During a mount request, the Docker Volume Plugin inspects the ACR (Access Control Record) on the volume. If the ACR does not match the initiator requesting to mount the volume, the ACR is removed and the volume taken offline. The volume is now fenced off and other nodes are unable to access any data in the volume. The volume then receives a new ACR matching the requesting initiator, and it is mounted for the container requesting the volume. This is done because the volumes are formatted with XFS, which is not a clustered filesystem and can be corrupted if the same volume is mounted to multiple hosts. The side effect of a fenced node is that I/O hangs indefinitely, and the initiator is rejected during login. If the fenced node rejoins the Docker Swarm using Docker SwarmKit, the swarm tries to shut down the services that were rescheduled elsewhere to maintain the desired replica set for the service. This operation will also hang indefinitely waiting for I/O. We recommend running a dedicated Docker host that does not host any other critical applications besides the Docker Engine. Doing this supports a safe way to reboot a node after a grace period and have it start cleanly when a hung task is detected. Otherwise, the node can remain in the hung state indefinitely. The following kernel parameters control the system behavior when a hung task is detected: # Reset after these many seconds after a panic kernel.panic = 5 # I do consider hung tasks reason enough to panic kernel.hung_task_panic = 1 # To not panic in vain, I'll wait these many seconds before I declare a hung task kernel.hung_task_timeout_secs = 150 Add these parameters to the /etc/sysctl.d/99-hung_task_timeout.conf file and reboot the system. Important Docker SwarmKit declares a node as failed after five (5) seconds. Services are then rescheduled and up and running again in less than ten (10) seconds. The parameters noted above provide the system a way to manage other tasks that may appear to be hung and avoid a system panic. Usage \u00b6 These are some basic examples on how to use the HPE Nimble Storage Volume Plugin for Docker. Create a Docker Volume \u00b6 Using docker volume create . Note The plugin applies a set of default options when you create new volumes unless you override them using the volume create -o key=value option flags. Create a Docker volume with a custom description: docker volume create -d nimble -o description=\"My volume description\" --name myvol1 (Optional) Inspect the new volume: docker volume inspect myvol1 (Optional) Attach the volume to an interactive container. docker run -it --rm -v myvol1:/data bash The volume is mounted inside the container on /data . Clone a Docker Volume \u00b6 Use the docker volume create command with the cloneOf option to clone a Docker volume to a new Docker volume. Clone the Docker volume named myvol1 to a new Docker volume named myvol1-clone . docker volume create -d nimble -o cloneOf=myvol1 --name=myvol1-clone (Optional) Select a snapshot on which to base the clone. docker volume create -d nimble -o snapshot=mysnap1 -o cloneOf=myvol1 --name=myvol2-clone Provisioning Docker Volumes \u00b6 There are several ways to provision a Docker volume depending on what tools are used: Docker Engine (CLI) Docker Compose file with either Docker UCP or Docker Engine The Docker Volume plugin leverages the existing Docker CLI and APIs, therefor all native Docker tools may be used to provision a volume. Note The plugin applies a set of default volume create options. Unless you override the default options using the volume option flags, the defaults are applied when you create volumes. For example, the default volume size is 10GiB. Config file volume-driver.json , which is stored at /etc/hpe-storage/volume-driver.json: { \"global\": {}, \"defaults\": { \"sizeInGiB\":\"10\", \"limitIOPS\":\"-1\", \"limitMBPS\":\"-1\", \"perfPolicy\": \"DockerDefault\", }, \"overrides\":{} } Import a volume to Docker \u00b6 Before you begin Take the volume you want to import offline before importing it. For information about how to take a volume offline, refer to either the CLI Administration Guide or the GUI Administration Guide on HPE InfoSight . Use the create command with the importVol option to import an HPE Nimble Storage volume to Docker and name it. Import the HPE Nimble Storage volume named mynimblevol as a Docker volume named myvol3-imported . docker volume create \u2013d nimble -o importVol=mynimblevol --name=myvol3-imported Import a volume snapshot to Docker \u00b6 Use the create command with the importVolAsClone option to import a HPE Nimble Storage volume snapshot as a Docker volume. Optionally, specify a particular snapshot on the HPE Nimble Storage volume using the snapshot option. Import the HPE Nimble Storage snapshot aSnapshot on the volume importMe as a Docker volume named importedSnap . docker volume create -d nimble -o importVolAsClone=mynimblevol -o snapshot=mysnap1 --name=myvol4-clone Note If no snapshot is specified, the latest snapshot on the volume is imported. Restore an offline Docker Volume with specified snapshot \u00b6 It's important that the volume to be restored is in an offline state on the array. If the volume snapshot is not specified, the last volume snapshot is used. docker volume create -d nimble -o importVol=myvol1.docker -o forceImport -o restore -o snapshot=mysnap1 --name=myvol1-restored List volumes \u00b6 List Docker volumes. docker volume ls DRIVER VOLUME NAME nimble:latest myvol1 nimble:latest myvol1-clone Remove a Docker Volume \u00b6 When you remove volumes from Docker control they are set to the offline state on the array. Access to the volumes and related snapshots using the Docker Volume plugin can be reestablished. Note To delete volumes from the HPE Nimble Storage array using the remove command, the volume should have been created with a -o destroyOnRm flag. Important: Be aware that when this option is set to true, volumes and all related snapshots are deleted from the group, and can no longer be accessed by the Docker Volume plugin. Remove the volume named myvol1 . docker volume rm myvol1 Uninstall \u00b6 The plugin can be removed using the docker plugin rm command. This command will not remove the configuration directory ( /etc/hpe-storage/ ). docker plugin rm nimble Important If this is the last plugin to reference the Nimble Group and to completely remove the configuration directory, follow the steps as below docker plugin set nimble PROVIDER_REMOVE=true docker plugin enable nimble docker plugin rm nimble Troubleshooting \u00b6 The config directory is at /etc/hpe-storage/ . When a plugin is installed and enabled, the Nimble Group certificates are created in the config directory. ls -l /etc/hpe-storage/ total 16 -r-------- 1 root root 1159 Aug 2 00:20 container_provider_host.cert -r-------- 1 root root 1671 Aug 2 00:20 container_provider_host.key -r-------- 1 root root 1521 Aug 2 00:20 container_provider_server.cert Additionally there is a config file volume-driver.json present at the same location. This file can be edited to set default parameters for create volumes for docker. Log file location \u00b6 The docker plugin logs are located at /var/log/hpe-docker-plugin.log Upgrade from older plugins \u00b6 Upgrading from 2.5.1 or older plugins, please follow the below steps Ubuntu 16.04 LTS and Ubuntu 18.04 LTS: docker plugin disable nimble:latest \u2013f docker plugin upgrade --grant-all-permissions nimble store/hpestorage/nimble:3.0.0 --skip-remote-check docker plugin set nimble PROVIDER_IP=192.168.1.1 PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin glibc_libs.source=/lib/x86_64-linux-gnu docker plugin enable nimble:latest Red Hat 7.5+, CentOS 7.5+, Oracle Enterprise Linux 7.5+ and Fedora 28+: docker plugin disable nimble:latest \u2013f docker plugin upgrade --grant-all-permissions nimble store/hpestorage/nimble:3.0.0 --skip-remote-check docker plugin enable nimble:latest Important In Swarm Mode, drain the existing running containers to the node where the plugin is upgraded.","title":"HPE Nimble Storage"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#introduction","text":"This is the documentation for HPE Nimble Storage Volume Plugin for Docker . It allows dynamic provisioning of Docker Volumes on standalone Docker Engine or Docker Swarm nodes. Introduction Requirements Limitations Installation Plugin privileges Host configuration and installation Making changes Security considerations Configuration files and options Node fencing Usage Create a Docker Volume Clone a Docker Volume Provisioning Docker Volumes Import a volume to Docker Import a volume snapshot to Docker Restore an offline Docker Volume with specified snapshot List volumes Remove a Docker Volume Uninstall Troubleshooting Log file location Upgrade from older plugins","title":"Introduction"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#requirements","text":"Docker Engine 17.09 or greater If using Docker Enterprise Edition 2.x, the plugin is only supported in swarmmode Recent Red Hat, Debian or Ubuntu-based Linux distribution NimbleOS 5.0.8/5.1.3 or greater Plugin HPE Nimble Storage Version Release Notes 3.0.0 5.0.8.x and 5.1.3.x onwards v3.0.0 3.1.0 5.0.8.x and 5.1.3.x onwards v3.1.0 Note Docker does not support certified and managed Docker Volume plugins with Docker EE Kubernetes. If you want to use Kubernetes on Docker EE with HPE Nimble Storage, please use the HPE Volume Driver for Kubernetes FlexVolume Plugin or the HPE CSI Driver for Kubernetes depending on your situation.","title":"Requirements"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#limitations","text":"HPE Nimble Storage provides a Docker certified plugin delivered through the Docker Store. HPE Nimble Storage also provides a Docker Volume plugin for Windows Containers, it's available on HPE InfoSight along with its documentation . Certain features and capabilities are not available through the managed plugin. Please understand these limitations before deploying either of these plugins. The managed plugin does NOT provide: Support for Docker's release of Kubernetes in Docker Enterprise Edition 2.x Support for older versions of NimbleOS (all versions below 5.x) Support for Windows Containers The managed plugin does provide a simple way to manage HPE Nimble Storage on your Docker hosts using Docker's interface to install and manage the plugin.","title":"Limitations"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#installation","text":"","title":"Installation"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#plugin_privileges","text":"In order to create connections, attach devices and mount file systems, the plugin requires more privileges than a standard application container. These privileges are enumerated during installation. These permissions need to be granted for the plugin to operate correctly. Plugin \"nimble\" is requesting the following privileges: - network: [host] - mount: [/dev] - mount: [/run/lock] - mount: [/sys] - mount: [/etc] - mount: [/var/lib] - mount: [/var/run/docker.sock] - mount: [/sbin/iscsiadm] - mount: [/lib/modules] - mount: [/usr/lib64] - allow-all-devices: [true] - capabilities: [CAP_SYS_ADMIN CAP_SYS_MODULE CAP_MKNOD]","title":"Plugin privileges"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#host_configuration_and_installation","text":"Setting up the plugin varies between Linux distributions. The following workflows have been tested using a Nimble iSCSI group array at 192.168.171.74 with PROVIDER_USERNAME admin and PROVIDER_PASSWORD admin : These procedures require root privileges. Red Hat 7.5+, CentOS 7.5+: yum install -y iscsi-initiator-utils device-mapper-multipath docker plugin install --disable --grant-all-permissions --alias nimble store/nimblestorage/nimble:3.1.0 docker plugin set nimble PROVIDER_IP=192.168.1.1 PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin docker plugin enable nimble systemctl daemon-reload systemctl enable iscsid multipathd systemctl start iscsid multipathd Ubuntu 16.04 LTS and Ubuntu 18.04 LTS: apt-get install -y open-iscsi multipath-tools xfsprogs modprobe xfs sed -i\"\" -e \"\\$axfs\" /etc/modules docker plugin install --disable --grant-all-permissions --alias nimble store/nimblestorage/nimble:3.1.0 docker plugin set nimble PROVIDER_IP=192.168.1.1 PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin glibc_libs.source=/lib/x86_64-linux-gnu docker plugin enable nimble systemctl daemon-reload systemctl restart open-iscsi multipath-tools Debian 9.x (stable): apt-get install -y open-iscsi multipath-tools xfsprogs modprobe xfs sed -i\"\" -e \"\\$axfs\" /etc/modules docker plugin install --disable --grant-all-permissions --alias nimble store/nimblestorage/nimble:3.1.0 docker plugin set nimble PROVIDER_IP=192.168.1.1 PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin iscsiadm.source=/usr/bin/iscsiadm glibc_libs.source=/lib/x86_64-linux-gnu docker plugin enable nimble systemctl daemon-reload systemctl restart open-iscsi multipath-tools NOTE: To use the plugin on Fibre Channel environments use the PROTOCOL=FC environment variable.","title":"Host configuration and installation"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#making_changes","text":"The docker plugin set command can only be used on the plugin if it is disabled. To disable the plugin, use the docker plugin disable command. For example: docker plugin disable nimble List of parameters which are supported to be settable by the plugin. Parameter Description Default PROVIDER_IP HPE Nimble Storage array ip \"\" PROVIDER_USERNAME HPE Nimble Storage array username \"\" PROVIDER_PASSWORD HPE Nimble Storage array password \"\" PROVIDER_REMOVE Unassociate Plugin from HPE Nimble Storage array false LOG_LEVEL Log level of the plugin ( info , debug , or trace ) debug SCOPE Scope of the plugin ( global or local ) global PROTOCOL Scsi protocol supported by the plugin ( iscsi or fc ) iscsi","title":"Making changes"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#security_considerations","text":"The HPE Nimble Storage credentials are visible to any user who can execute docker plugin inspect nimble . To limit credential visibility, the variables should be unset after certificates have been generated. The following set of steps can be used to accomplish this: Add the credentials docker plugin set PROVIDER_IP=192.168.1.1 PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin Start the plugin docker plugin enable nimble Stop the plugin docker plugin disable nimble Remove the credentials docker plugin set nimble PROVIDER_USERNAME=\"true\" PROVIDER_PASSWORD=\"true\" Start the plugin docker plugin enable nimble Note Certificates are stored in /etc/hpe-storage/ on the host and will be preserved across plugin updates. In the event of reassociating the plugin with a different HPE Nimble Storage group, certain procedures need to be followed: Disable the plugin docker plugin disable nimble Set new paramters docker plugin set nimble PROVIDER_REMOVE=true Enable the plugin docker plugin enable nimble Disable the plugin docker plugin disable nimble The plugin is now ready for re-configuration docker plugin set nimble PROVIDER_IP=< New IP address > PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin PROVIDER_REMOVE=false Note: The PROVIDER_REMOVE=false parameter must be set if the plugin ever has been unassociated from a HPE Nimble Storage group.","title":"Security considerations"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#configuration_files_and_options","text":"The configuration directory for the plugin is /etc/hpe-storage on the host. Files in this directory are preserved between plugin upgrades. The /etc/hpe-storage/volume-driver.json file contains three sections, global , defaults and overrides . The global options are plugin runtime parameters and doesn't have any end-user configurable keys at this time. The defaults map allows the docker host administrator to set default options during volume creation. The docker user may override these default options with their own values for a specific option. The overrides map allows the docker host administrator to enforce a certain option for every volume creation. The docker user may not override the option and any attempt to do so will be silently ignored. These maps are essential to discuss with the HPE Nimble Storage administrator. A common pattern is that a default protection template is selected for all volumes to fulfill a certain data protection policy enforced by the business it's serving. Another useful option is to override the volume placement options to allow a single HPE Nimble Storage array to provide multi-tenancy for docker environments. Note: defaults and overrides are dynamically read during runtime while global changes require a plugin restart. Below is an example /etc/hpe-storage/volume-driver.json outlining the above use cases: { \"global\": { \"nameSuffix\": \".docker\" }, \"defaults\": { \"description\": \"Volume provisioned by Docker\", \"protectionTemplate\": \"Retain-90Daily\" }, \"overrides\": { \"folder\": \"docker-prod\" } } For an exhaustive list of options use the help option from the docker CLI: $ docker volume create -d nimble -o help Nimble Storage Docker Volume Driver: Create Help Create or Clone a Nimble Storage backed Docker Volume or Import an existing Nimble Volume or Clone of a Snapshot into Docker. Universal options: -o mountConflictDelay=X X is the number of seconds to delay a mount request when there is a conflict (default is 0) Create options: -o sizeInGiB=X X is the size of volume specified in GiB -o size=X X is the size of volume specified in GiB (short form of sizeInGiB) -o fsOwner=X X is the user id and group id that should own the root directory of the filesystem, in the form of [userId:groupId] -o fsMode=X X is 1 to 4 octal digits that represent the file mode to be applied to the root directory of the filesystem -o description=X X is the text to be added to volume description (optional) -o perfPolicy=X X is the name of the performance policy (optional) Performance Policies: Exchange 2003 data store, Exchange log, Exchange 2007 data store, SQL Server, SharePoint, Exchange 2010 data store, SQL Server Logs, SQL Server 2012, Oracle OLTP, Windows File Server, Other Workloads, DockerDefault, General, MariaDB, Veeam Backup Repository, Backup Repository -o pool=X X is the name of pool in which to place the volume Needed with -o folder (optional) -o folder=X X is the name of folder in which to place the volume Needed with -o pool (optional). -o encryption indicates that the volume should be encrypted (optional, dedupe and encryption are mutually exclusive) -o thick indicates that the volume should be thick provisioned (optional, dedupe and thick are mutually exclusive) -o dedupe indicates that the volume should be deduplicated -o limitIOPS=X X is the IOPS limit of the volume. IOPS limit should be in range [256, 4294967294] or -1 for unlimited. -o limitMBPS=X X is the MB/s throughput limit for this volume. If both limitIOPS and limitMBPS are specified, limitMBPS must not be hit before limitIOPS -o destroyOnRm indicates that the Nimble volume (including snapshots) backing this volume should be destroyed when this volume is deleted -o syncOnUnmount only valid with \"protectionTemplate\", if the protectionTemplate includes a replica destination, unmount calls will snapshot and transfer the last delta to the destination. (optional) -o protectionTemplate=X X is the name of the protection template (optional) Protection Templates: General, Retain-90Daily, Retain-30Daily, Retain-48Hourly-30Daily-52Weekly Clone options: -o cloneOf=X X is the name of Docker Volume to create a clone of -o snapshot=X X is the name of the snapshot to base the clone on (optional, if missing, a new snapshot is created) -o createSnapshot indicates that a new snapshot of the volume should be taken and used for the clone (optional) -o destroyOnRm indicates that the Nimble volume (including snapshots) backing this volume should be destroyed when this volume is deleted -o destroyOnDetach indicates that the Nimble volume (including snapshots) backing this volume should be destroyed when this volume is unmounted or detached Import Volume options: -o importVol=X X is the name of the Nimble Volume to import -o pool=X X is the name of the pool in which the volume to be imported resides (optional) -o folder=X X is the name of the folder in which the volume to be imported resides (optional) -o forceImport forces the import of the volume. Note that overwrites application metadata (optional) -o restore restores the volume to the last snapshot taken on the volume (optional) -o snapshot=X X is the name of the snapshot which the volume will be restored to, only used with -o restore (optional) -o takeover indicates the current group will takeover the ownership of the Nimble volume and volume collection (optional) -o reverseRepl reverses the replication direction so that writes to the Nimble volume are replicated back to the group where it was replicated from (optional) Import Clone of Snapshot options: -o importVolAsClone=X X is the name of the Nimble Volume and Nimble Snapshot to clone and import -o snapshot=X X is the name of the Nimble snapshot to clone and import (optional, if missing, will use the most recent snapshot) -o createSnapshot indicates that a new snapshot of the volume should be taken and used for the clone (optional) -o pool=X X is the name of the pool in which the volume to be imported resides (optional) -o folder=X X is the name of the folder in which the volume to be imported resides (optional) -o destroyOnRm indicates that the Nimble volume (including snapshots) backing this volume should be destroyed when this volume is deleted -o destroyOnDetach indicates that the Nimble volume (including snapshots) backing this volume should be destroyed when this volume is unmounted or detached","title":"Configuration files and options"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#node_fencing","text":"If you are considering using any Docker clustering technologies for your Docker deployment, it is important to understand the fencing mechanism used to protect data. Attaching the same Docker Volume to multiple containers on the same host is fully supported. Mounting the same volume on multiple hosts is not supported. Docker does not provide a fencing mechanism for nodes that have become disconnected from the Docker Swarm. This results in the isolated nodes continuing to run their containers. When the containers are rescheduled on a surviving node, the Docker Engine will request that the Docker Volume(s) be mounted. In order to prevent data corruption, the Docker Volume Plugin will stop serving the Docker Volume to the original node before mounting it on the newly requested node. During a mount request, the Docker Volume Plugin inspects the ACR (Access Control Record) on the volume. If the ACR does not match the initiator requesting to mount the volume, the ACR is removed and the volume taken offline. The volume is now fenced off and other nodes are unable to access any data in the volume. The volume then receives a new ACR matching the requesting initiator, and it is mounted for the container requesting the volume. This is done because the volumes are formatted with XFS, which is not a clustered filesystem and can be corrupted if the same volume is mounted to multiple hosts. The side effect of a fenced node is that I/O hangs indefinitely, and the initiator is rejected during login. If the fenced node rejoins the Docker Swarm using Docker SwarmKit, the swarm tries to shut down the services that were rescheduled elsewhere to maintain the desired replica set for the service. This operation will also hang indefinitely waiting for I/O. We recommend running a dedicated Docker host that does not host any other critical applications besides the Docker Engine. Doing this supports a safe way to reboot a node after a grace period and have it start cleanly when a hung task is detected. Otherwise, the node can remain in the hung state indefinitely. The following kernel parameters control the system behavior when a hung task is detected: # Reset after these many seconds after a panic kernel.panic = 5 # I do consider hung tasks reason enough to panic kernel.hung_task_panic = 1 # To not panic in vain, I'll wait these many seconds before I declare a hung task kernel.hung_task_timeout_secs = 150 Add these parameters to the /etc/sysctl.d/99-hung_task_timeout.conf file and reboot the system. Important Docker SwarmKit declares a node as failed after five (5) seconds. Services are then rescheduled and up and running again in less than ten (10) seconds. The parameters noted above provide the system a way to manage other tasks that may appear to be hung and avoid a system panic.","title":"Node fencing"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#usage","text":"These are some basic examples on how to use the HPE Nimble Storage Volume Plugin for Docker.","title":"Usage"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#create_a_docker_volume","text":"Using docker volume create . Note The plugin applies a set of default options when you create new volumes unless you override them using the volume create -o key=value option flags. Create a Docker volume with a custom description: docker volume create -d nimble -o description=\"My volume description\" --name myvol1 (Optional) Inspect the new volume: docker volume inspect myvol1 (Optional) Attach the volume to an interactive container. docker run -it --rm -v myvol1:/data bash The volume is mounted inside the container on /data .","title":"Create a Docker Volume"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#clone_a_docker_volume","text":"Use the docker volume create command with the cloneOf option to clone a Docker volume to a new Docker volume. Clone the Docker volume named myvol1 to a new Docker volume named myvol1-clone . docker volume create -d nimble -o cloneOf=myvol1 --name=myvol1-clone (Optional) Select a snapshot on which to base the clone. docker volume create -d nimble -o snapshot=mysnap1 -o cloneOf=myvol1 --name=myvol2-clone","title":"Clone a Docker Volume"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#provisioning_docker_volumes","text":"There are several ways to provision a Docker volume depending on what tools are used: Docker Engine (CLI) Docker Compose file with either Docker UCP or Docker Engine The Docker Volume plugin leverages the existing Docker CLI and APIs, therefor all native Docker tools may be used to provision a volume. Note The plugin applies a set of default volume create options. Unless you override the default options using the volume option flags, the defaults are applied when you create volumes. For example, the default volume size is 10GiB. Config file volume-driver.json , which is stored at /etc/hpe-storage/volume-driver.json: { \"global\": {}, \"defaults\": { \"sizeInGiB\":\"10\", \"limitIOPS\":\"-1\", \"limitMBPS\":\"-1\", \"perfPolicy\": \"DockerDefault\", }, \"overrides\":{} }","title":"Provisioning Docker Volumes"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#import_a_volume_to_docker","text":"Before you begin Take the volume you want to import offline before importing it. For information about how to take a volume offline, refer to either the CLI Administration Guide or the GUI Administration Guide on HPE InfoSight . Use the create command with the importVol option to import an HPE Nimble Storage volume to Docker and name it. Import the HPE Nimble Storage volume named mynimblevol as a Docker volume named myvol3-imported . docker volume create \u2013d nimble -o importVol=mynimblevol --name=myvol3-imported","title":"Import a volume to Docker"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#import_a_volume_snapshot_to_docker","text":"Use the create command with the importVolAsClone option to import a HPE Nimble Storage volume snapshot as a Docker volume. Optionally, specify a particular snapshot on the HPE Nimble Storage volume using the snapshot option. Import the HPE Nimble Storage snapshot aSnapshot on the volume importMe as a Docker volume named importedSnap . docker volume create -d nimble -o importVolAsClone=mynimblevol -o snapshot=mysnap1 --name=myvol4-clone Note If no snapshot is specified, the latest snapshot on the volume is imported.","title":"Import a volume snapshot to Docker"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#restore_an_offline_docker_volume_with_specified_snapshot","text":"It's important that the volume to be restored is in an offline state on the array. If the volume snapshot is not specified, the last volume snapshot is used. docker volume create -d nimble -o importVol=myvol1.docker -o forceImport -o restore -o snapshot=mysnap1 --name=myvol1-restored","title":"Restore an offline Docker Volume with specified snapshot"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#list_volumes","text":"List Docker volumes. docker volume ls DRIVER VOLUME NAME nimble:latest myvol1 nimble:latest myvol1-clone","title":"List volumes"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#remove_a_docker_volume","text":"When you remove volumes from Docker control they are set to the offline state on the array. Access to the volumes and related snapshots using the Docker Volume plugin can be reestablished. Note To delete volumes from the HPE Nimble Storage array using the remove command, the volume should have been created with a -o destroyOnRm flag. Important: Be aware that when this option is set to true, volumes and all related snapshots are deleted from the group, and can no longer be accessed by the Docker Volume plugin. Remove the volume named myvol1 . docker volume rm myvol1","title":"Remove a Docker Volume"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#uninstall","text":"The plugin can be removed using the docker plugin rm command. This command will not remove the configuration directory ( /etc/hpe-storage/ ). docker plugin rm nimble Important If this is the last plugin to reference the Nimble Group and to completely remove the configuration directory, follow the steps as below docker plugin set nimble PROVIDER_REMOVE=true docker plugin enable nimble docker plugin rm nimble","title":"Uninstall"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#troubleshooting","text":"The config directory is at /etc/hpe-storage/ . When a plugin is installed and enabled, the Nimble Group certificates are created in the config directory. ls -l /etc/hpe-storage/ total 16 -r-------- 1 root root 1159 Aug 2 00:20 container_provider_host.cert -r-------- 1 root root 1671 Aug 2 00:20 container_provider_host.key -r-------- 1 root root 1521 Aug 2 00:20 container_provider_server.cert Additionally there is a config file volume-driver.json present at the same location. This file can be edited to set default parameters for create volumes for docker.","title":"Troubleshooting"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#log_file_location","text":"The docker plugin logs are located at /var/log/hpe-docker-plugin.log","title":"Log file location"},{"location":"docker_volume_plugins/hpe_nimble_storage/index.html#upgrade_from_older_plugins","text":"Upgrading from 2.5.1 or older plugins, please follow the below steps Ubuntu 16.04 LTS and Ubuntu 18.04 LTS: docker plugin disable nimble:latest \u2013f docker plugin upgrade --grant-all-permissions nimble store/hpestorage/nimble:3.0.0 --skip-remote-check docker plugin set nimble PROVIDER_IP=192.168.1.1 PROVIDER_USERNAME=admin PROVIDER_PASSWORD=admin glibc_libs.source=/lib/x86_64-linux-gnu docker plugin enable nimble:latest Red Hat 7.5+, CentOS 7.5+, Oracle Enterprise Linux 7.5+ and Fedora 28+: docker plugin disable nimble:latest \u2013f docker plugin upgrade --grant-all-permissions nimble store/hpestorage/nimble:3.0.0 --skip-remote-check docker plugin enable nimble:latest Important In Swarm Mode, drain the existing running containers to the node where the plugin is upgraded.","title":"Upgrade from older plugins"},{"location":"flexvolume_driver/index.html","text":"Legacy FlexVolume drivers \u00b6 Container Provider-based: HPE Nimble Storage and HPE Cloud Volumes Ansible installer for HPE 3PAR and Primera","title":"Legacy FlexVolume drivers"},{"location":"flexvolume_driver/index.html#legacy_flexvolume_drivers","text":"Container Provider-based: HPE Nimble Storage and HPE Cloud Volumes Ansible installer for HPE 3PAR and Primera","title":"Legacy FlexVolume drivers"},{"location":"flexvolume_driver/container_provider/index.html","text":"Overview \u00b6 The HPE Volume Driver for Kubernetes FlexVolume Plugin leverages HPE Nimble Storage or HPE Cloud Volumes to provide scalable and persistent storage for stateful applications. Important Using HPE Nimble Storage with Kubernetes 1.13 and newer, please use the HPE CSI Driver for Kubernetes . Source code and developer documentation is available in the hpe-storage/flexvolume-driver GitHub repo. Overview Platform requirements HPE Nimble Storage Platform Requirements HPE Cloud Volumes Platform Requirements Deploying to Kubernetes Step 1: Create a secret HPE Nimble Storage HPE Cloud Volumes Step 2. Create a ConfigMap HPE Nimble Storage HPE Cloud Volumes Step 3. Deploy the FlexVolume driver and dynamic provisioner HPE Nimble Storage HPE Cloud Volumes Using Sample StorageClass Test and verify volume provisioning Use case specific examples Data protection Clone and throttle for devs Clone a non-containerized volume Import (cutover) a volume Using overrides Creating clones of PVCs StorageClass parameters HPE Nimble Storage StorageClass parameters Common parameters for Provisioning and Cloning Provisioning parameters Cloning parameters Import parameters HPE Cloud Volumes StorageClass parameters Common parameters for Provisioning and Cloning Provisioning parameters Cloning parameters Import parameters Diagnostics Troubleshooting FlexVolume driver Locations Override defaults Connectivity FlexVolume and dynamic provisioner driver logs Log Collector Advanced Configuration Set defaults at the compute node level Global options Common Platform requirements \u00b6 The FlexVolume driver supports multiple backends that are based on a \"container provider\" architecture. Currently, Nimble and Cloud Volumes are supported. HPE Nimble Storage Platform Requirements \u00b6 Driver HPE Nimble Storage Version Release Notes Blog v3.0.0 5.0.8.x and 5.1.3.x onwards v3.0.0 HPE Storage Tech Insiders v3.1.0 5.0.8.x and 5.1.3.x onwards v3.1.0 OpenShift Container Platform 3.9, 3.10 and 3.11. Kubernetes 1.10 and above. Redhat/CentOS 7.5+ Ubuntu 16.04/18.04 LTS Note: Synchronous replication (Peer Persistence) is not supported by the HPE Volume Driver for Kubernetes FlexVolume Plugin. HPE Cloud Volumes Platform Requirements \u00b6 Driver Release Notes Blog v3.1.0 v3.1.0 Using HPE Cloud Volumes with Amazon EKS Amazon EKS 1.12/1.13 Microsoft Azure AKS 1.12/1.13 US regions only Deploying to Kubernetes \u00b6 The recommended way to deploy and manage the HPE Volume Driver for Kubernetes FlexVolume Plugin is to use Helm. Please see the co-deployments repository for further information. Use the following steps for a manual installation. Step 1: Create a secret \u00b6 HPE Nimble Storage \u00b6 Replace the password string ( YWRtaW4= ) below with a base64 encoded version of your password and replace the backend with your array IP address and save it as hpe-secret.yaml . apiVersion: v1 kind: Secret metadata: name: hpe-secret namespace: kube-system stringData: backend: 192.168.1.1 username: admin protocol: \"iscsi\" data: # echo -n \"admin\" | base64 password: YWRtaW4= HPE Cloud Volumes \u00b6 Replace the username and password strings ( YWRtaW4= ) with a base64 encoded version of your HPE Cloud Volumes \"access_key\" and \"access_secret\". Also, replace the backend with HPE Cloud Volumes portal fully qualified domain name (FQDN) and save it as hpe-secret.yaml . apiVersion: v1 kind: Secret metadata: name: hpe-secret namespace: kube-system stringData: backend: cloudvolumes.hpe.com protocol: \"iscsi\" serviceName: cv-cp-svc servicePort: \"8080\" data: # echo -n \"<my very confidential access key>\" | base64 username: YWRtaW4= # echo -n \"<my very confidential secret key>\" | base64 password: YWRtaW4= Create the secret: kubectl create -f hpe-secret.yaml secret \"hpe-secret\" created You should now see the HPE secret in the kube-system namespace. kubectl get secret/hpe-secret -n kube-system NAME TYPE DATA AGE hpe-secret Opaque 5 3s Step 2. Create a ConfigMap \u00b6 The ConfigMap is used to set and tweak defaults for both the FlexVolume driver and Dynamic Provisioner. HPE Nimble Storage \u00b6 Edit the below default parameters as required for FlexVolume driver and save it as hpe-config.yaml . kind: ConfigMap apiVersion: v1 metadata: name: hpe-config namespace: kube-system data: volume-driver.json: |- { \"global\": {}, \"defaults\": { \"limitIOPS\":\"-1\", \"limitMBPS\":\"-1\", \"perfPolicy\": \"Other\" }, \"overrides\":{} } Tip Please see Advanced for more volume-driver.json configuration options. HPE Cloud Volumes \u00b6 Edit the below parameters as required with your public cloud info and save it as hpe-config.yaml . kind: ConfigMap apiVersion: v1 metadata: name: hpe-config namespace: kube-system data: volume-driver.json: |- { \"global\": { \"snapPrefix\": \"BaseFor\", \"initiators\": [\"eth0\"], \"automatedConnection\": true, \"existingCloudSubnet\": \"10.1.0.0/24\", \"region\": \"us-east-1\", \"privateCloud\": \"vpc-data\", \"cloudComputeProvider\": \"Amazon AWS\" }, \"defaults\": { \"limitIOPS\": 1000, \"fsOwner\": \"0:0\", \"fsMode\": \"600\", \"description\": \"Volume provisioned by the HPE Volume Driver for Kubernetes FlexVolume Plugin\", \"perfPolicy\": \"Other\", \"protectionTemplate\": \"twicedaily:4\", \"encryption\": true, \"volumeType\": \"PF\", \"destroyOnRm\": true }, \"overrides\": { } } Create the ConfigMap : kubectl create -f hpe-config.yaml configmap/hpe-config created Step 3. Deploy the FlexVolume driver and dynamic provisioner \u00b6 Deploy the driver as a DaemonSet and the dynamic provisioner as a Deployment . HPE Nimble Storage \u00b6 Version 3.0.0: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-nimble-storage/hpe-flexvolume-driver-v3.0.0.yaml Version 3.1.0: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-nimble-storage/hpe-flexvolume-driver-v3.1.0.yaml HPE Cloud Volumes \u00b6 Container-Provider Service: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-cloud-volumes/hpecv-cp-v3.1.0.yaml The FlexVolume driver have different declarations depending on the Kubernetes distribution. Amazon EKS: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-cloud-volumes/hpecv-aws-flexvolume-driver-v3.1.0.yaml Microsoft Azure AKS: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-cloud-volumes/hpecv-azure-flexvolume-driver-v3.1.0.yaml Generic: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-cloud-volumes/hpecv-flexvolume-driver-v3.1.0.yaml Note The declarations for HPE Volume Driver for Kubernetes FlexVolume Plugin can be found in the co-deployments repository. Check to see all hpe-flexvolume-driver Pods (one per compute node) and the hpe-dynamic-provisioner Pod are running. kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE hpe-flexvolume-driver-2rdt4 1/1 Running 0 45s hpe-flexvolume-driver-md562 1/1 Running 0 44s hpe-flexvolume-driver-x4k96 1/1 Running 0 44s hpe-dynamic-provisioner-59f9d495d4-hxh29 1/1 Running 0 24s For HPE Cloud Volumes, check that hpe-cv-cp pod is running as well. kubectl get pods -n kube-system -l=app=cv-cp NAME READY STATUS RESTARTS AGE hpe-cv-cp-2rdt4 1/1 Running 0 45s Using \u00b6 Get started using the FlexVolume driver by setting up StorageClass , PVC API objects. See Using for examples. These instructions are provided as an example on how to use the HPE Volume Driver for Kubernetes FlexVolume Plugin with a HPE Nimble Storage Array. The below YAML declarations are meant to be created with kubectl create . Either copy the content to a file on the host where kubectl is being executed, or copy & paste into the terminal, like this: kubectl create -f- < paste the YAML > ^D (CTRL + D) Tip Some of the examples supported by the HPE Volume Driver for Kubernetes FlexVolume Plugin are available for HPE Nimble Storage or HPE Cloud Volumes in the GitHub repo. To get started, create a StorageClass API object referencing the hpe-secret and defining additional (optional) StorageClass parameters: Sample StorageClass \u00b6 Sample storage classes can be found for HPE Nimble Storage and HPE Cloud Volumes . Hint See StorageClass parameters for HPE Nimble Storage and HPE Clound Volumes for a comprehensive overview. Test and verify volume provisioning \u00b6 Create a StorageClass with volume parameters as required. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: sc-nimble provisioner: hpe.com/nimble parameters: description: \"Volume from HPE FlexVolume driver\" perfPolicy: \"Other Workloads\" limitIOPS: \"76800\" Create a PersistentVolumeClaim . This makes sure a volume is created and provisioned on your behalf: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-nimble spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: sc-nimble Check that a new PersistentVolume is created based on your claim: kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE sc-nimble-13336da3-7ca3-11e9-826c-00505693581f 10Gi RWO Delete Bound default/pvc-nimble sc-nimble 3s The above output means that the FlexVolume driver successfully provisioned a new volume and bound to the requesting PVC to a new PV . The volume is not attached to any node yet. It will only be attached to a node if a workload is scheduled to a specific node. Now let us create a Pod that refers to the above volume. When the Pod is created, the volume will be attached, formatted and mounted to the specified container: kind: Pod apiVersion: v1 metadata: name: pod-nimble spec: containers: - name: pod-nimble-con-1 image: nginx command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: export1 mountPath: /data - name: pod-nimble-cont-2 image: debian command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: export1 mountPath: /data volumes: - name: export1 persistentVolumeClaim: claimName: pvc-nimble Check if the pod is running successfully: kubectl get pod pod-nimble NAME READY STATUS RESTARTS AGE pod-nimble 2/2 Running 0 2m29s Use case specific examples \u00b6 This StorageClass examples help guide combinations of options when provisioning volumes. Data protection \u00b6 This StorageClass creates thinly provisioned volumes with deduplication turned on. It will also apply the Performance Policy \"SQL Server\" along with a Protection Template. The Protection Template needs to be defined on the array. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: oltp-prod provisioner: hpe.com/nimble parameters: thick: \"false\" dedupe: \"true\" perfPolicy: \"SQL Server\" protectionTemplate: \"Retain-48Hourly-30Daily-52Weekly\" Clone and throttle for devs \u00b6 This StorageClass will create clones of a \"production\" volume and throttle the performance of each clone to 1000 IOPS. When the PVC is deleted, it will be permanently deleted from the backend array. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: oltp-dev-clone-of-prod provisioner: hpe.com/nimble parameters: limitIOPS: \"1000\" cloneOf: \"oltp-prod-1adee106-110b-11e8-ac84-00505696c45f\" destroyOnRm: \"true\" Clone a non-containerized volume \u00b6 This StorageClass will clone a standard backend volume (without container metadata on it) from a particular pool on the backend. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: import-clone-legacy-prod rovisioner: hpe.com/nimble parameters: pool: \"flash\" importVolAsClone: \"production-db-vol\" destroyOnRm: \"true\" Import (cutover) a volume \u00b6 This StorageClass will import an existing Nimble volume to Kubernetes. The source volume needs to be offline for the import to succeed. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: import-clone-legacy-prod provisioner: hpe.com/nimble parameters: pool: \"flash\" importVol: \"production-db-vol\" Using overrides \u00b6 The HPE Dynamic Provisioner for Kubernetes understands a set of annotation keys a user can set on a PVC . If the corresponding keys exists in the list of the allowOverrides key in the StorageClass , the end-user can tweak certain aspects of the provisioning workflow. This opens up for very advanced data services. StorageClass object: apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: my-sc provisioner: hpe.com/nimble parameters: description: \"Volume provisioned by StorageClass my-sc\" dedupe: \"false\" destroyOnRm: \"true\" perfPolicy: \"Windows File Server\" folder: \"myfolder\" allowOverrides: snapshot,limitIOPS,perfPolicy PersistentVolumeClaim object: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc annotations: hpe.com/description: \"This is my custom description\" hpe.com/limitIOPS: \"8000\" hpe.com/perfPolicy: \"SQL Server\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: my-sc This will create a PV of 8000 IOPS with the Performance Policy of \"SQL Server\" and a custom volume description. Creating clones of PVCs \u00b6 Using a StorageClass to clone a PV is practical when there's needs to clone across namespaces (for example from prod to test or stage). If a user wants to clone any arbitrary volume, it becomes a bit tedious to create a StorageClass for each clone. The annotation hpe.com/CloneOfPVC allows a user to clone any PVC within a namespace. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-clone annotations: hpe.com/cloneOfPVC: my-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: my-sc StorageClass parameters \u00b6 This section highlights all the available StorageClass parameters that are supported. HPE Nimble Storage StorageClass parameters \u00b6 A StorageClass is used to provision or clone an HPE Nimble Storage-backed persistent volume. It can also be used to import an existing HPE Nimble Storage volume or clone of a snapshot into the Kubernetes cluster. The parameters are grouped below by those same workflows. A sample StorageClass is provided. Note These are optional parameters. Common parameters for Provisioning and Cloning \u00b6 These parameters are mutable betweeen a parent volume and creating a clone from a snapshot. Parameter String Description nameSuffix Text Suffix to append to Nimble volumes. Defaults to .docker destroyOnRm Boolean Indicates the backing Nimble volume (including snapshots) should be destroyed when the PVC is deleted. limitIOPS Integer The IOPS limit of the volume. The IOPS limit should be in the range 256 to 4294967294, or -1 for unlimited (default). limitMBPS Integer The MB/s throughput limit for the volume. description Text Text to be added to the volume's description on the Nimble array. perfPolicy Text The name of the performance policy to assign to the volume. Default example performance policies include \"Backup Repository\", \"Exchange 2003 data store\", \"Exchange 2007 data store\", \"Exchange 2010 data store\", \"Exchange log\", \"Oracle OLTP\", \"Other Workloads\", \"SharePoint\", \"SQL Server\", \"SQL Server 2012\", \"SQL Server Logs\". protectionTemplate Text The name of the protection template to assign to the volume. Default examples of protection templates include \"Retain-30Daily\", \"Retain-48Hourly-30aily-52Weekly\", and \"Retain-90Daily\". folder Text The name of the Nimble folder in which to place the volume. thick Boolean Indicates that the volume should be thick provisioned. dedupeEnabled Boolean Indicates that the volume should enable deduplication. syncOnUnmount Boolean Indicates that a snapshot of the volume should be synced to the replication partner each time it is detached from a node. Note Performance Policies, Folders and Protection Templates are Nimble specific constructs that can be created on the Nimble array itself to address particular requirements or workloads. Please consult with the storage admin or read the admin guide found on HPE InfoSight . Provisioning parameters \u00b6 These parameters are immutable for clones once a volume has been created. Parameter String Description fsOwner userId:groupId The user id and group id that should own the root directory of the filesystem. fsMode Octal digits 1 to 4 octal digits that represent the file mode to be applied to the root directory of the filesystem. encryption Boolean Indicates that the volume should be encrypted. pool Text The name of the pool in which to place the volume. Cloning parameters \u00b6 Cloning supports two modes of cloning. Either use cloneOf and reference a PVC in the current namespace or use importVolAsClone and reference a Nimble volume name to clone and import to Kubernetes. Parameter String Description cloneOf Text The name of the PV to be cloned. cloneOf and importVolAsClone are mutually exclusive. importVolAsClone Text The name of the Nimble volume to clone and import. importVolAsClone and cloneOf are mutually exclusive. snapshot Text The name of the snapshot to base the clone on. This is optional. If not specified, a new snapshot is created. createSnapshot Boolean Indicates that a new snapshot of the volume should be taken matching the name provided in the snapshot parameter. If the snapshot parameter is not specified, a default name will be created. snapshotPrefix Text A prefix to add to the beginning of the snapshot name. Import parameters \u00b6 Importing volumes to Kubernetes requires the source Nimble volume to be offline. All previous Access Control Records and Initiator Groups will be stripped from the volume when put under control of the HPE Volume Driver for Kubernetes FlexVolume Plugin. Parameter String Description importVol Text The name of the Nimble volume to import. snapshot Text The name of the Nimble snapshot to restore the imported volume to after takeover. If not specified, the volume will not be restored. restore Boolean Restores the volume to the last snapshot taken on the volume. takeover Boolean Indicates the current group will takeover ownership of the Nimble volume and volume collection. This should be performed against a downstream replica. reverseRepl Boolean Reverses the replication direction so that writes to the Nimble volume are replicated back to the group where it was replicated from. forceImport Boolean Forces the import of a volume that is not owned by the group and is not part of a volume collection. If the volume is part of a volume collection, use takeover instead. Note HPE Nimble Docker Volume workflows works with a 1-1 mapping between volume and volume collection. HPE Cloud Volumes StorageClass parameters \u00b6 A StorageClass is used to provision or clone an HPE Cloud Volumes-backed persistent volume. It can also be used to import an existing HPE Cloud Volumes volume or clone of a snapshot into the Kubernetes cluster. The parameters are grouped below by those same workflows. A sample StorageClass is provided. Note These are optional parameters. Common parameters for Provisioning and Cloning \u00b6 These parameters are mutable betweeen a parent volume and creating a clone from a snapshot. Parameter String Description nameSuffix Text Suffix to append to Cloud Volumes. destroyOnRm Boolean Indicates the backing Cloud volume (including snapshots) should be destroyed when the PVC is deleted. limitIOPS Integer The IOPS limit of the volume. The IOPS limit should be in the range 300 to 50000. perfPolicy Text The name of the performance policy to assign to the volume. Default example performance policies include \"Other, Exchange, Oracle, SharePoint, SQL, Windows File Server\". protectionTemplate Text The name of the protection template to assign to the volume. Default examples of protection templates include \"daily:3, daily:7, daily:14, hourly:6, hourly:12, hourly:24, twicedaily:4, twicedaily:8, twicedaily:14, weekly:2, weekly:4, weekly:8, monthly:3, monthly:6, monthly:12 or none\". volumeType Text Cloud Volume type. Supported types are PF and GPF. Provisioning parameters \u00b6 These parameters are immutable for clones once a volume has been created. Parameter String Description fsOwner userId:groupId The user id and group id that should own the root directory of the filesystem. fsMode Octal digits 1 to 4 octal digits that represent the file mode to be applied to the root directory of the filesystem. encryption Boolean Indicates that the volume should be encrypted. Cloning parameters \u00b6 Cloning supports two modes of cloning. Either use cloneOf and reference a PVC in the current namespace or use importVolAsClone and reference a Cloud volume name to clone and import to Kubernetes. Parameter String Description cloneOf Text The name of the PV to be cloned. cloneOf and importVolAsClone are mutually exclusive. importVolAsClone Text The name of the Cloud Volume volume to clone and import. importVolAsClone and cloneOf are mutually exclusive. snapshot Text The name of the snapshot to base the clone on. This is optional. If not specified, a new snapshot is created. createSnapshot Boolean Indicates that a new snapshot of the volume should be taken matching the name provided in the snapshot parameter. If the snapshot parameter is not specified, a default name will be created. snapshotPrefix Text A prefix to add to the beginning of the snapshot name. replStore Text Replication store name. Should be used with importVolAsClone parameter to clone a replica volume Import parameters \u00b6 Importing volumes to Kubernetes requires the source Cloud volume to be not attached to any nodes. All previous Access Control Records will be stripped from the volume when put under control of the HPE Volume Driver for Kubernetes FlexVolume Plugin. Parameter String Description importVol Text The name of the Cloud volume to import. forceImport Boolean Forces the import of a volume that is provisioned by another K8s cluster but not attached to any nodes. Diagnostics \u00b6 This section outlines a few troubleshooting steps for the HPE Volume Driver for Kubernetes Plugin. This product is supported by HPE, please consult with your support organization (Nimble, Cloud Volumes etc) prior attempting any configuration changes. Troubleshooting FlexVolume driver \u00b6 The FlexVolume driver is a binary executed by the kubelet to perform mount/unmount/attach/detach operations as workloads request storage resources. The binary relies on communicating with a socket on the host where the volume plugin responsible for the MUAD operations perform control-plane or data-plane operations against the backend system hosting the actual volumes. Locations \u00b6 The driver has a configuration file where certain defaults can be tweaked to accommodate a certain behavior. Under normal circumstances, this file does not need any tweaking. The name and the location of the binary varies based on Kubernetes distribution (the default 'exec' path) and what backend driver is being used. In a typical scenario, using Nimble, this is expected: Binary: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~nimble/nimble Config file: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~nimble/nimble.json Override defaults \u00b6 By default, it contains only the path to the socket file for the volume plugin: { \"dockerVolumePluginSocketPath\": \"/etc/hpe-storage/nimble.sock\" } Valid options for the FlexVolume driver can be inspected by executing the binary on the host with the config argument: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~nimble/nimble config Error processing option 'logFilePath' - key:logFilePath not found Error processing option 'logDebug' - key:logDebug not found Error processing option 'supportsCapabilities' - key:supportsCapabilities not found Error processing option 'stripK8sFromOptions' - key:stripK8sFromOptions not found Error processing option 'createVolumes' - key:createVolumes not found Error processing option 'listOfStorageResourceOptions' - key:listOfStorageResourceOptions not found Error processing option 'factorForConversion' - key:factorForConversion not found Error processing option 'enable1.6' - key:enable1.6 not found Driver=nimble Version=v2.5.1-50fbff2aa14a693a9a18adafb834da33b9e7cc89 Current Config: dockerVolumePluginSocketPath = /etc/hpe-storage/nimble.sock stripK8sFromOptions = true logFilePath = /var/log/dory.log logDebug = false createVolumes = false enable1.6 = false factorForConversion = 1073741824 listOfStorageResourceOptions = [size sizeInGiB] supportsCapabilities = true An example tweak could be to enable debug logging and enable support for Kubernetes 1.6 (which we don't officially support). The config file would then end up like this: { \"dockerVolumePluginSocketPath\": \"/etc/hpe-storage/nimble.sock\", \"logDebug\": true, \"enable1.6\": true } Execute the binary again ( nimble config ) to ensure the parameters and config file gets parsed correctly. Since the config file is read on each FlexVolume operation, no restart of anything is needed. See Advanced for more parameters for the driver.json file. Connectivity \u00b6 To verify the FlexVolume binary can actually communicate with the backend volume plugin, issue a faux mount request: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~nimble/nimble mount no/op '{\"name\":\"myvol1\"}' If the FlexVolume driver can successfully communicate with the volume plugin socket: {\"status\":\"Failure\",\"message\":\"configured to NOT create volumes\"} In the case of any other output, check if the backend volume plugin is alive with curl : curl --unix-socket /etc/hpe-storage/nimble.sock -d '{}' http://localhost/VolumeDriver.Capabilities It should output: {\"capabilities\":{\"scope\":\"global\"},\"Err\":\"\"} FlexVolume and dynamic provisioner driver logs \u00b6 Log files associated with the HPE Volume Driver for Kubernetes FlexVolume Plugin logs data to the standard output stream. If the logs need to be retained for long term, use a standard logging solution. Some of the logs on the host are persisted which follow standard logrotate policies. FlexVolume driver logs: kubectl logs -f daemonset.apps/hpe-flexvolume-driver -n kube-system The logs are persisted at /var/log/hpe-docker-plugin.log and /var/log/dory.log Dynamic Provisioner logs: kubectl logs -f deployment.apps/hpe-dynamic-provisioner -n kube-system The logs are persisted at /var/log/hpe-dynamic-provisioner.log Log Collector \u00b6 Log collector script hpe-logcollector.sh can be used to collect diagnostic logs using kubectl Download the script as follows: curl -O https://raw.githubusercontent.com/hpe-storage/flexvolume-driver/master/hpe-logcollector.sh chmod 555 hpe-logcollector.sh Usage: ./hpe-logcollector.sh -h Diagnostic Script to collect HPE Storage logs using kubectl Usage: hpe-logcollector.sh [-h|--help][--node-name NODE_NAME][-n|--namespace NAMESPACE][-a|--all] Where -h|--help Print the Usage text --node-name NODE_NAME where NODE_NAME is kubernetes Node Name needed to collect the hpe diagnostic logs of the Node -n|--namespace NAMESPACE where NAMESPACE is namespace of the pod deployment. default is kube-system -a|--all collect diagnostic logs of all the nodes.If nothing is specified logs would be collected from all the nodes Advanced Configuration \u00b6 This section describes some of the advanced configuration steps available to tweak behavior of the HPE Volume Driver for Kubernetes FlexVolume Plugin. Set defaults at the compute node level \u00b6 During normal operations, defaults are set in either the ConfigMap or in a StorageClass itself. The picking order is: StorageClass ConfigMap driver.json Please see Diagnostics to locate the driver for your particular environment. Add this object to the configuration file, nimble.json , for example: { \"defaultOptions\": [{\"option1\": \"value1\"}, {\"option2\": \"value2\"}] } Where option1 and option2 are valid backend volume plugin create options. Note It's highly recommended to control defaults with StorageClass API objects or the ConfigMap . Global options \u00b6 Each driver supports setting certain \"global\" options in the ConfigMap . Some options are common, some are driver specific. Common \u00b6 Parameter String Description volumeDir Text Root directory on the host to mount the volumes. This parameter needs correlation with the podsmountdir path in the volumeMounts stanzas of the deployment. logDebug Boolean Turn on debug logging, set to false by default.","title":"Container Provider: Nimble and CV"},{"location":"flexvolume_driver/container_provider/index.html#overview","text":"The HPE Volume Driver for Kubernetes FlexVolume Plugin leverages HPE Nimble Storage or HPE Cloud Volumes to provide scalable and persistent storage for stateful applications. Important Using HPE Nimble Storage with Kubernetes 1.13 and newer, please use the HPE CSI Driver for Kubernetes . Source code and developer documentation is available in the hpe-storage/flexvolume-driver GitHub repo. Overview Platform requirements HPE Nimble Storage Platform Requirements HPE Cloud Volumes Platform Requirements Deploying to Kubernetes Step 1: Create a secret HPE Nimble Storage HPE Cloud Volumes Step 2. Create a ConfigMap HPE Nimble Storage HPE Cloud Volumes Step 3. Deploy the FlexVolume driver and dynamic provisioner HPE Nimble Storage HPE Cloud Volumes Using Sample StorageClass Test and verify volume provisioning Use case specific examples Data protection Clone and throttle for devs Clone a non-containerized volume Import (cutover) a volume Using overrides Creating clones of PVCs StorageClass parameters HPE Nimble Storage StorageClass parameters Common parameters for Provisioning and Cloning Provisioning parameters Cloning parameters Import parameters HPE Cloud Volumes StorageClass parameters Common parameters for Provisioning and Cloning Provisioning parameters Cloning parameters Import parameters Diagnostics Troubleshooting FlexVolume driver Locations Override defaults Connectivity FlexVolume and dynamic provisioner driver logs Log Collector Advanced Configuration Set defaults at the compute node level Global options Common","title":"Overview"},{"location":"flexvolume_driver/container_provider/index.html#platform_requirements","text":"The FlexVolume driver supports multiple backends that are based on a \"container provider\" architecture. Currently, Nimble and Cloud Volumes are supported.","title":"Platform requirements"},{"location":"flexvolume_driver/container_provider/index.html#hpe_nimble_storage_platform_requirements","text":"Driver HPE Nimble Storage Version Release Notes Blog v3.0.0 5.0.8.x and 5.1.3.x onwards v3.0.0 HPE Storage Tech Insiders v3.1.0 5.0.8.x and 5.1.3.x onwards v3.1.0 OpenShift Container Platform 3.9, 3.10 and 3.11. Kubernetes 1.10 and above. Redhat/CentOS 7.5+ Ubuntu 16.04/18.04 LTS Note: Synchronous replication (Peer Persistence) is not supported by the HPE Volume Driver for Kubernetes FlexVolume Plugin.","title":"HPE Nimble Storage Platform Requirements"},{"location":"flexvolume_driver/container_provider/index.html#hpe_cloud_volumes_platform_requirements","text":"Driver Release Notes Blog v3.1.0 v3.1.0 Using HPE Cloud Volumes with Amazon EKS Amazon EKS 1.12/1.13 Microsoft Azure AKS 1.12/1.13 US regions only","title":"HPE Cloud Volumes Platform Requirements"},{"location":"flexvolume_driver/container_provider/index.html#deploying_to_kubernetes","text":"The recommended way to deploy and manage the HPE Volume Driver for Kubernetes FlexVolume Plugin is to use Helm. Please see the co-deployments repository for further information. Use the following steps for a manual installation.","title":"Deploying to Kubernetes"},{"location":"flexvolume_driver/container_provider/index.html#step_1_create_a_secret","text":"","title":"Step 1: Create a secret"},{"location":"flexvolume_driver/container_provider/index.html#hpe_nimble_storage","text":"Replace the password string ( YWRtaW4= ) below with a base64 encoded version of your password and replace the backend with your array IP address and save it as hpe-secret.yaml . apiVersion: v1 kind: Secret metadata: name: hpe-secret namespace: kube-system stringData: backend: 192.168.1.1 username: admin protocol: \"iscsi\" data: # echo -n \"admin\" | base64 password: YWRtaW4=","title":"HPE Nimble Storage"},{"location":"flexvolume_driver/container_provider/index.html#hpe_cloud_volumes","text":"Replace the username and password strings ( YWRtaW4= ) with a base64 encoded version of your HPE Cloud Volumes \"access_key\" and \"access_secret\". Also, replace the backend with HPE Cloud Volumes portal fully qualified domain name (FQDN) and save it as hpe-secret.yaml . apiVersion: v1 kind: Secret metadata: name: hpe-secret namespace: kube-system stringData: backend: cloudvolumes.hpe.com protocol: \"iscsi\" serviceName: cv-cp-svc servicePort: \"8080\" data: # echo -n \"<my very confidential access key>\" | base64 username: YWRtaW4= # echo -n \"<my very confidential secret key>\" | base64 password: YWRtaW4= Create the secret: kubectl create -f hpe-secret.yaml secret \"hpe-secret\" created You should now see the HPE secret in the kube-system namespace. kubectl get secret/hpe-secret -n kube-system NAME TYPE DATA AGE hpe-secret Opaque 5 3s","title":"HPE Cloud Volumes"},{"location":"flexvolume_driver/container_provider/index.html#step_2_create_a_configmap","text":"The ConfigMap is used to set and tweak defaults for both the FlexVolume driver and Dynamic Provisioner.","title":"Step 2. Create a ConfigMap"},{"location":"flexvolume_driver/container_provider/index.html#hpe_nimble_storage_1","text":"Edit the below default parameters as required for FlexVolume driver and save it as hpe-config.yaml . kind: ConfigMap apiVersion: v1 metadata: name: hpe-config namespace: kube-system data: volume-driver.json: |- { \"global\": {}, \"defaults\": { \"limitIOPS\":\"-1\", \"limitMBPS\":\"-1\", \"perfPolicy\": \"Other\" }, \"overrides\":{} } Tip Please see Advanced for more volume-driver.json configuration options.","title":"HPE Nimble Storage"},{"location":"flexvolume_driver/container_provider/index.html#hpe_cloud_volumes_1","text":"Edit the below parameters as required with your public cloud info and save it as hpe-config.yaml . kind: ConfigMap apiVersion: v1 metadata: name: hpe-config namespace: kube-system data: volume-driver.json: |- { \"global\": { \"snapPrefix\": \"BaseFor\", \"initiators\": [\"eth0\"], \"automatedConnection\": true, \"existingCloudSubnet\": \"10.1.0.0/24\", \"region\": \"us-east-1\", \"privateCloud\": \"vpc-data\", \"cloudComputeProvider\": \"Amazon AWS\" }, \"defaults\": { \"limitIOPS\": 1000, \"fsOwner\": \"0:0\", \"fsMode\": \"600\", \"description\": \"Volume provisioned by the HPE Volume Driver for Kubernetes FlexVolume Plugin\", \"perfPolicy\": \"Other\", \"protectionTemplate\": \"twicedaily:4\", \"encryption\": true, \"volumeType\": \"PF\", \"destroyOnRm\": true }, \"overrides\": { } } Create the ConfigMap : kubectl create -f hpe-config.yaml configmap/hpe-config created","title":"HPE Cloud Volumes"},{"location":"flexvolume_driver/container_provider/index.html#step_3_deploy_the_flexvolume_driver_and_dynamic_provisioner","text":"Deploy the driver as a DaemonSet and the dynamic provisioner as a Deployment .","title":"Step 3. Deploy the FlexVolume driver and dynamic provisioner"},{"location":"flexvolume_driver/container_provider/index.html#hpe_nimble_storage_2","text":"Version 3.0.0: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-nimble-storage/hpe-flexvolume-driver-v3.0.0.yaml Version 3.1.0: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-nimble-storage/hpe-flexvolume-driver-v3.1.0.yaml","title":"HPE Nimble Storage"},{"location":"flexvolume_driver/container_provider/index.html#hpe_cloud_volumes_2","text":"Container-Provider Service: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-cloud-volumes/hpecv-cp-v3.1.0.yaml The FlexVolume driver have different declarations depending on the Kubernetes distribution. Amazon EKS: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-cloud-volumes/hpecv-aws-flexvolume-driver-v3.1.0.yaml Microsoft Azure AKS: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-cloud-volumes/hpecv-azure-flexvolume-driver-v3.1.0.yaml Generic: kubectl create -f https://raw.githubusercontent.com/hpe-storage/co-deployments/master/yaml/flexvolume-driver/hpe-cloud-volumes/hpecv-flexvolume-driver-v3.1.0.yaml Note The declarations for HPE Volume Driver for Kubernetes FlexVolume Plugin can be found in the co-deployments repository. Check to see all hpe-flexvolume-driver Pods (one per compute node) and the hpe-dynamic-provisioner Pod are running. kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE hpe-flexvolume-driver-2rdt4 1/1 Running 0 45s hpe-flexvolume-driver-md562 1/1 Running 0 44s hpe-flexvolume-driver-x4k96 1/1 Running 0 44s hpe-dynamic-provisioner-59f9d495d4-hxh29 1/1 Running 0 24s For HPE Cloud Volumes, check that hpe-cv-cp pod is running as well. kubectl get pods -n kube-system -l=app=cv-cp NAME READY STATUS RESTARTS AGE hpe-cv-cp-2rdt4 1/1 Running 0 45s","title":"HPE Cloud Volumes"},{"location":"flexvolume_driver/container_provider/index.html#using","text":"Get started using the FlexVolume driver by setting up StorageClass , PVC API objects. See Using for examples. These instructions are provided as an example on how to use the HPE Volume Driver for Kubernetes FlexVolume Plugin with a HPE Nimble Storage Array. The below YAML declarations are meant to be created with kubectl create . Either copy the content to a file on the host where kubectl is being executed, or copy & paste into the terminal, like this: kubectl create -f- < paste the YAML > ^D (CTRL + D) Tip Some of the examples supported by the HPE Volume Driver for Kubernetes FlexVolume Plugin are available for HPE Nimble Storage or HPE Cloud Volumes in the GitHub repo. To get started, create a StorageClass API object referencing the hpe-secret and defining additional (optional) StorageClass parameters:","title":"Using"},{"location":"flexvolume_driver/container_provider/index.html#sample_storageclass","text":"Sample storage classes can be found for HPE Nimble Storage and HPE Cloud Volumes . Hint See StorageClass parameters for HPE Nimble Storage and HPE Clound Volumes for a comprehensive overview.","title":"Sample StorageClass"},{"location":"flexvolume_driver/container_provider/index.html#test_and_verify_volume_provisioning","text":"Create a StorageClass with volume parameters as required. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: sc-nimble provisioner: hpe.com/nimble parameters: description: \"Volume from HPE FlexVolume driver\" perfPolicy: \"Other Workloads\" limitIOPS: \"76800\" Create a PersistentVolumeClaim . This makes sure a volume is created and provisioned on your behalf: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-nimble spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: sc-nimble Check that a new PersistentVolume is created based on your claim: kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE sc-nimble-13336da3-7ca3-11e9-826c-00505693581f 10Gi RWO Delete Bound default/pvc-nimble sc-nimble 3s The above output means that the FlexVolume driver successfully provisioned a new volume and bound to the requesting PVC to a new PV . The volume is not attached to any node yet. It will only be attached to a node if a workload is scheduled to a specific node. Now let us create a Pod that refers to the above volume. When the Pod is created, the volume will be attached, formatted and mounted to the specified container: kind: Pod apiVersion: v1 metadata: name: pod-nimble spec: containers: - name: pod-nimble-con-1 image: nginx command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: export1 mountPath: /data - name: pod-nimble-cont-2 image: debian command: [\"bin/sh\"] args: [\"-c\", \"while true; do date >> /data/mydata.txt; sleep 1; done\"] volumeMounts: - name: export1 mountPath: /data volumes: - name: export1 persistentVolumeClaim: claimName: pvc-nimble Check if the pod is running successfully: kubectl get pod pod-nimble NAME READY STATUS RESTARTS AGE pod-nimble 2/2 Running 0 2m29s","title":"Test and verify volume provisioning"},{"location":"flexvolume_driver/container_provider/index.html#use_case_specific_examples","text":"This StorageClass examples help guide combinations of options when provisioning volumes.","title":"Use case specific examples"},{"location":"flexvolume_driver/container_provider/index.html#data_protection","text":"This StorageClass creates thinly provisioned volumes with deduplication turned on. It will also apply the Performance Policy \"SQL Server\" along with a Protection Template. The Protection Template needs to be defined on the array. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: oltp-prod provisioner: hpe.com/nimble parameters: thick: \"false\" dedupe: \"true\" perfPolicy: \"SQL Server\" protectionTemplate: \"Retain-48Hourly-30Daily-52Weekly\"","title":"Data protection"},{"location":"flexvolume_driver/container_provider/index.html#clone_and_throttle_for_devs","text":"This StorageClass will create clones of a \"production\" volume and throttle the performance of each clone to 1000 IOPS. When the PVC is deleted, it will be permanently deleted from the backend array. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: oltp-dev-clone-of-prod provisioner: hpe.com/nimble parameters: limitIOPS: \"1000\" cloneOf: \"oltp-prod-1adee106-110b-11e8-ac84-00505696c45f\" destroyOnRm: \"true\"","title":"Clone and throttle for devs"},{"location":"flexvolume_driver/container_provider/index.html#clone_a_non-containerized_volume","text":"This StorageClass will clone a standard backend volume (without container metadata on it) from a particular pool on the backend. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: import-clone-legacy-prod rovisioner: hpe.com/nimble parameters: pool: \"flash\" importVolAsClone: \"production-db-vol\" destroyOnRm: \"true\"","title":"Clone a non-containerized volume"},{"location":"flexvolume_driver/container_provider/index.html#import_cutover_a_volume","text":"This StorageClass will import an existing Nimble volume to Kubernetes. The source volume needs to be offline for the import to succeed. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: import-clone-legacy-prod provisioner: hpe.com/nimble parameters: pool: \"flash\" importVol: \"production-db-vol\"","title":"Import (cutover) a volume"},{"location":"flexvolume_driver/container_provider/index.html#using_overrides","text":"The HPE Dynamic Provisioner for Kubernetes understands a set of annotation keys a user can set on a PVC . If the corresponding keys exists in the list of the allowOverrides key in the StorageClass , the end-user can tweak certain aspects of the provisioning workflow. This opens up for very advanced data services. StorageClass object: apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: my-sc provisioner: hpe.com/nimble parameters: description: \"Volume provisioned by StorageClass my-sc\" dedupe: \"false\" destroyOnRm: \"true\" perfPolicy: \"Windows File Server\" folder: \"myfolder\" allowOverrides: snapshot,limitIOPS,perfPolicy PersistentVolumeClaim object: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc annotations: hpe.com/description: \"This is my custom description\" hpe.com/limitIOPS: \"8000\" hpe.com/perfPolicy: \"SQL Server\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: my-sc This will create a PV of 8000 IOPS with the Performance Policy of \"SQL Server\" and a custom volume description.","title":"Using overrides"},{"location":"flexvolume_driver/container_provider/index.html#creating_clones_of_pvcs","text":"Using a StorageClass to clone a PV is practical when there's needs to clone across namespaces (for example from prod to test or stage). If a user wants to clone any arbitrary volume, it becomes a bit tedious to create a StorageClass for each clone. The annotation hpe.com/CloneOfPVC allows a user to clone any PVC within a namespace. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc-clone annotations: hpe.com/cloneOfPVC: my-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: my-sc","title":"Creating clones of PVCs"},{"location":"flexvolume_driver/container_provider/index.html#storageclass_parameters","text":"This section highlights all the available StorageClass parameters that are supported.","title":"StorageClass parameters"},{"location":"flexvolume_driver/container_provider/index.html#hpe_nimble_storage_storageclass_parameters","text":"A StorageClass is used to provision or clone an HPE Nimble Storage-backed persistent volume. It can also be used to import an existing HPE Nimble Storage volume or clone of a snapshot into the Kubernetes cluster. The parameters are grouped below by those same workflows. A sample StorageClass is provided. Note These are optional parameters.","title":"HPE Nimble Storage StorageClass parameters"},{"location":"flexvolume_driver/container_provider/index.html#common_parameters_for_provisioning_and_cloning","text":"These parameters are mutable betweeen a parent volume and creating a clone from a snapshot. Parameter String Description nameSuffix Text Suffix to append to Nimble volumes. Defaults to .docker destroyOnRm Boolean Indicates the backing Nimble volume (including snapshots) should be destroyed when the PVC is deleted. limitIOPS Integer The IOPS limit of the volume. The IOPS limit should be in the range 256 to 4294967294, or -1 for unlimited (default). limitMBPS Integer The MB/s throughput limit for the volume. description Text Text to be added to the volume's description on the Nimble array. perfPolicy Text The name of the performance policy to assign to the volume. Default example performance policies include \"Backup Repository\", \"Exchange 2003 data store\", \"Exchange 2007 data store\", \"Exchange 2010 data store\", \"Exchange log\", \"Oracle OLTP\", \"Other Workloads\", \"SharePoint\", \"SQL Server\", \"SQL Server 2012\", \"SQL Server Logs\". protectionTemplate Text The name of the protection template to assign to the volume. Default examples of protection templates include \"Retain-30Daily\", \"Retain-48Hourly-30aily-52Weekly\", and \"Retain-90Daily\". folder Text The name of the Nimble folder in which to place the volume. thick Boolean Indicates that the volume should be thick provisioned. dedupeEnabled Boolean Indicates that the volume should enable deduplication. syncOnUnmount Boolean Indicates that a snapshot of the volume should be synced to the replication partner each time it is detached from a node. Note Performance Policies, Folders and Protection Templates are Nimble specific constructs that can be created on the Nimble array itself to address particular requirements or workloads. Please consult with the storage admin or read the admin guide found on HPE InfoSight .","title":"Common parameters for Provisioning and Cloning"},{"location":"flexvolume_driver/container_provider/index.html#provisioning_parameters","text":"These parameters are immutable for clones once a volume has been created. Parameter String Description fsOwner userId:groupId The user id and group id that should own the root directory of the filesystem. fsMode Octal digits 1 to 4 octal digits that represent the file mode to be applied to the root directory of the filesystem. encryption Boolean Indicates that the volume should be encrypted. pool Text The name of the pool in which to place the volume.","title":"Provisioning parameters"},{"location":"flexvolume_driver/container_provider/index.html#cloning_parameters","text":"Cloning supports two modes of cloning. Either use cloneOf and reference a PVC in the current namespace or use importVolAsClone and reference a Nimble volume name to clone and import to Kubernetes. Parameter String Description cloneOf Text The name of the PV to be cloned. cloneOf and importVolAsClone are mutually exclusive. importVolAsClone Text The name of the Nimble volume to clone and import. importVolAsClone and cloneOf are mutually exclusive. snapshot Text The name of the snapshot to base the clone on. This is optional. If not specified, a new snapshot is created. createSnapshot Boolean Indicates that a new snapshot of the volume should be taken matching the name provided in the snapshot parameter. If the snapshot parameter is not specified, a default name will be created. snapshotPrefix Text A prefix to add to the beginning of the snapshot name.","title":"Cloning parameters"},{"location":"flexvolume_driver/container_provider/index.html#import_parameters","text":"Importing volumes to Kubernetes requires the source Nimble volume to be offline. All previous Access Control Records and Initiator Groups will be stripped from the volume when put under control of the HPE Volume Driver for Kubernetes FlexVolume Plugin. Parameter String Description importVol Text The name of the Nimble volume to import. snapshot Text The name of the Nimble snapshot to restore the imported volume to after takeover. If not specified, the volume will not be restored. restore Boolean Restores the volume to the last snapshot taken on the volume. takeover Boolean Indicates the current group will takeover ownership of the Nimble volume and volume collection. This should be performed against a downstream replica. reverseRepl Boolean Reverses the replication direction so that writes to the Nimble volume are replicated back to the group where it was replicated from. forceImport Boolean Forces the import of a volume that is not owned by the group and is not part of a volume collection. If the volume is part of a volume collection, use takeover instead. Note HPE Nimble Docker Volume workflows works with a 1-1 mapping between volume and volume collection.","title":"Import parameters"},{"location":"flexvolume_driver/container_provider/index.html#hpe_cloud_volumes_storageclass_parameters","text":"A StorageClass is used to provision or clone an HPE Cloud Volumes-backed persistent volume. It can also be used to import an existing HPE Cloud Volumes volume or clone of a snapshot into the Kubernetes cluster. The parameters are grouped below by those same workflows. A sample StorageClass is provided. Note These are optional parameters.","title":"HPE Cloud Volumes StorageClass parameters"},{"location":"flexvolume_driver/container_provider/index.html#common_parameters_for_provisioning_and_cloning_1","text":"These parameters are mutable betweeen a parent volume and creating a clone from a snapshot. Parameter String Description nameSuffix Text Suffix to append to Cloud Volumes. destroyOnRm Boolean Indicates the backing Cloud volume (including snapshots) should be destroyed when the PVC is deleted. limitIOPS Integer The IOPS limit of the volume. The IOPS limit should be in the range 300 to 50000. perfPolicy Text The name of the performance policy to assign to the volume. Default example performance policies include \"Other, Exchange, Oracle, SharePoint, SQL, Windows File Server\". protectionTemplate Text The name of the protection template to assign to the volume. Default examples of protection templates include \"daily:3, daily:7, daily:14, hourly:6, hourly:12, hourly:24, twicedaily:4, twicedaily:8, twicedaily:14, weekly:2, weekly:4, weekly:8, monthly:3, monthly:6, monthly:12 or none\". volumeType Text Cloud Volume type. Supported types are PF and GPF.","title":"Common parameters for Provisioning and Cloning"},{"location":"flexvolume_driver/container_provider/index.html#provisioning_parameters_1","text":"These parameters are immutable for clones once a volume has been created. Parameter String Description fsOwner userId:groupId The user id and group id that should own the root directory of the filesystem. fsMode Octal digits 1 to 4 octal digits that represent the file mode to be applied to the root directory of the filesystem. encryption Boolean Indicates that the volume should be encrypted.","title":"Provisioning parameters"},{"location":"flexvolume_driver/container_provider/index.html#cloning_parameters_1","text":"Cloning supports two modes of cloning. Either use cloneOf and reference a PVC in the current namespace or use importVolAsClone and reference a Cloud volume name to clone and import to Kubernetes. Parameter String Description cloneOf Text The name of the PV to be cloned. cloneOf and importVolAsClone are mutually exclusive. importVolAsClone Text The name of the Cloud Volume volume to clone and import. importVolAsClone and cloneOf are mutually exclusive. snapshot Text The name of the snapshot to base the clone on. This is optional. If not specified, a new snapshot is created. createSnapshot Boolean Indicates that a new snapshot of the volume should be taken matching the name provided in the snapshot parameter. If the snapshot parameter is not specified, a default name will be created. snapshotPrefix Text A prefix to add to the beginning of the snapshot name. replStore Text Replication store name. Should be used with importVolAsClone parameter to clone a replica volume","title":"Cloning parameters"},{"location":"flexvolume_driver/container_provider/index.html#import_parameters_1","text":"Importing volumes to Kubernetes requires the source Cloud volume to be not attached to any nodes. All previous Access Control Records will be stripped from the volume when put under control of the HPE Volume Driver for Kubernetes FlexVolume Plugin. Parameter String Description importVol Text The name of the Cloud volume to import. forceImport Boolean Forces the import of a volume that is provisioned by another K8s cluster but not attached to any nodes.","title":"Import parameters"},{"location":"flexvolume_driver/container_provider/index.html#diagnostics","text":"This section outlines a few troubleshooting steps for the HPE Volume Driver for Kubernetes Plugin. This product is supported by HPE, please consult with your support organization (Nimble, Cloud Volumes etc) prior attempting any configuration changes.","title":"Diagnostics"},{"location":"flexvolume_driver/container_provider/index.html#troubleshooting_flexvolume_driver","text":"The FlexVolume driver is a binary executed by the kubelet to perform mount/unmount/attach/detach operations as workloads request storage resources. The binary relies on communicating with a socket on the host where the volume plugin responsible for the MUAD operations perform control-plane or data-plane operations against the backend system hosting the actual volumes.","title":"Troubleshooting FlexVolume driver"},{"location":"flexvolume_driver/container_provider/index.html#locations","text":"The driver has a configuration file where certain defaults can be tweaked to accommodate a certain behavior. Under normal circumstances, this file does not need any tweaking. The name and the location of the binary varies based on Kubernetes distribution (the default 'exec' path) and what backend driver is being used. In a typical scenario, using Nimble, this is expected: Binary: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~nimble/nimble Config file: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~nimble/nimble.json","title":"Locations"},{"location":"flexvolume_driver/container_provider/index.html#override_defaults","text":"By default, it contains only the path to the socket file for the volume plugin: { \"dockerVolumePluginSocketPath\": \"/etc/hpe-storage/nimble.sock\" } Valid options for the FlexVolume driver can be inspected by executing the binary on the host with the config argument: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~nimble/nimble config Error processing option 'logFilePath' - key:logFilePath not found Error processing option 'logDebug' - key:logDebug not found Error processing option 'supportsCapabilities' - key:supportsCapabilities not found Error processing option 'stripK8sFromOptions' - key:stripK8sFromOptions not found Error processing option 'createVolumes' - key:createVolumes not found Error processing option 'listOfStorageResourceOptions' - key:listOfStorageResourceOptions not found Error processing option 'factorForConversion' - key:factorForConversion not found Error processing option 'enable1.6' - key:enable1.6 not found Driver=nimble Version=v2.5.1-50fbff2aa14a693a9a18adafb834da33b9e7cc89 Current Config: dockerVolumePluginSocketPath = /etc/hpe-storage/nimble.sock stripK8sFromOptions = true logFilePath = /var/log/dory.log logDebug = false createVolumes = false enable1.6 = false factorForConversion = 1073741824 listOfStorageResourceOptions = [size sizeInGiB] supportsCapabilities = true An example tweak could be to enable debug logging and enable support for Kubernetes 1.6 (which we don't officially support). The config file would then end up like this: { \"dockerVolumePluginSocketPath\": \"/etc/hpe-storage/nimble.sock\", \"logDebug\": true, \"enable1.6\": true } Execute the binary again ( nimble config ) to ensure the parameters and config file gets parsed correctly. Since the config file is read on each FlexVolume operation, no restart of anything is needed. See Advanced for more parameters for the driver.json file.","title":"Override defaults"},{"location":"flexvolume_driver/container_provider/index.html#connectivity","text":"To verify the FlexVolume binary can actually communicate with the backend volume plugin, issue a faux mount request: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~nimble/nimble mount no/op '{\"name\":\"myvol1\"}' If the FlexVolume driver can successfully communicate with the volume plugin socket: {\"status\":\"Failure\",\"message\":\"configured to NOT create volumes\"} In the case of any other output, check if the backend volume plugin is alive with curl : curl --unix-socket /etc/hpe-storage/nimble.sock -d '{}' http://localhost/VolumeDriver.Capabilities It should output: {\"capabilities\":{\"scope\":\"global\"},\"Err\":\"\"}","title":"Connectivity"},{"location":"flexvolume_driver/container_provider/index.html#flexvolume_and_dynamic_provisioner_driver_logs","text":"Log files associated with the HPE Volume Driver for Kubernetes FlexVolume Plugin logs data to the standard output stream. If the logs need to be retained for long term, use a standard logging solution. Some of the logs on the host are persisted which follow standard logrotate policies. FlexVolume driver logs: kubectl logs -f daemonset.apps/hpe-flexvolume-driver -n kube-system The logs are persisted at /var/log/hpe-docker-plugin.log and /var/log/dory.log Dynamic Provisioner logs: kubectl logs -f deployment.apps/hpe-dynamic-provisioner -n kube-system The logs are persisted at /var/log/hpe-dynamic-provisioner.log","title":"FlexVolume and dynamic provisioner driver logs"},{"location":"flexvolume_driver/container_provider/index.html#log_collector","text":"Log collector script hpe-logcollector.sh can be used to collect diagnostic logs using kubectl Download the script as follows: curl -O https://raw.githubusercontent.com/hpe-storage/flexvolume-driver/master/hpe-logcollector.sh chmod 555 hpe-logcollector.sh Usage: ./hpe-logcollector.sh -h Diagnostic Script to collect HPE Storage logs using kubectl Usage: hpe-logcollector.sh [-h|--help][--node-name NODE_NAME][-n|--namespace NAMESPACE][-a|--all] Where -h|--help Print the Usage text --node-name NODE_NAME where NODE_NAME is kubernetes Node Name needed to collect the hpe diagnostic logs of the Node -n|--namespace NAMESPACE where NAMESPACE is namespace of the pod deployment. default is kube-system -a|--all collect diagnostic logs of all the nodes.If nothing is specified logs would be collected from all the nodes","title":"Log Collector"},{"location":"flexvolume_driver/container_provider/index.html#advanced_configuration","text":"This section describes some of the advanced configuration steps available to tweak behavior of the HPE Volume Driver for Kubernetes FlexVolume Plugin.","title":"Advanced Configuration"},{"location":"flexvolume_driver/container_provider/index.html#set_defaults_at_the_compute_node_level","text":"During normal operations, defaults are set in either the ConfigMap or in a StorageClass itself. The picking order is: StorageClass ConfigMap driver.json Please see Diagnostics to locate the driver for your particular environment. Add this object to the configuration file, nimble.json , for example: { \"defaultOptions\": [{\"option1\": \"value1\"}, {\"option2\": \"value2\"}] } Where option1 and option2 are valid backend volume plugin create options. Note It's highly recommended to control defaults with StorageClass API objects or the ConfigMap .","title":"Set defaults at the compute node level"},{"location":"flexvolume_driver/container_provider/index.html#global_options","text":"Each driver supports setting certain \"global\" options in the ConfigMap . Some options are common, some are driver specific.","title":"Global options"},{"location":"flexvolume_driver/container_provider/index.html#common","text":"Parameter String Description volumeDir Text Root directory on the host to mount the volumes. This parameter needs correlation with the podsmountdir path in the volumeMounts stanzas of the deployment. logDebug Boolean Turn on debug logging, set to false by default.","title":"Common"},{"location":"flexvolume_driver/dory/index.html","text":"Introduction \u00b6 The Open Source project Dory was designed in 2017 to transition Docker Volume plugins to be used with Kubernetes. Dory is the shim between the FlexVolume exec calls to the Docker Volume API. The main repository is not currently maintained and the most up-to-date version lives in the HPE Volume Driver for Kubernetes FlexVolume Plugin repository where Dory is packaged as a privileged DaemonSet to support HPE storage products. There may be other forks associated with other Docker Volume plugins out there. Why is the driver called Dory? Dory speaks whale ! Dynamic Provisioning \u00b6 As the FlexVolume Plugin doesn't provide any dynamic provisioning, HPE designed a provisioner to work with Docker Volume plugins as well, Doryd, to have a complete solution for Docker Volume plugins. It's run as a Deployment and monitor PVC requests. FlexVolume Plugin in Kubernetes \u00b6 According to the Kubernetes SIG storage community , the FlexVolume Plugin interface will continue to be supported. Move to CSI \u00b6 HPE encourages using the available CSI drivers for Kubernetes 1.13 and newer where available.","title":"History: Dory and Doryd "},{"location":"flexvolume_driver/dory/index.html#introduction","text":"The Open Source project Dory was designed in 2017 to transition Docker Volume plugins to be used with Kubernetes. Dory is the shim between the FlexVolume exec calls to the Docker Volume API. The main repository is not currently maintained and the most up-to-date version lives in the HPE Volume Driver for Kubernetes FlexVolume Plugin repository where Dory is packaged as a privileged DaemonSet to support HPE storage products. There may be other forks associated with other Docker Volume plugins out there. Why is the driver called Dory? Dory speaks whale !","title":"Introduction"},{"location":"flexvolume_driver/dory/index.html#dynamic_provisioning","text":"As the FlexVolume Plugin doesn't provide any dynamic provisioning, HPE designed a provisioner to work with Docker Volume plugins as well, Doryd, to have a complete solution for Docker Volume plugins. It's run as a Deployment and monitor PVC requests.","title":"Dynamic Provisioning"},{"location":"flexvolume_driver/dory/index.html#flexvolume_plugin_in_kubernetes","text":"According to the Kubernetes SIG storage community , the FlexVolume Plugin interface will continue to be supported.","title":"FlexVolume Plugin in Kubernetes"},{"location":"flexvolume_driver/dory/index.html#move_to_csi","text":"HPE encourages using the available CSI drivers for Kubernetes 1.13 and newer where available.","title":"Move to CSI"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html","text":"Overview \u00b6 The HPE 3PAR and Primera Volume Plug-in for Docker leverages Ansible to deploy the 3PAR/Primera driver for Kubernetes in order to provide scalable and persistent storage for stateful applications. Important Using HPE 3PAR/Primera Storage with Kubernetes 1.15 and newer, please use the HPE CSI Driver for Kubernetes . Source code is available in the hpe-storage/python-hpedockerplugin GitHub repo. Overview Platform requirements HPE 3PAR/Primera Storage Platform Requirements Deploying to Kubernetes Step 1: Install Ansible Ansible: Connecting to remote nodes Ansible: Check your SSH connections Step 2: Clone the Github repository Step 3: Modify the Ansible hosts file Step 4: Create the properties file File Persona Example Configuration Multiple Backend Example Configuration Step 5: Run the Ansible playbook Step 6: Verify the installation Using Sample StorageClass Test and verify volume provisioning Use case specific examples Snapshot a volume Clone a volume Replicate a containerized volume Import (cutover) a volume Using overrides Upgrade Uninstall StorageClass parameters HPE 3PAR/Primera Storage StorageClass parameters Common parameters for Provisioning and Cloning Cloning/Snapshot parameters Import parameters Replication Support Diagnostics Troubleshooting FlexVolume driver Locations Connectivity ETCD HPE 3PAR/Primera FlexVolume and Dynamic Provisioner driver (doryd) logs Refer to the SPOCK page for the latest support matrix for HPE 3PAR and HPE Primera Volume Plug-in for Docker. Platform requirements \u00b6 The HPE 3PAR/Primera FlexVolume driver supports multiple backends that are based on a \"container provider\" architecture. HPE 3PAR/Primera Storage Platform Requirements \u00b6 Ensure that you have reviewed the System Requirements . Driver HPE 3PAR/Primera OS Version Release Notes v3.3.1 3PAR OS: 3.3.1 MU5+ Primera OS: 4.0+ v3.3.1 OpenShift Container Platform 3.9, 3.10 and 3.11. Kubernetes 1.10 and above. Redhat/CentOS 7.5+ Note: Refer to SPOCK page for the latest support matrix for HPE 3PAR and HPE Primera Volume Plug-in for Docker. Deploying to Kubernetes \u00b6 The recommended way to deploy and manage the HPE 3PAR and Primera Volume Plug-in for Kubernetes is to use Ansible. Use the following steps to configure Ansible to perform the installation. Step 1: Install Ansible \u00b6 Ensure that Ansible (v2.5 to v2.8) is installed. For more information, see Ansible Installation Guide . NOTE: Ansible only needs to be installed on the machine that will be performing the deployment. Ansible does not need to be installed on your Kubernetes cluster. $ pip install ansible $ ansible --version ansible 2.7.12 Ansible: Connecting to remote nodes \u00b6 Ansible communicates with remote machines over the SSH protocol. By default, Ansible uses native OpenSSH and connects to remote machines using your current user name, just as SSH does. Ansible: Check your SSH connections \u00b6 Confirm that you can connect using SSH to all the nodes in your Kubernetes cluster using the same username. If necessary, add your public SSH key to the authorized_keys file on those systems. Step 2: Clone the Github repository \u00b6 $ cd ~ $ git clone https://github.com/hpe-storage/python-hpedockerplugin Step 3: Modify the Ansible hosts file \u00b6 Modify the hosts file to define the Kubernetes/OpenShift Master and Worker nodes. Also define where the HPE etcd cluster will be deployed, this can be done within the cluster or on external servers. Shell $ vi python-hpedockerplugin/ansible_3par_docker_plugin/hosts Yaml [masters] 192.168.1.51 [workers] 192.168.1.52 192.168.1.53 [etcd] 192.168.1.51 192.168.1.52 192.168.1.53 Step 4: Create the properties file \u00b6 Create the properties/plugin_configuration_properties.yml based on your HPE 3PAR/Primera Storage array configuration. $ vi python-hpedockerplugin/ansible_3par_docker_plugin/properties/plugin_configuration_properties.yml NOTE: Some of the properties are mandatory and must be specified in the properties file while others are optional. INVENTORY: DEFAULT: #Mandatory Parameters-------------------------------------------------------------------------------- # Specify the port to be used by HPE 3PAR plugin etcd cluster host_etcd_port_number: 23790 # Plugin Driver - iSCSI hpedockerplugin_driver: hpedockerplugin.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver hpe3par_ip: <3par_array_IP> hpe3par_username: <3par_user> hpe3par_password: <3par_password> #Specify the 3PAR port - 8080 default hpe3par_port: 8080 hpe3par_cpg: <cpg_name> # Plugin version - Required only in DEFAULT backend volume_plugin: hpestorage/legacyvolumeplugin:3.3.1 # Dory installer version - Required for Openshift/Kubernetes setup # Supported versions are dory_installer_v31, dory_installer_v32 dory_installer_version: dory_installer_v32 #Optional Parameters-------------------------------------------------------------------------------- logging: DEBUG hpe3par_snapcpg: FC_r6 #hpe3par_iscsi_chap_enabled: True use_multipath: True #enforce_multipath: False #vlan_tag: True Available Properties Parameters Property Mandatory Default Value Description hpedockerplugin_driver Yes No default value ISCSI/FC driver (hpedockerplugin.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver/hpedockerplugin.hpe.hpe_3par_fc.HPE3PARFCDriver) hpe3par_ip Yes No default value IP address of 3PAR array hpe3par_username Yes No default value 3PAR username hpe3par_password Yes No default value 3PAR password hpe3par_port Yes 8080 3PAR HTTP_PORT port hpe3par_cpg Yes No default value Primary user CPG volume_plugin Yes No default value Name of the docker volume image (only required with DEFAULT backend) encryptor_key No No default value Encryption key string for 3PAR password logging No INFO Log level hpe3par_debug No No default value 3PAR log level suppress_requests_ssl_warning No True Suppress request SSL warnings hpe3par_snapcpg No hpe3par_cpg Snapshot CPG hpe3par_iscsi_chap_enabled No False ISCSI chap toggle hpe3par_iscsi_ips No No default value Comma separated iscsi port IPs (only required if driver is ISCSI based) use_multipath No False Mutltipath toggle enforce_multipath No False Forcefully enforce multipath ssh_hosts_key_file No /root/.ssh/known_hosts Path to hosts key file quorum_witness_ip No No default value Quorum witness IP mount_prefix No No default value Alternate mount path prefix hpe3par_iscsi_ips No No default value Comma separated iscsi IPs. If not provided, all iscsi IPs will be read from the array and populated in hpe.conf vlan_tag No False Populates the iscsi_ips which are vlan tagged, only applicable if hpe3par_iscsi_ips is not specified replication_device No No default value Replication backend properties dory_installer_version No dory_installer_v32 Required for Openshift/Kubernetes setup. Dory installer version, supported versions are dory_installer_v31, dory_installer_v32 hpe3par_server_ip_pool Yes No default value This parameter is specific to fileshare. It can be specified as a mix of range of IPs and individual IPs delimited by comma. Each range or individual IP must be followed by the corresponding subnet mask delimited by semi-colon E.g.: IP-Range:Subnet-Mask,Individual-IP:SubnetMask hpe3par_default_fpg_size No No default value This parameter is specific to fileshare. Default fpg size, It must be in the range 1TiB to 64TiB. If not specified here, it defaults to 16TiB Hint Refer to Replication Support for details on enabling Replication support. File Persona Example Configuration \u00b6 #Mandatory Parameters for Filepersona--------------------------------------------------------------- DEFAULT_FILE: # Specify the port to be used by HPE 3PAR plugin etcd cluster host_etcd_port_number: 23790 # Plugin Driver - File driver hpedockerplugin_driver: hpedockerplugin.hpe.hpe_3par_file.HPE3PARFileDriver hpe3par_ip: 192.168.2.50 hpe3par_username: demo_user hpe3par_password: demo_pass hpe3par_cpg: demo_cpg hpe3par_port: 8080 hpe3par_server_ip_pool: 192.168.98.3-192.168.98.10:255.255.192.0 #Optional Parameters for Filepersona---------------------------------------------------------------- hpe3par_default_fpg_size: 16 Multiple Backend Example Configuration \u00b6 INVENTORY: DEFAULT: #Mandatory Parameters------------------------------------------------------------------------------- # Specify the port to be used by HPE 3PAR plugin etcd cluster host_etcd_port_number: 23790 # Plugin Driver - iSCSI hpedockerplugin_driver: hpedockerplugin.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver hpe3par_ip: 192.168.1.50 hpe3par_username: 3paradm hpe3par_password: 3pardata hpe3par_port: 8080 hpe3par_cpg: FC_r6 # Plugin version - Required only in DEFAULT backend volume_plugin: hpestorage/legacyvolumeplugin:3.3.1 # Dory installer version - Required for Openshift/Kubernetes setup # Supported versions are dory_installer_v31, dory_installer_v32 dory_installer_version: dory_installer_v32 #Optional Parameters-------------------------------------------------------------------------------- #ssh_hosts_key_file: '/root/.ssh/known_hosts' logging: DEBUG #hpe3par_debug: True #suppress_requests_ssl_warning: True #hpe3par_snapcpg: FC_r6 #hpe3par_iscsi_chap_enabled: True #use_multipath: False #enforce_multipath: False #vlan_tag: True #Additional Backend (Optional)---------------------------------------------------------------------- 3PAR1: #Mandatory Parameters------------------------------------------------------------------------------- # Specify the port to be used by HPE 3PAR plugin etcd cluster host_etcd_port_number: 23790 # Plugin Driver - Fibre Channel hpedockerplugin_driver: hpedockerplugin.hpe.hpe_3par_fc.HPE3PARFCDriver hpe3par_ip: 192.168.2.50 hpe3par_username: 3paradm hpe3par_password: 3pardata hpe3par_port: 8080 hpe3par_cpg: FC_r6 #Optional Parameters-------------------------------------------------------------------------------- #ssh_hosts_key_file: '/root/.ssh/known_hosts' logging: DEBUG #hpe3par_debug: True #suppress_requests_ssl_warning: True hpe3par_snapcpg: FC_r6 #use_multipath: False #enforce_multipath: False Step 5: Run the Ansible playbook \u00b6 $ cd python-hpedockerplugin/ansible_3par_docker_plugin/ $ ansible-playbook -i hosts install_hpe_3par_volume_driver.yml Step 6: Verify the installation \u00b6 Once playbook has completed successfully, the PLAY RECAP should look like below Installer should not show any failures and PLAY RECAP should look like below PLAY RECAP *********************************************************************** <Master1-IP> : ok=85 changed=33 unreachable=0 failed=0 <Master2-IP> : ok=76 changed=29 unreachable=0 failed=0 <Master3-IP> : ok=76 changed=29 unreachable=0 failed=0 <Worker1-IP> : ok=70 changed=27 unreachable=0 failed=0 <Worker2-IP> : ok=70 changed=27 unreachable=0 failed=0 localhost : ok=9 changed=3 unreachable=0 failed=0 Verify plugin installation on all nodes. $ docker ps | grep plugin; ssh <Master2-IP> \"docker ps | grep plugin\";ssh <Master3-IP> \"docker ps | grep plugin\";ssh <Worker1-IP> \"docker ps | grep plugin\";ssh <Worker2-IP> \"docker ps | grep plugin\" 51b9d4b1d591 hpestorage/legacyvolumeplugin:3.3.1 \"/bin/sh -c ./plugin\u2026\" 12 minutes ago Up 12 minutes plugin_container a43f6d8f5080 hpestorage/legacyvolumeplugin:3.3.1 \"/bin/sh -c ./plugin\u2026\" 12 minutes ago Up 12 minutes plugin_container a88af9f46a0d hpestorage/legacyvolumeplugin:3.3.1 \"/bin/sh -c ./plugin\u2026\" 12 minutes ago Up 12 minutes plugin_container 5b20f16ab3af hpestorage/legacyvolumeplugin:3.3.1 \"/bin/sh -c ./plugin\u2026\" 12 minutes ago Up 12 minutes plugin_container b0813a22cbd8 hpestorage/legacyvolumeplugin:3.3.1 \"/bin/sh -c ./plugin\u2026\" 12 minutes ago Up 12 minutes plugin_container Verify the HPE FlexVolume driver Pod is running. kubectl get pods -n kube-system | grep doryd NAME READY STATUS RESTARTS AGE kube-storage-controller-doryd-7dd487b446-xr6q2 1/1 Running 0 45s Using \u00b6 Get started using the FlexVolume driver by setting up StorageClass , PVC API objects. See Using for examples. These instructions are provided as an example on how to use the HPE 3PAR/Primera Volume Plug-in with a HPE 3PAR/Primera Storage Array. The below YAML declarations are meant to be created with kubectl create . Either copy the content to a file on the host where kubectl is being executed, or copy & paste into the terminal, like this: kubectl create -f- < paste the YAML > ^D (CTRL + D) Tip Some of the examples supported by the HPE 3PAR/Primera FlexVolume driver are available for HPE 3PAR/Primera Storage in the GitHub repo. To get started, create a StorageClass API object referencing the hpe-secret and defining additional (optional) StorageClass parameters: Sample StorageClass \u00b6 Sample storage classes can be found for HPE 3PAR/Primera Storage . Test and verify volume provisioning \u00b6 Create a StorageClass with volume parameters as required. Change the CPG per your requirements. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: sc-gold provisioner: hpe.com/hpe parameters: provisioning: 'full' cpg: 'SSD_r6' fsOwner: '1001:1001' Create a PersistentVolumeClaim . This makes sure a volume is created and provisioned on your behalf: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: sc-gold-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 25Gi storageClassName: sc-gold Check that a new PersistentVolume is created based on your claim: $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE sc-gold-pvc-13336da3-7ca3-11e9-826c-00505692581f 25Gi RWO Delete Bound default/pvc-gold sc-gold 3s The above output means that the FlexVolume driver successfully provisioned a new volume and bound to the requesting PVC to a new PV . The volume is not attached to any node yet. It will only be attached to a node if a workload is scheduled to a specific node. Now let us create a Pod that refers to the above volume. When the Pod is created, the volume will be attached, formatted and mounted to the specified container: kind: Pod apiVersion: v1 metadata: name: pod-nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - name: export mountPath: \"/usr/share/nginx/html\" volumes: - name: export persistentVolumeClaim: claimName: sc-gold-pvc Check if the pod is running successfully: $ kubectl get pod pod-nginx NAME READY STATUS RESTARTS AGE pod-nginx 1/1 Running 0 2m29s Use case specific examples \u00b6 This StorageClass examples help guide combinations of options when provisioning volumes. Snapshot a volume \u00b6 This StorageClass will create a snapshot of a \"production\" volume. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: sc-gold-snap-mongo provisioner: hpe.com/hpe parameters: virtualCopyOf: \"sc-mongo-10dc1195-779b-11e9-b787-0050569bb07c\" Clone a volume \u00b6 This StorageClass will create clones of a \"production\" volume. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: sc-gold-clone provisioner: hpe.com/hpe parameters: cloneOf: \"sc-gold-2a82c9e5-6213-11e9-8d53-0050569bb07c\" Replicate a containerized volume \u00b6 This StorageClass will add a standard backend volume to a 3PAR Replication Group. If the replicationGroup specified does not exist, the plugin will create one. See Replication Support for more details on configuring replication. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: sc-mongodb-replicated provisioner: hpe.com/hpe parameters: provisioning: 'full' replicationGroup: 'mongodb-app1' Import (cutover) a volume \u00b6 This StorageClass will import an existing 3PAR/Primera volume to Kubernetes. The source volume needs to be offline for the import to succeed. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: import-clone-legacy-prod provisioner: hpe.com/hpe parameters: importVol: \"production-db-vol\" Using overrides \u00b6 The HPE Dynamic Provisioner for Kubernetes (doryd) understands a set of annotation keys a user can set on a PVC . If the corresponding keys exists in the list of the allowOverrides key in the StorageClass , the end-user can tweak certain aspects of the provisioning workflow. This opens up for very advanced data services. StorageClass object: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: sc-gold provisioner: hpe.com/hpe parameters: provisioning: 'full' cpg: 'SSD_r6' fsOwner: '1001:1001' allowOverrides: provisioning,compression,cpg,fsOwner PersistentVolumeClaim object: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc annotations: hpe.com/provisioning: \"thin\" hpe.com/cpg: \"FC_r6\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 25Gi storageClassName: sc-gold This will create a PV thinly provisioned using the FC-r6 cpg. Upgrade \u00b6 In order to upgrade the driver, simply modify the ansible_3par_docker_plugin/properties/plugin_configuration_properties_sample.yml used for the initial deployment and modify hpestorage/legacyvolumeplugin to the latest image from docker hub. For example: volume_plugin: hpestorage/legacyvolumeplugin:3.3 Change to: volume_plugin: hpestorage/legacyvolumeplugin:3.3.1 Re-run the installer. $ ansible-playbook -i hosts install_hpe_3par_volume_driver.yml Uninstall \u00b6 Run the following to uninstall the FlexVolume driver from the cluster. $ cd ~ $ cd python-hpedockerplugin/ansible_3par_docker_plugin $ ansible-playbook -i hosts uninstall/uninstall_hpe_3par_volume_driver.yml StorageClass parameters \u00b6 This section highlights all the available StorageClass parameters that are supported. HPE 3PAR/Primera Storage StorageClass parameters \u00b6 A StorageClass is used to provision or clone an HPE 3PAR\\Primera Storage-backed persistent volume. It can also be used to import an existing HPE 3PAR/Primera Storage volume or clone of a snapshot into the Kubernetes cluster. The parameters are grouped below by those same workflows. A sample StorageClass is provided. Note These are optional parameters. Common parameters for Provisioning and Cloning \u00b6 These parameters are mutable betweeen a parent volume and creating a clone from a snapshot. Parameter Type Options Example size Integer - size: \"10\" provisioning thin, full, dedupe provisioning: \"thin\" flash-cache Text true, false flash-cache: \"true\" compression boolean true, false compression: \"true\" MountConflictDelay Integer - MountConflictDelay: \"30\" qos-name Text vvset name qos-name: \" \" replicationGroup Text 3PAR RCG name replicationGroup: \"Test-RCG\" fsOwner userId:groupId The user id and group id that should own the root directory of the filesystem. fsMode Octal digits 1 to 4 octal digits that represent the file mode to be applied to the root directory of the filesystem. Cloning/Snapshot parameters \u00b6 Either use cloneOf and reference a PVC in the current namespace or use virtualCopyOf and reference a 3PAR/Primera volume name to snapshot/clone and import into Kubernetes. Parameter Type Options Example cloneOf Text volume name cloneOf: \"<volume_name>\" virtualCopyOf Text volume name virtualCopyOf: \"<volume_name>\" expirationHours Integer option of virtualCopyOf expirationHours: \"10\" retentionHours Integer option of virtualCopyOf retentionHours: \"10\" Import parameters \u00b6 Importing volumes to Kubernetes requires the source 3PAR/Primera volume to be offline. Parameter Type Description Example importVol Text volume name importVol: \"<volume_name>\" Replication Support \u00b6 The HPE 3PAR/Primer FlexVolume driver supports array based synchronous and asynchronous replication. In order to enable replication within the FlexVolume driver, the arrays need to be properly zoned, visible to the Kubernetes cluster, and replication configured. For Peer Persistence, a quorum witness will need to be configured. Once the replication is enabled at the array level, the FlexVolume driver will need to be configured. Important Replication support can be enabled during initial deployment through the plugin configuration file. In order to enable replication support post deployment, modify the plugin_configuration_properties.yml used for deployment, add the replication parameter section below, and re-run the Ansible installer. Edit the plugin_configuration_properties.yml file and edit the Optional Replication Section. INVENTORY: DEFAULT: #Mandatory Parameters------------------------------------------------------------------------------- # Specify the port to be used by HPE 3PAR plugin etcd cluster host_etcd_port_number: 23790 # Plugin Driver - iSCSI hpedockerplugin_driver: hpedockerplugin.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver hpe3par_ip: <local_3par_ip> hpe3par_username: <local_3par_user> hpe3par_password: <local_3par_password> hpe3par_port: 8080 hpe3par_cpg: FC_r6 # Plugin version - Required only in DEFAULT backend volume_plugin: hpestorage/legacyvolumeplugin:3.3.1 # Dory installer version - Required for Openshift/Kubernetes setup dory_installer_version: dory_installer_v32 #Optional Parameters-------------------------------------------------------------------------------- logging: DEBUG hpe3par_snapcpg: FC_r6 use_multipath: False enforce_multipath: False #Optional Replication Parameters-------------------------------------------------------------------- replication_device: backend_id: remote_3PAR #Quorum Witness required for Peer Persistence only #quorum_witness_ip: <quorum_witness_ip> replication_mode: synchronous cpg_map: \"local_CPG:remote_CPG\" snap_cpg_map: \"local_copy_CPG:remote_copy_CPG\" hpe3par_ip: <remote_3par_ip> hpe3par_username: <remote_3par_user> hpe3par_password: <remote_3par_password> hpe3par_port: 8080 #vlan_tag: False Once the properties file is configured, you can proceed with the standard installation steps . Diagnostics \u00b6 This section outlines a few troubleshooting steps for the HPE 3PAR/Primera FlexVolume driver. This product is supported by HPE, please consult with your support organization prior attempting any configuration changes. Troubleshooting FlexVolume driver \u00b6 The FlexVolume driver is a binary executed by the kubelet to perform mount/unmount/attach/detach operations as workloads request storage resources. The binary relies on communicating with a socket on the host where the volume plugin responsible for the MUAD operations perform control-plane or data-plane operations against the backend system hosting the actual volumes. Locations \u00b6 The driver has a configuration file where certain defaults can be tweaked to accommodate a certain behavior. Under normal circumstances, this file does not need any tweaking. The name and the location of the binary varies based on Kubernetes distribution (the default 'exec' path) and what backend driver is being used. In a typical scenario, using 3PAR/Primera, this is expected: Binary: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~hpe/hpe Config file: /etc/hpedockerplugin/hpe.conf Connectivity \u00b6 To verify the FlexVolume binary can actually communicate with the backend volume plugin, issue a faux mount request: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~hpe/hpe mount no/op '{\"name\":\"myvol1\"}' If the FlexVolume driver can successfully communicate with the volume plugin socket: {\"status\":\"Failure\",\"message\":\"configured to NOT create volumes\"} In the case of any other output, check if the backend volume plugin is alive: $ docker volume create -d hpe -o help=backends It should output: ================================= NAME STATUS ================================= DEFAULT OK ETCD \u00b6 To verify the etcd members on nodes. $ /usr/bin/etcdctl --endpoints http://<Master1-IP>:23790 member list It should output: b70ca254f54dd23: name=<Worker2-IP> peerURLs=http://<Worker2-IP>:23800 clientURLs=http://<Worker2-IP>:23790 isLeader=true 236bf7d5cc7a32d4: name=<Worker1-IP> peerURLs=http://<Worker1-IP>:23800 clientURLs=http://<Worker1-IP>:23790 isLeader=false 445e80419ae8729b: name=<Master1-IP> peerURLs=http://<Master1-IP>:23800 clientURLs=http://<Master1-IP>:23790 isLeader=false e340a5833e93861e: name=<Master3-IP> peerURLs=http://<Master3-IP>:23800 clientURLs=http://<Master3-IP>:23790 isLeader=false f5b5599d719d376e: name=<Master2-IP> peerURLs=http://<Master2-IP>:23800 clientURLs=http://<Master2-IP>:23790 isLeader=false HPE 3PAR/Primera FlexVolume and Dynamic Provisioner driver (doryd) logs \u00b6 Log files associated with the HPE 3PAR/Primera FlexVolume driver logs data to the standard output stream. If the logs need to be retained for long term, use a standard logging solution. Some of the logs on the host are persisted which follow standard logrotate policies. HPE 3PAR/Primera FlexVolume logs: (per node) $ docker logs -f plugin_container Dynamic Provisioner logs: kubectl logs -f kube-storage-controller-doryd -n kube-system The logs are persisted at /var/log/hpe-dynamic-provisioner.log","title":"Ansible installer for 3PAR/Primera"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#overview","text":"The HPE 3PAR and Primera Volume Plug-in for Docker leverages Ansible to deploy the 3PAR/Primera driver for Kubernetes in order to provide scalable and persistent storage for stateful applications. Important Using HPE 3PAR/Primera Storage with Kubernetes 1.15 and newer, please use the HPE CSI Driver for Kubernetes . Source code is available in the hpe-storage/python-hpedockerplugin GitHub repo. Overview Platform requirements HPE 3PAR/Primera Storage Platform Requirements Deploying to Kubernetes Step 1: Install Ansible Ansible: Connecting to remote nodes Ansible: Check your SSH connections Step 2: Clone the Github repository Step 3: Modify the Ansible hosts file Step 4: Create the properties file File Persona Example Configuration Multiple Backend Example Configuration Step 5: Run the Ansible playbook Step 6: Verify the installation Using Sample StorageClass Test and verify volume provisioning Use case specific examples Snapshot a volume Clone a volume Replicate a containerized volume Import (cutover) a volume Using overrides Upgrade Uninstall StorageClass parameters HPE 3PAR/Primera Storage StorageClass parameters Common parameters for Provisioning and Cloning Cloning/Snapshot parameters Import parameters Replication Support Diagnostics Troubleshooting FlexVolume driver Locations Connectivity ETCD HPE 3PAR/Primera FlexVolume and Dynamic Provisioner driver (doryd) logs Refer to the SPOCK page for the latest support matrix for HPE 3PAR and HPE Primera Volume Plug-in for Docker.","title":"Overview"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#platform_requirements","text":"The HPE 3PAR/Primera FlexVolume driver supports multiple backends that are based on a \"container provider\" architecture.","title":"Platform requirements"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#hpe_3parprimera_storage_platform_requirements","text":"Ensure that you have reviewed the System Requirements . Driver HPE 3PAR/Primera OS Version Release Notes v3.3.1 3PAR OS: 3.3.1 MU5+ Primera OS: 4.0+ v3.3.1 OpenShift Container Platform 3.9, 3.10 and 3.11. Kubernetes 1.10 and above. Redhat/CentOS 7.5+ Note: Refer to SPOCK page for the latest support matrix for HPE 3PAR and HPE Primera Volume Plug-in for Docker.","title":"HPE 3PAR/Primera Storage Platform Requirements"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#deploying_to_kubernetes","text":"The recommended way to deploy and manage the HPE 3PAR and Primera Volume Plug-in for Kubernetes is to use Ansible. Use the following steps to configure Ansible to perform the installation.","title":"Deploying to Kubernetes"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#step_1_install_ansible","text":"Ensure that Ansible (v2.5 to v2.8) is installed. For more information, see Ansible Installation Guide . NOTE: Ansible only needs to be installed on the machine that will be performing the deployment. Ansible does not need to be installed on your Kubernetes cluster. $ pip install ansible $ ansible --version ansible 2.7.12","title":"Step 1: Install Ansible"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#ansible_connecting_to_remote_nodes","text":"Ansible communicates with remote machines over the SSH protocol. By default, Ansible uses native OpenSSH and connects to remote machines using your current user name, just as SSH does.","title":"Ansible: Connecting to remote nodes"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#ansible_check_your_ssh_connections","text":"Confirm that you can connect using SSH to all the nodes in your Kubernetes cluster using the same username. If necessary, add your public SSH key to the authorized_keys file on those systems.","title":"Ansible: Check your SSH connections"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#step_2_clone_the_github_repository","text":"$ cd ~ $ git clone https://github.com/hpe-storage/python-hpedockerplugin","title":"Step 2: Clone the Github repository"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#step_3_modify_the_ansible_hosts_file","text":"Modify the hosts file to define the Kubernetes/OpenShift Master and Worker nodes. Also define where the HPE etcd cluster will be deployed, this can be done within the cluster or on external servers. Shell $ vi python-hpedockerplugin/ansible_3par_docker_plugin/hosts Yaml [masters] 192.168.1.51 [workers] 192.168.1.52 192.168.1.53 [etcd] 192.168.1.51 192.168.1.52 192.168.1.53","title":"Step 3: Modify the Ansible hosts file"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#step_4_create_the_properties_file","text":"Create the properties/plugin_configuration_properties.yml based on your HPE 3PAR/Primera Storage array configuration. $ vi python-hpedockerplugin/ansible_3par_docker_plugin/properties/plugin_configuration_properties.yml NOTE: Some of the properties are mandatory and must be specified in the properties file while others are optional. INVENTORY: DEFAULT: #Mandatory Parameters-------------------------------------------------------------------------------- # Specify the port to be used by HPE 3PAR plugin etcd cluster host_etcd_port_number: 23790 # Plugin Driver - iSCSI hpedockerplugin_driver: hpedockerplugin.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver hpe3par_ip: <3par_array_IP> hpe3par_username: <3par_user> hpe3par_password: <3par_password> #Specify the 3PAR port - 8080 default hpe3par_port: 8080 hpe3par_cpg: <cpg_name> # Plugin version - Required only in DEFAULT backend volume_plugin: hpestorage/legacyvolumeplugin:3.3.1 # Dory installer version - Required for Openshift/Kubernetes setup # Supported versions are dory_installer_v31, dory_installer_v32 dory_installer_version: dory_installer_v32 #Optional Parameters-------------------------------------------------------------------------------- logging: DEBUG hpe3par_snapcpg: FC_r6 #hpe3par_iscsi_chap_enabled: True use_multipath: True #enforce_multipath: False #vlan_tag: True Available Properties Parameters Property Mandatory Default Value Description hpedockerplugin_driver Yes No default value ISCSI/FC driver (hpedockerplugin.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver/hpedockerplugin.hpe.hpe_3par_fc.HPE3PARFCDriver) hpe3par_ip Yes No default value IP address of 3PAR array hpe3par_username Yes No default value 3PAR username hpe3par_password Yes No default value 3PAR password hpe3par_port Yes 8080 3PAR HTTP_PORT port hpe3par_cpg Yes No default value Primary user CPG volume_plugin Yes No default value Name of the docker volume image (only required with DEFAULT backend) encryptor_key No No default value Encryption key string for 3PAR password logging No INFO Log level hpe3par_debug No No default value 3PAR log level suppress_requests_ssl_warning No True Suppress request SSL warnings hpe3par_snapcpg No hpe3par_cpg Snapshot CPG hpe3par_iscsi_chap_enabled No False ISCSI chap toggle hpe3par_iscsi_ips No No default value Comma separated iscsi port IPs (only required if driver is ISCSI based) use_multipath No False Mutltipath toggle enforce_multipath No False Forcefully enforce multipath ssh_hosts_key_file No /root/.ssh/known_hosts Path to hosts key file quorum_witness_ip No No default value Quorum witness IP mount_prefix No No default value Alternate mount path prefix hpe3par_iscsi_ips No No default value Comma separated iscsi IPs. If not provided, all iscsi IPs will be read from the array and populated in hpe.conf vlan_tag No False Populates the iscsi_ips which are vlan tagged, only applicable if hpe3par_iscsi_ips is not specified replication_device No No default value Replication backend properties dory_installer_version No dory_installer_v32 Required for Openshift/Kubernetes setup. Dory installer version, supported versions are dory_installer_v31, dory_installer_v32 hpe3par_server_ip_pool Yes No default value This parameter is specific to fileshare. It can be specified as a mix of range of IPs and individual IPs delimited by comma. Each range or individual IP must be followed by the corresponding subnet mask delimited by semi-colon E.g.: IP-Range:Subnet-Mask,Individual-IP:SubnetMask hpe3par_default_fpg_size No No default value This parameter is specific to fileshare. Default fpg size, It must be in the range 1TiB to 64TiB. If not specified here, it defaults to 16TiB Hint Refer to Replication Support for details on enabling Replication support.","title":"Step 4: Create the properties file"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#file_persona_example_configuration","text":"#Mandatory Parameters for Filepersona--------------------------------------------------------------- DEFAULT_FILE: # Specify the port to be used by HPE 3PAR plugin etcd cluster host_etcd_port_number: 23790 # Plugin Driver - File driver hpedockerplugin_driver: hpedockerplugin.hpe.hpe_3par_file.HPE3PARFileDriver hpe3par_ip: 192.168.2.50 hpe3par_username: demo_user hpe3par_password: demo_pass hpe3par_cpg: demo_cpg hpe3par_port: 8080 hpe3par_server_ip_pool: 192.168.98.3-192.168.98.10:255.255.192.0 #Optional Parameters for Filepersona---------------------------------------------------------------- hpe3par_default_fpg_size: 16","title":"File Persona Example Configuration"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#multiple_backend_example_configuration","text":"INVENTORY: DEFAULT: #Mandatory Parameters------------------------------------------------------------------------------- # Specify the port to be used by HPE 3PAR plugin etcd cluster host_etcd_port_number: 23790 # Plugin Driver - iSCSI hpedockerplugin_driver: hpedockerplugin.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver hpe3par_ip: 192.168.1.50 hpe3par_username: 3paradm hpe3par_password: 3pardata hpe3par_port: 8080 hpe3par_cpg: FC_r6 # Plugin version - Required only in DEFAULT backend volume_plugin: hpestorage/legacyvolumeplugin:3.3.1 # Dory installer version - Required for Openshift/Kubernetes setup # Supported versions are dory_installer_v31, dory_installer_v32 dory_installer_version: dory_installer_v32 #Optional Parameters-------------------------------------------------------------------------------- #ssh_hosts_key_file: '/root/.ssh/known_hosts' logging: DEBUG #hpe3par_debug: True #suppress_requests_ssl_warning: True #hpe3par_snapcpg: FC_r6 #hpe3par_iscsi_chap_enabled: True #use_multipath: False #enforce_multipath: False #vlan_tag: True #Additional Backend (Optional)---------------------------------------------------------------------- 3PAR1: #Mandatory Parameters------------------------------------------------------------------------------- # Specify the port to be used by HPE 3PAR plugin etcd cluster host_etcd_port_number: 23790 # Plugin Driver - Fibre Channel hpedockerplugin_driver: hpedockerplugin.hpe.hpe_3par_fc.HPE3PARFCDriver hpe3par_ip: 192.168.2.50 hpe3par_username: 3paradm hpe3par_password: 3pardata hpe3par_port: 8080 hpe3par_cpg: FC_r6 #Optional Parameters-------------------------------------------------------------------------------- #ssh_hosts_key_file: '/root/.ssh/known_hosts' logging: DEBUG #hpe3par_debug: True #suppress_requests_ssl_warning: True hpe3par_snapcpg: FC_r6 #use_multipath: False #enforce_multipath: False","title":"Multiple Backend Example Configuration"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#step_5_run_the_ansible_playbook","text":"$ cd python-hpedockerplugin/ansible_3par_docker_plugin/ $ ansible-playbook -i hosts install_hpe_3par_volume_driver.yml","title":"Step 5: Run the Ansible playbook"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#step_6_verify_the_installation","text":"Once playbook has completed successfully, the PLAY RECAP should look like below Installer should not show any failures and PLAY RECAP should look like below PLAY RECAP *********************************************************************** <Master1-IP> : ok=85 changed=33 unreachable=0 failed=0 <Master2-IP> : ok=76 changed=29 unreachable=0 failed=0 <Master3-IP> : ok=76 changed=29 unreachable=0 failed=0 <Worker1-IP> : ok=70 changed=27 unreachable=0 failed=0 <Worker2-IP> : ok=70 changed=27 unreachable=0 failed=0 localhost : ok=9 changed=3 unreachable=0 failed=0 Verify plugin installation on all nodes. $ docker ps | grep plugin; ssh <Master2-IP> \"docker ps | grep plugin\";ssh <Master3-IP> \"docker ps | grep plugin\";ssh <Worker1-IP> \"docker ps | grep plugin\";ssh <Worker2-IP> \"docker ps | grep plugin\" 51b9d4b1d591 hpestorage/legacyvolumeplugin:3.3.1 \"/bin/sh -c ./plugin\u2026\" 12 minutes ago Up 12 minutes plugin_container a43f6d8f5080 hpestorage/legacyvolumeplugin:3.3.1 \"/bin/sh -c ./plugin\u2026\" 12 minutes ago Up 12 minutes plugin_container a88af9f46a0d hpestorage/legacyvolumeplugin:3.3.1 \"/bin/sh -c ./plugin\u2026\" 12 minutes ago Up 12 minutes plugin_container 5b20f16ab3af hpestorage/legacyvolumeplugin:3.3.1 \"/bin/sh -c ./plugin\u2026\" 12 minutes ago Up 12 minutes plugin_container b0813a22cbd8 hpestorage/legacyvolumeplugin:3.3.1 \"/bin/sh -c ./plugin\u2026\" 12 minutes ago Up 12 minutes plugin_container Verify the HPE FlexVolume driver Pod is running. kubectl get pods -n kube-system | grep doryd NAME READY STATUS RESTARTS AGE kube-storage-controller-doryd-7dd487b446-xr6q2 1/1 Running 0 45s","title":"Step 6: Verify the installation"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#using","text":"Get started using the FlexVolume driver by setting up StorageClass , PVC API objects. See Using for examples. These instructions are provided as an example on how to use the HPE 3PAR/Primera Volume Plug-in with a HPE 3PAR/Primera Storage Array. The below YAML declarations are meant to be created with kubectl create . Either copy the content to a file on the host where kubectl is being executed, or copy & paste into the terminal, like this: kubectl create -f- < paste the YAML > ^D (CTRL + D) Tip Some of the examples supported by the HPE 3PAR/Primera FlexVolume driver are available for HPE 3PAR/Primera Storage in the GitHub repo. To get started, create a StorageClass API object referencing the hpe-secret and defining additional (optional) StorageClass parameters:","title":"Using"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#sample_storageclass","text":"Sample storage classes can be found for HPE 3PAR/Primera Storage .","title":"Sample StorageClass"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#test_and_verify_volume_provisioning","text":"Create a StorageClass with volume parameters as required. Change the CPG per your requirements. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: sc-gold provisioner: hpe.com/hpe parameters: provisioning: 'full' cpg: 'SSD_r6' fsOwner: '1001:1001' Create a PersistentVolumeClaim . This makes sure a volume is created and provisioned on your behalf: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: sc-gold-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 25Gi storageClassName: sc-gold Check that a new PersistentVolume is created based on your claim: $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE sc-gold-pvc-13336da3-7ca3-11e9-826c-00505692581f 25Gi RWO Delete Bound default/pvc-gold sc-gold 3s The above output means that the FlexVolume driver successfully provisioned a new volume and bound to the requesting PVC to a new PV . The volume is not attached to any node yet. It will only be attached to a node if a workload is scheduled to a specific node. Now let us create a Pod that refers to the above volume. When the Pod is created, the volume will be attached, formatted and mounted to the specified container: kind: Pod apiVersion: v1 metadata: name: pod-nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - name: export mountPath: \"/usr/share/nginx/html\" volumes: - name: export persistentVolumeClaim: claimName: sc-gold-pvc Check if the pod is running successfully: $ kubectl get pod pod-nginx NAME READY STATUS RESTARTS AGE pod-nginx 1/1 Running 0 2m29s","title":"Test and verify volume provisioning"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#use_case_specific_examples","text":"This StorageClass examples help guide combinations of options when provisioning volumes.","title":"Use case specific examples"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#snapshot_a_volume","text":"This StorageClass will create a snapshot of a \"production\" volume. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: sc-gold-snap-mongo provisioner: hpe.com/hpe parameters: virtualCopyOf: \"sc-mongo-10dc1195-779b-11e9-b787-0050569bb07c\"","title":"Snapshot a volume"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#clone_a_volume","text":"This StorageClass will create clones of a \"production\" volume. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: sc-gold-clone provisioner: hpe.com/hpe parameters: cloneOf: \"sc-gold-2a82c9e5-6213-11e9-8d53-0050569bb07c\"","title":"Clone a volume"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#replicate_a_containerized_volume","text":"This StorageClass will add a standard backend volume to a 3PAR Replication Group. If the replicationGroup specified does not exist, the plugin will create one. See Replication Support for more details on configuring replication. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: sc-mongodb-replicated provisioner: hpe.com/hpe parameters: provisioning: 'full' replicationGroup: 'mongodb-app1'","title":"Replicate a containerized volume"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#import_cutover_a_volume","text":"This StorageClass will import an existing 3PAR/Primera volume to Kubernetes. The source volume needs to be offline for the import to succeed. kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: import-clone-legacy-prod provisioner: hpe.com/hpe parameters: importVol: \"production-db-vol\"","title":"Import (cutover) a volume"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#using_overrides","text":"The HPE Dynamic Provisioner for Kubernetes (doryd) understands a set of annotation keys a user can set on a PVC . If the corresponding keys exists in the list of the allowOverrides key in the StorageClass , the end-user can tweak certain aspects of the provisioning workflow. This opens up for very advanced data services. StorageClass object: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: sc-gold provisioner: hpe.com/hpe parameters: provisioning: 'full' cpg: 'SSD_r6' fsOwner: '1001:1001' allowOverrides: provisioning,compression,cpg,fsOwner PersistentVolumeClaim object: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc annotations: hpe.com/provisioning: \"thin\" hpe.com/cpg: \"FC_r6\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 25Gi storageClassName: sc-gold This will create a PV thinly provisioned using the FC-r6 cpg.","title":"Using overrides"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#upgrade","text":"In order to upgrade the driver, simply modify the ansible_3par_docker_plugin/properties/plugin_configuration_properties_sample.yml used for the initial deployment and modify hpestorage/legacyvolumeplugin to the latest image from docker hub. For example: volume_plugin: hpestorage/legacyvolumeplugin:3.3 Change to: volume_plugin: hpestorage/legacyvolumeplugin:3.3.1 Re-run the installer. $ ansible-playbook -i hosts install_hpe_3par_volume_driver.yml","title":"Upgrade"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#uninstall","text":"Run the following to uninstall the FlexVolume driver from the cluster. $ cd ~ $ cd python-hpedockerplugin/ansible_3par_docker_plugin $ ansible-playbook -i hosts uninstall/uninstall_hpe_3par_volume_driver.yml","title":"Uninstall"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#storageclass_parameters","text":"This section highlights all the available StorageClass parameters that are supported.","title":"StorageClass parameters"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#hpe_3parprimera_storage_storageclass_parameters","text":"A StorageClass is used to provision or clone an HPE 3PAR\\Primera Storage-backed persistent volume. It can also be used to import an existing HPE 3PAR/Primera Storage volume or clone of a snapshot into the Kubernetes cluster. The parameters are grouped below by those same workflows. A sample StorageClass is provided. Note These are optional parameters.","title":"HPE 3PAR/Primera Storage StorageClass parameters"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#common_parameters_for_provisioning_and_cloning","text":"These parameters are mutable betweeen a parent volume and creating a clone from a snapshot. Parameter Type Options Example size Integer - size: \"10\" provisioning thin, full, dedupe provisioning: \"thin\" flash-cache Text true, false flash-cache: \"true\" compression boolean true, false compression: \"true\" MountConflictDelay Integer - MountConflictDelay: \"30\" qos-name Text vvset name qos-name: \" \" replicationGroup Text 3PAR RCG name replicationGroup: \"Test-RCG\" fsOwner userId:groupId The user id and group id that should own the root directory of the filesystem. fsMode Octal digits 1 to 4 octal digits that represent the file mode to be applied to the root directory of the filesystem.","title":"Common parameters for Provisioning and Cloning"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#cloningsnapshot_parameters","text":"Either use cloneOf and reference a PVC in the current namespace or use virtualCopyOf and reference a 3PAR/Primera volume name to snapshot/clone and import into Kubernetes. Parameter Type Options Example cloneOf Text volume name cloneOf: \"<volume_name>\" virtualCopyOf Text volume name virtualCopyOf: \"<volume_name>\" expirationHours Integer option of virtualCopyOf expirationHours: \"10\" retentionHours Integer option of virtualCopyOf retentionHours: \"10\"","title":"Cloning/Snapshot parameters"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#import_parameters","text":"Importing volumes to Kubernetes requires the source 3PAR/Primera volume to be offline. Parameter Type Description Example importVol Text volume name importVol: \"<volume_name>\"","title":"Import parameters"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#replication_support","text":"The HPE 3PAR/Primer FlexVolume driver supports array based synchronous and asynchronous replication. In order to enable replication within the FlexVolume driver, the arrays need to be properly zoned, visible to the Kubernetes cluster, and replication configured. For Peer Persistence, a quorum witness will need to be configured. Once the replication is enabled at the array level, the FlexVolume driver will need to be configured. Important Replication support can be enabled during initial deployment through the plugin configuration file. In order to enable replication support post deployment, modify the plugin_configuration_properties.yml used for deployment, add the replication parameter section below, and re-run the Ansible installer. Edit the plugin_configuration_properties.yml file and edit the Optional Replication Section. INVENTORY: DEFAULT: #Mandatory Parameters------------------------------------------------------------------------------- # Specify the port to be used by HPE 3PAR plugin etcd cluster host_etcd_port_number: 23790 # Plugin Driver - iSCSI hpedockerplugin_driver: hpedockerplugin.hpe.hpe_3par_iscsi.HPE3PARISCSIDriver hpe3par_ip: <local_3par_ip> hpe3par_username: <local_3par_user> hpe3par_password: <local_3par_password> hpe3par_port: 8080 hpe3par_cpg: FC_r6 # Plugin version - Required only in DEFAULT backend volume_plugin: hpestorage/legacyvolumeplugin:3.3.1 # Dory installer version - Required for Openshift/Kubernetes setup dory_installer_version: dory_installer_v32 #Optional Parameters-------------------------------------------------------------------------------- logging: DEBUG hpe3par_snapcpg: FC_r6 use_multipath: False enforce_multipath: False #Optional Replication Parameters-------------------------------------------------------------------- replication_device: backend_id: remote_3PAR #Quorum Witness required for Peer Persistence only #quorum_witness_ip: <quorum_witness_ip> replication_mode: synchronous cpg_map: \"local_CPG:remote_CPG\" snap_cpg_map: \"local_copy_CPG:remote_copy_CPG\" hpe3par_ip: <remote_3par_ip> hpe3par_username: <remote_3par_user> hpe3par_password: <remote_3par_password> hpe3par_port: 8080 #vlan_tag: False Once the properties file is configured, you can proceed with the standard installation steps .","title":"Replication Support"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#diagnostics","text":"This section outlines a few troubleshooting steps for the HPE 3PAR/Primera FlexVolume driver. This product is supported by HPE, please consult with your support organization prior attempting any configuration changes.","title":"Diagnostics"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#troubleshooting_flexvolume_driver","text":"The FlexVolume driver is a binary executed by the kubelet to perform mount/unmount/attach/detach operations as workloads request storage resources. The binary relies on communicating with a socket on the host where the volume plugin responsible for the MUAD operations perform control-plane or data-plane operations against the backend system hosting the actual volumes.","title":"Troubleshooting FlexVolume driver"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#locations","text":"The driver has a configuration file where certain defaults can be tweaked to accommodate a certain behavior. Under normal circumstances, this file does not need any tweaking. The name and the location of the binary varies based on Kubernetes distribution (the default 'exec' path) and what backend driver is being used. In a typical scenario, using 3PAR/Primera, this is expected: Binary: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~hpe/hpe Config file: /etc/hpedockerplugin/hpe.conf","title":"Locations"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#connectivity","text":"To verify the FlexVolume binary can actually communicate with the backend volume plugin, issue a faux mount request: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/hpe.com~hpe/hpe mount no/op '{\"name\":\"myvol1\"}' If the FlexVolume driver can successfully communicate with the volume plugin socket: {\"status\":\"Failure\",\"message\":\"configured to NOT create volumes\"} In the case of any other output, check if the backend volume plugin is alive: $ docker volume create -d hpe -o help=backends It should output: ================================= NAME STATUS ================================= DEFAULT OK","title":"Connectivity"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#etcd","text":"To verify the etcd members on nodes. $ /usr/bin/etcdctl --endpoints http://<Master1-IP>:23790 member list It should output: b70ca254f54dd23: name=<Worker2-IP> peerURLs=http://<Worker2-IP>:23800 clientURLs=http://<Worker2-IP>:23790 isLeader=true 236bf7d5cc7a32d4: name=<Worker1-IP> peerURLs=http://<Worker1-IP>:23800 clientURLs=http://<Worker1-IP>:23790 isLeader=false 445e80419ae8729b: name=<Master1-IP> peerURLs=http://<Master1-IP>:23800 clientURLs=http://<Master1-IP>:23790 isLeader=false e340a5833e93861e: name=<Master3-IP> peerURLs=http://<Master3-IP>:23800 clientURLs=http://<Master3-IP>:23790 isLeader=false f5b5599d719d376e: name=<Master2-IP> peerURLs=http://<Master2-IP>:23800 clientURLs=http://<Master2-IP>:23790 isLeader=false","title":"ETCD"},{"location":"flexvolume_driver/hpe_3par_primera_installer/index.html#hpe_3parprimera_flexvolume_and_dynamic_provisioner_driver_doryd_logs","text":"Log files associated with the HPE 3PAR/Primera FlexVolume driver logs data to the standard output stream. If the logs need to be retained for long term, use a standard logging solution. Some of the logs on the host are persisted which follow standard logrotate policies. HPE 3PAR/Primera FlexVolume logs: (per node) $ docker logs -f plugin_container Dynamic Provisioner logs: kubectl logs -f kube-storage-controller-doryd -n kube-system The logs are persisted at /var/log/hpe-dynamic-provisioner.log","title":"HPE 3PAR/Primera FlexVolume and Dynamic Provisioner driver (doryd) logs"},{"location":"learn/containers101/index.html","text":"Overview \u00b6 Welcome to the \"101\" section of SCOD. The goal of this section is to create a learning resource for individuals who want to learn about emerging topics in a cloud native world where containers are the focal point. The content is slightly biased towards storage. Mission Statement \u00b6 We aim to provide a learning resource collection that is generic enough to comprehend nuances in the different solutions and paradigms. Hewlett Packard Enterprise Products are highly likely referenced in some examples and resources. We can therefore not claim vendor neutrality nor a Switzerland opinion. External resources are the primary learning assets used to frame certain topics. Let's start the learning journey. Overview Mission Statement Cloud Native Computing Key Attributes Learning Resources Practical Exercises Cloud Native Tooling Key Attributes Learning Resources Practical Exercises Cloud Native Storage Key Attributes Learning Resources Practical Exercises Containers Intro Key Attributes Learning Resources Practical Exercises Container Tooling Key Attributes Learning Resources Practical Exercises Container Storage Key Attributes Learning Resources Practical Exercises DevOps Key Attributes Learning Resources Practical Exercises DevOps Tooling Key Attributes Learning Resources Practical Exercises DevOps Storage Key Attributes Learning Resources Practical Exercises Summary Cloud Native Computing \u00b6 The term \"cloud native\" stems from a software development model where resources are consumed as services. Compute, network and storage consumed through APIs, CLIs and web administration interfaces. Consumption is often modeled around paying only for what is being used. The applications deployed into Cloud Native Computing environments are often divided into small chunks that are operated independently, referred to as microservices. On the uprising is a broader adoption of a concept called serverless where your application runs only when called and is billed in milliseconds. Many public cloud vendors provide many already cloud native applications as services on their respective clouds. An example would be to consume a SQL database as a service rather than deploying and managing it by yourself. Key Attributes \u00b6 These are some of the key elements of Cloud Native Computing. Resources are provisioned through complete self-service. API first strategies to promote interoperability and collaboration. Separation of concerns in microservice architectures. High degree of automation of resource provisioning and deprovisioning. Modern languges and frameworks. Infrastructure-as-a-Service (IAAS) Learning Resources \u00b6 Curated list of learning resources for Cloud Native Computing. Webinar: What is Cloud Native and Why Does It Exist? A webinar by WeaveWorks endorsed by the CNCF (Cloud Native Computing Foundation). Market Overview: CNCF Cloud Native Interactive Landscape Many applications and vendors claim to be cloud native. This map is compiled by the CNCF. Reference: 12factor.net A design pattern for microservice architectures. Blog: The rise of cloud native programming languages A blog post that outlines the journey from bare-metal beyond serverless. Blog: 10 Key Attributes of Cloud-native Applications A blog post from thenewstack.io Practical Exercises \u00b6 How to get hands-on experience of Cloud Native Computing. Sign-up on any of the public clouds. Provision an instance and get remote access to the host OS of the instance. Deploy an \"as-a-service\" of an application technology you're familiar with. Connect a client from your instance to your provisioned service. Deploy either web server or Layer-4 load-balancer to give external access to your client application. Cloud Native Tooling \u00b6 Tools to interact with infrastructure and applications come in many shapes and forms. A common pattern is to learn by visually creating and deleting resources to understand an end-state. Once a pattern has been established, either APIs, 3rd party or a custom CLI is used to manage the life-cycle of the deployment in a declarative manner by manipulating RESTful APIs. Also known as Infrastructure-as-Code. Key Attributes \u00b6 These are some of the key elements of Cloud Native Computing Tooling. State stored in a Source Code Control System (SCCS). Changes made to state are peer reviewed and automatically tested in non-production environments before being merged and deployed. Industry standard IT automation tools are often used to implement changes. Ansible, Puppet, Salt and Chef are example tools. Public clouds often provide CLIs to manage resources. These are great to prepare, inspect and test deployments with. Configuration and deployment files are often written in a human and machine readable format, such as JSON, YAML or TOML. Learning Resources \u00b6 Curated list of learning resources for Cloud Native Computing Tooling. Blog: Imperative vs Declarative A blog that highlights the fundamental differences between the two. Reference: json.org Definitive guide on JavaScript Object Notation (JSON) data structures. Reference: YAML Syntax Simple guide for YAML Ain't Markup Language (YAML). Reference: RESTful API Tutorial Learn the design principles of REpresentational State Transfer (REST). Screencast: Super-basic Introduction to Ansible The simplest of Ansible tutorials starting with nothing. Practical Exercises \u00b6 How to get hands-on experience of Cloud Native Computing Tooling. Sign-up on AWS. Install the AWS CLI and Ansible in a Linux instance. Configure BOTO . Use the Ansible EC2 module to create and delete an instance. Cloud Native Storage \u00b6 Storage for cloud computing come in many shapes and forms. Compute instances boot off block devices provided by the IaaS through the hypervisor. More devices may be attached for application data to keep host OS and application separate. Most clouds allow these devices to be snapshotted, cloned and reattached to other instances. These block devices are normally offered with different backend media, such as flash or spinning disks. Depending on the use cases and budgets parameters may be tuned to be just right. For unstructured workloads, API driven object storage is the dominant technology due to the dramatic difference in cost and simplicity vs cloud provided block storage. An object is uploaded through an API endpoint with HTTP and automatically distributed (highly configurable) to provide high durability. The URL of the object will remain static for the duration of its lifetime. The main prohibitor for object storage adoption is that existing applications relying on POSIX filesystems need to be rewritten. Key Attributes \u00b6 These are some of the key elements of Cloud Native Storage. Provisioned and attached via APIs through IaaS if using block storage. Data and metadata is managed with RESTful APIs if using object. No backend to manage. Consumers use standard URLs to retrieve data. Highly durable with object storage. Durability equal to a local RAID device for block storage. Some cloud providers offer Filesystem-as-a-Service, normally standard NFS or CIFS. Backup and recovery of application data still needs to managed like traditional storage for block. Multi-region multi-copy persistence for object storage. Learning Resources \u00b6 Curated list of learning resources for Cloud Native Storage. Wikipedia: Object storage Digestible overview of Object Storage. Tutorial: Host images on Amazon S3 A five minute step-by-step guide how to host images on Amazon S3. Reference: Amazon EBS features An overview of typical attributes for cloud provided block storage. Reference: HPE Cloud Storage Cost Calculator Calculate the real costs of cloud storage based on highly dynamic data management environments. Practical Exercises \u00b6 How to get hands-on experience of Cloud Native Storage. Setup a S3 compatible object storage server or use a public cloud. Scality has a open source S3 server for non-production use. Configure s3cmd to upload and retrieve files from a bucket. Analyze costs of 100TB of data for one year on Amazon S3 vs Azure Manage Disks. Containers Intro \u00b6 A container is operating system-level virtualization and has been around for quite some time. By definition, the container share the kernel of the host and relies on certain abstractions to be useful. Docker the company made the technology approachable and incredibly more convenient than any predecessor. In the simplest of forms, a container image contains a virtual filesystem that contains only the dependencies the application needs. An example would be to include the Python interpreter if you wrote a program in Python. Containerized applications are primarily designed to run headless. In most cases these applications need to communicate with the outside world or allow inbound traffic depending on the application. Docker containers should be treated as transient, each instance starts in a known state and any data stored inside the virtual filesystem should be treated as ephemeral. This makes it extremely easy and convenient to upgrade and rollback a container. If data is required to persist between upgrades and rollbacks of the container, it needs to be stored outside of the container mapped from the host operating system. The wide adoption of containers are because they're lightweight, reproducible and run everywhere. Iterations of software delivery lifecycles may be cut down to seconds from weeks with the right processes and tools. Container images are layered per change made when the container is built. Each layer has a cryptographic hash and the layer itself can be shared between multiple containers readonly. When a new container is started from an image, the container runtime creates a COW (copy-on-write) filesystem where the particular container data is stored. This is in turn very effective as you only need one copy of a layer on the host. For example, if a bunch of applications are based off a Ubuntu base image, the base image only needs to be stored once on the host. Key Attributes \u00b6 These are some of the key elements of Containers. Runs on modern architectures and operating systems. Not necessarily as a single source image. Headless services (webservers, databases etc) in microservice architectures. Often orchestrated on compute clusters like Kubernetes, Apache Mesos Marathon or Docker Swarm. Software vendors often provide official and well tested container images for their applications. Learning Resources \u00b6 Curated list of learning resources for Containers. Interactive: Play with Docker Great interactive tutorials where you learn how to build, ship and run containers. Also has a follow-on interactive training on Kubernetes. Tutorial: Docker for beginners Comprehensive introduction to get started with Docker all the way to running it on a PaaS. Cartoon: The Illustrated Children's Guide to Kubernetes Illustrative and easy to grasp story of what Kubernetes is. Bonus cartoon: A Kubernetes story: Phippy goes to the zoo A high production quality cartoon explaining Kubernetes API objects. Blog: How to choose the right container orchestration and how to deploy it A brief overview of container orchestrators. Standards: opencontainers.org Components of a container system is standards based. The Open Container Initiative is the standards body. Blog/reference: Demystifying container runtimes Discusses different container runtime engines. Practical Exercises \u00b6 How to get hands-on experience of Containers. Install Docker Desktop or just Docker if using Linux. Click through the Get Started tutorial. Advanced: Run any of the images built in the tutorial on a public cloud service. Container Tooling \u00b6 Most of the tooling around containers is centered around what particular container orchestrator or development environment is being utilized. Usage of the tools differ greatly depending on the role of the user. As an operator the toolkit includes both IaaS and managing the platform to perform upgrades, user management and peripheral services such as storage and ingress load balancers. While many popular platforms today are based on Kubernetes, the tooling has nuances. Upstream Kubernetes uses kubectl , Red Hat OpenShift uses the OpenShift CLI, oc . With other platforms such as Rancher, nearly all management can be done through a web UI. Key Attributes \u00b6 These are some of the key elements of Container Tooling. Most tools are simple, yet powerful and follow UNIX principles of doing one thing and doing it well. The docker and kubectl CLIs are the two most dominant for low level management. Workload management usually relies on external tools for simplicity, such as docker-compose , kompose and helm . Some platforms have ancillary tools to marry the IaaS with the cluster orchestrator. Such an example is rke for Rancher and gkectl for GKE On-Prem. The public clouds have builtin container orchestrator and container management into their native CLIs, such as aws and gcloud . Client side tools normally rely on environment variables and user environment configuration files that store credentials, API endpoint locations and other security aspects. Learning Resources \u00b6 Curated list of learning resources for Container Tooling. Reference: Use the Docker command line Docker CLI reference. Reference: The kubectl Cheat Sheet The kubectl cheat sheet. Utility: kustomize.io Kubernetes native configuration managment. Tutorial: The Ultimate Guide to Podman, Skopeo and Buildah An alternative container toolchain to Docker using Podman, Buildah and Skopeo. Practical Exercises \u00b6 How to get hands-on experience of Container Tooling. Install Docker Desktop or just Docker if using Linux. Build a container image of an application you understand (docker build). Run the container image locally (docker run). Ship it to Docker Hub (docker push). Create an Amazon EKS cluster or equivalent. Retrieve the kubeconfig file. Run kubectl get nodes on your local machine. Start a Pod using the container image built in previous exercise. Container Storage \u00b6 Due to the ephemeral nature of a container, storage is predominantly served from the host the container is running on and is dependent on which container runtime is being used where data is stored. In the case of Docker, the overlay filesystems are under /var/lib/docker . If a certain path inside the container need to persist between upgrades, restarts on a different host or any other operation that will lose the locality of the data, the mount point needs to be replaced with a \"bind\" mount from the host. There are also container runtime technologies that are designed to persist the entire container, effectively treating the container more like a long-lived Virtual Machine. Examples are Canonical LXD, WeaveWorks Footloose and HPE BlueData. This is particularly important for applications that rely on its projected node info to remain static throughout its entire lifecycle. We can then begin to categorize containers into three main categories based on their lifecycle vs persistence needs. Stateless Containers No persistence needed across restarts/upgrades/rollbacks Stateful Containers Require certain mountpoints to persist across restarts/upgrades/rollbacks Persistent Containers Require static node identity information across restarts/upgrades/rollbacks Some modern Software-defined Storage solutions are offered to run alongside applications in a distributed fashion. Effectively enforcing multi-way replicas for reliability and eat into CPU and memory resources of the IaaS bill. This also introduces the dilemma of effectively locking the data into the container orchestrator and its compute nodes. Although it's convenient for developers to become self-entitled storage administrators. To stay in control of the data and remain mobile, storing data outside of the container orchestrator is preferable. Many container orchestrators provide plugins for external storage, some are built-in and some are supplied and supported by the storage vendor. Public clouds provide storage drivers for their IaaS storage services directly to the container orchestrator. This is widely popular pattern we're also seeing in BYO IaaS solutions such as VMware vSphere. Key Attributes \u00b6 These are some of the key elements of Container Storage. Ephemeral storage needs to be fast and expandable as environments scale with more diverse applications. Data for stateful containers is ideally stored outside of the container orchestrator, either the IaaS or external highly-available storage. Persistent containers require niche storage solution tightly coupled with the container runtime and the container orchestrator or scheduler. Most storage solutions provide an \"access mode\" often referred to as ReadWriteOnce (RWO) which only allow one Pod (in the Kubernetes case or containers from the same host access the volume. To allow multiple Pods and containers from multiple hosts, a distributed filesystem or an NFS server (widely adopted) is required to provide ReadWriteMany (RWX) access. Learning Resources \u00b6 Curated list of learning resources for Container Storage. Talk: Kubernetes Storage Lingo 101 A talk that lays out the nomenclature for storage in Kubernetes in an understandable way. Reference: Docker: Volumes Fundamental reference on how to make mount points persist for containers. Reference: Kubernetes: Volumes Using volumes in Kubernetes Pods. Podcast: Kubernetes Storage with Saad Ali Essential listen understand the difference between high-availability and automatic recovery. Practical Exercises \u00b6 How to get hands-on experience of Container Storage. Use Docker Desktop. Replace a mount point in an interactive container with a mount point from the host Deploy a Amazon EKS or equivalent cluster. Create a Persistent Volume Claim. Run kubectl get pv -o yaml and match the Persistent Volume against the IaaS block volumes. DevOps \u00b6 There are many interpretations of what DevOps \"is\". A bird's eye view is that there are people, processes and tools that come together to drive business outcomes through value streams. There are many core principles that could ultimately drive the outcome and no cookie cutter solution for any given organization. Breaking down problems into small pieces and creating safe systems to work in and eliminate toil are some of those principles. Agile development and lean manufacturing are both predecessors and role models for driving DevOps principles. Key Attributes \u00b6 These are some of the key elements of DevOps. Well-defined processes, safe and proportionally sized work units for each step in a value stream. Autonomy through tooling backed by well-defined processes. Operations, development and stakeholders unified behind common goals. Continuous improvement, robust feedback loops and problem \"swarming\" of value streams. All work in the value stream must be visible and measurable. From CEO and down buy-in to prevent failure of DevOps implementations. DevOps is essential to be successful when investing in a \"digital transformation\". Learning Resources \u00b6 Curated list of learning resources for DevOps. Opinions: Define DevOps: What is DevOps Industry voices defining what DevOps is and means. Blog: Toil: Finally a Name For a Problem We've All Felt Broad definition of toil. Hardbacks: DevOps Books Author Gene Kim has written novels and \"cookbooks\" of DevOps. Reference: The DevOps Institute Focuses on the human side of successfully implementing DevOps. Talk: Bank on Open Source for DevOps Success - Capital One How a disruptive company differentiate with DevOps at its core. Practical Exercises \u00b6 How to get hands-on experience of DevOps. Getting practical with DevOps requires an organization and a value stream. Listen to the The Phonenix Project for a glimpse into how to implement DevOps. DevOps Tooling \u00b6 The tools in DevOps are centered around the processes and value streams that support the business. Said tools also promote visibility, openness and collaboration. Inherently following security patterns, audit trails and safety. No one person should be able to misuse one tool to cause major disturbance in a value stream without quick remediation plans. Many times CI/CD (Continuous Integration, Continuous Delivery and/or Deployment) is considered synonymous with DevOps. That is both right and wrong. If the value stream inherently contains software, yes. Key Attributes \u00b6 These are some of the key elements of DevOps Tooling. Just the right amount of privileges for a particular task. Issue/project tracking, kanban, source code control, CI/CD, logging and reporting are essential. Visibility and traceability is a key element, no work should be hidden. By person or machine. Learning Resources \u00b6 Curated list of learning resources for DevOps Tooling. Reference: Periodic Table of DevOps Tools The most comprehensive chart of current and popular DevOps tools. Blog: Continuous integration vs. continuous delivery vs. continuous deployment Distinguish the components of CI/CD and what each facet encompass. Reference: Kanban Understanding Kanban helps understanding flow of work to adjust tools to work for humans, not against them. Practical Exercises \u00b6 How to get hands-on experience of DevOps Tooling. Study some of the tools available to perform automated tasks on complex systems. Jenkins Rundeck Ansible Tower Morpheus Data Delphix The common denominator across these platforms is the observability and the ability to limit scope of controls through Role-based Access Control (RBAC). Ensuring the tasks are well-defined, automated, scoped and safe to operate. DevOps Storage \u00b6 There aren't any particular storage paradigms (file/block/object) that are associated with DevOps. It's the implementation of the application and how it consumes storage that we vaguely may associate with DevOps. It's more of the practice that the right security controls are in place and whomever needs storage resource are fully self serviced. Human or machine. Key Attributes \u00b6 These are some of the key elements of DevOps Storage. API driven through RBAC. Ensuring automation may put in place for the endpoint or person that needs access to the resource. Rich data management. If a value stream only needs a low performing read-only view of a certain dataset, resources supporting the value stream should only have read-only access with performance constrains. Agile and mobile. At will, data should be made available for a certain application or resource for its purpose. Whether it's in the public cloud, on-prem or as-a-service through safe and secure automation. Learning Resources \u00b6 Curated list of learning resources for DevOps Storage. Blog: Is Your Storage Too Slow for DevOps? Characterization of DevOps Storage attributes. Practical Exercises \u00b6 How to get hands-on experience of DevOps Storage. Familiarize yourself with a storage system's RESTful API and automation capabilities. Deploy an Ansible Tower trial. Write an Ansible playbook that creates a storage resource on said system. Create a job in Ansible Tower with the playbook and make it available to a restricted user. Summary \u00b6 If you have any suggestions or comments, head over to GitHub and file a PR or leave an issue.","title":"Cloud-Native, Containers & DevOps"},{"location":"learn/containers101/index.html#overview","text":"Welcome to the \"101\" section of SCOD. The goal of this section is to create a learning resource for individuals who want to learn about emerging topics in a cloud native world where containers are the focal point. The content is slightly biased towards storage.","title":"Overview"},{"location":"learn/containers101/index.html#mission_statement","text":"We aim to provide a learning resource collection that is generic enough to comprehend nuances in the different solutions and paradigms. Hewlett Packard Enterprise Products are highly likely referenced in some examples and resources. We can therefore not claim vendor neutrality nor a Switzerland opinion. External resources are the primary learning assets used to frame certain topics. Let's start the learning journey. Overview Mission Statement Cloud Native Computing Key Attributes Learning Resources Practical Exercises Cloud Native Tooling Key Attributes Learning Resources Practical Exercises Cloud Native Storage Key Attributes Learning Resources Practical Exercises Containers Intro Key Attributes Learning Resources Practical Exercises Container Tooling Key Attributes Learning Resources Practical Exercises Container Storage Key Attributes Learning Resources Practical Exercises DevOps Key Attributes Learning Resources Practical Exercises DevOps Tooling Key Attributes Learning Resources Practical Exercises DevOps Storage Key Attributes Learning Resources Practical Exercises Summary","title":"Mission Statement"},{"location":"learn/containers101/index.html#cloud_native_computing","text":"The term \"cloud native\" stems from a software development model where resources are consumed as services. Compute, network and storage consumed through APIs, CLIs and web administration interfaces. Consumption is often modeled around paying only for what is being used. The applications deployed into Cloud Native Computing environments are often divided into small chunks that are operated independently, referred to as microservices. On the uprising is a broader adoption of a concept called serverless where your application runs only when called and is billed in milliseconds. Many public cloud vendors provide many already cloud native applications as services on their respective clouds. An example would be to consume a SQL database as a service rather than deploying and managing it by yourself.","title":"Cloud Native Computing"},{"location":"learn/containers101/index.html#key_attributes","text":"These are some of the key elements of Cloud Native Computing. Resources are provisioned through complete self-service. API first strategies to promote interoperability and collaboration. Separation of concerns in microservice architectures. High degree of automation of resource provisioning and deprovisioning. Modern languges and frameworks. Infrastructure-as-a-Service (IAAS)","title":"Key Attributes"},{"location":"learn/containers101/index.html#learning_resources","text":"Curated list of learning resources for Cloud Native Computing. Webinar: What is Cloud Native and Why Does It Exist? A webinar by WeaveWorks endorsed by the CNCF (Cloud Native Computing Foundation). Market Overview: CNCF Cloud Native Interactive Landscape Many applications and vendors claim to be cloud native. This map is compiled by the CNCF. Reference: 12factor.net A design pattern for microservice architectures. Blog: The rise of cloud native programming languages A blog post that outlines the journey from bare-metal beyond serverless. Blog: 10 Key Attributes of Cloud-native Applications A blog post from thenewstack.io","title":"Learning Resources"},{"location":"learn/containers101/index.html#practical_exercises","text":"How to get hands-on experience of Cloud Native Computing. Sign-up on any of the public clouds. Provision an instance and get remote access to the host OS of the instance. Deploy an \"as-a-service\" of an application technology you're familiar with. Connect a client from your instance to your provisioned service. Deploy either web server or Layer-4 load-balancer to give external access to your client application.","title":"Practical Exercises"},{"location":"learn/containers101/index.html#cloud_native_tooling","text":"Tools to interact with infrastructure and applications come in many shapes and forms. A common pattern is to learn by visually creating and deleting resources to understand an end-state. Once a pattern has been established, either APIs, 3rd party or a custom CLI is used to manage the life-cycle of the deployment in a declarative manner by manipulating RESTful APIs. Also known as Infrastructure-as-Code.","title":"Cloud Native Tooling"},{"location":"learn/containers101/index.html#key_attributes_1","text":"These are some of the key elements of Cloud Native Computing Tooling. State stored in a Source Code Control System (SCCS). Changes made to state are peer reviewed and automatically tested in non-production environments before being merged and deployed. Industry standard IT automation tools are often used to implement changes. Ansible, Puppet, Salt and Chef are example tools. Public clouds often provide CLIs to manage resources. These are great to prepare, inspect and test deployments with. Configuration and deployment files are often written in a human and machine readable format, such as JSON, YAML or TOML.","title":"Key Attributes"},{"location":"learn/containers101/index.html#learning_resources_1","text":"Curated list of learning resources for Cloud Native Computing Tooling. Blog: Imperative vs Declarative A blog that highlights the fundamental differences between the two. Reference: json.org Definitive guide on JavaScript Object Notation (JSON) data structures. Reference: YAML Syntax Simple guide for YAML Ain't Markup Language (YAML). Reference: RESTful API Tutorial Learn the design principles of REpresentational State Transfer (REST). Screencast: Super-basic Introduction to Ansible The simplest of Ansible tutorials starting with nothing.","title":"Learning Resources"},{"location":"learn/containers101/index.html#practical_exercises_1","text":"How to get hands-on experience of Cloud Native Computing Tooling. Sign-up on AWS. Install the AWS CLI and Ansible in a Linux instance. Configure BOTO . Use the Ansible EC2 module to create and delete an instance.","title":"Practical Exercises"},{"location":"learn/containers101/index.html#cloud_native_storage","text":"Storage for cloud computing come in many shapes and forms. Compute instances boot off block devices provided by the IaaS through the hypervisor. More devices may be attached for application data to keep host OS and application separate. Most clouds allow these devices to be snapshotted, cloned and reattached to other instances. These block devices are normally offered with different backend media, such as flash or spinning disks. Depending on the use cases and budgets parameters may be tuned to be just right. For unstructured workloads, API driven object storage is the dominant technology due to the dramatic difference in cost and simplicity vs cloud provided block storage. An object is uploaded through an API endpoint with HTTP and automatically distributed (highly configurable) to provide high durability. The URL of the object will remain static for the duration of its lifetime. The main prohibitor for object storage adoption is that existing applications relying on POSIX filesystems need to be rewritten.","title":"Cloud Native Storage"},{"location":"learn/containers101/index.html#key_attributes_2","text":"These are some of the key elements of Cloud Native Storage. Provisioned and attached via APIs through IaaS if using block storage. Data and metadata is managed with RESTful APIs if using object. No backend to manage. Consumers use standard URLs to retrieve data. Highly durable with object storage. Durability equal to a local RAID device for block storage. Some cloud providers offer Filesystem-as-a-Service, normally standard NFS or CIFS. Backup and recovery of application data still needs to managed like traditional storage for block. Multi-region multi-copy persistence for object storage.","title":"Key Attributes"},{"location":"learn/containers101/index.html#learning_resources_2","text":"Curated list of learning resources for Cloud Native Storage. Wikipedia: Object storage Digestible overview of Object Storage. Tutorial: Host images on Amazon S3 A five minute step-by-step guide how to host images on Amazon S3. Reference: Amazon EBS features An overview of typical attributes for cloud provided block storage. Reference: HPE Cloud Storage Cost Calculator Calculate the real costs of cloud storage based on highly dynamic data management environments.","title":"Learning Resources"},{"location":"learn/containers101/index.html#practical_exercises_2","text":"How to get hands-on experience of Cloud Native Storage. Setup a S3 compatible object storage server or use a public cloud. Scality has a open source S3 server for non-production use. Configure s3cmd to upload and retrieve files from a bucket. Analyze costs of 100TB of data for one year on Amazon S3 vs Azure Manage Disks.","title":"Practical Exercises"},{"location":"learn/containers101/index.html#containers_intro","text":"A container is operating system-level virtualization and has been around for quite some time. By definition, the container share the kernel of the host and relies on certain abstractions to be useful. Docker the company made the technology approachable and incredibly more convenient than any predecessor. In the simplest of forms, a container image contains a virtual filesystem that contains only the dependencies the application needs. An example would be to include the Python interpreter if you wrote a program in Python. Containerized applications are primarily designed to run headless. In most cases these applications need to communicate with the outside world or allow inbound traffic depending on the application. Docker containers should be treated as transient, each instance starts in a known state and any data stored inside the virtual filesystem should be treated as ephemeral. This makes it extremely easy and convenient to upgrade and rollback a container. If data is required to persist between upgrades and rollbacks of the container, it needs to be stored outside of the container mapped from the host operating system. The wide adoption of containers are because they're lightweight, reproducible and run everywhere. Iterations of software delivery lifecycles may be cut down to seconds from weeks with the right processes and tools. Container images are layered per change made when the container is built. Each layer has a cryptographic hash and the layer itself can be shared between multiple containers readonly. When a new container is started from an image, the container runtime creates a COW (copy-on-write) filesystem where the particular container data is stored. This is in turn very effective as you only need one copy of a layer on the host. For example, if a bunch of applications are based off a Ubuntu base image, the base image only needs to be stored once on the host.","title":"Containers Intro"},{"location":"learn/containers101/index.html#key_attributes_3","text":"These are some of the key elements of Containers. Runs on modern architectures and operating systems. Not necessarily as a single source image. Headless services (webservers, databases etc) in microservice architectures. Often orchestrated on compute clusters like Kubernetes, Apache Mesos Marathon or Docker Swarm. Software vendors often provide official and well tested container images for their applications.","title":"Key Attributes"},{"location":"learn/containers101/index.html#learning_resources_3","text":"Curated list of learning resources for Containers. Interactive: Play with Docker Great interactive tutorials where you learn how to build, ship and run containers. Also has a follow-on interactive training on Kubernetes. Tutorial: Docker for beginners Comprehensive introduction to get started with Docker all the way to running it on a PaaS. Cartoon: The Illustrated Children's Guide to Kubernetes Illustrative and easy to grasp story of what Kubernetes is. Bonus cartoon: A Kubernetes story: Phippy goes to the zoo A high production quality cartoon explaining Kubernetes API objects. Blog: How to choose the right container orchestration and how to deploy it A brief overview of container orchestrators. Standards: opencontainers.org Components of a container system is standards based. The Open Container Initiative is the standards body. Blog/reference: Demystifying container runtimes Discusses different container runtime engines.","title":"Learning Resources"},{"location":"learn/containers101/index.html#practical_exercises_3","text":"How to get hands-on experience of Containers. Install Docker Desktop or just Docker if using Linux. Click through the Get Started tutorial. Advanced: Run any of the images built in the tutorial on a public cloud service.","title":"Practical Exercises"},{"location":"learn/containers101/index.html#container_tooling","text":"Most of the tooling around containers is centered around what particular container orchestrator or development environment is being utilized. Usage of the tools differ greatly depending on the role of the user. As an operator the toolkit includes both IaaS and managing the platform to perform upgrades, user management and peripheral services such as storage and ingress load balancers. While many popular platforms today are based on Kubernetes, the tooling has nuances. Upstream Kubernetes uses kubectl , Red Hat OpenShift uses the OpenShift CLI, oc . With other platforms such as Rancher, nearly all management can be done through a web UI.","title":"Container Tooling"},{"location":"learn/containers101/index.html#key_attributes_4","text":"These are some of the key elements of Container Tooling. Most tools are simple, yet powerful and follow UNIX principles of doing one thing and doing it well. The docker and kubectl CLIs are the two most dominant for low level management. Workload management usually relies on external tools for simplicity, such as docker-compose , kompose and helm . Some platforms have ancillary tools to marry the IaaS with the cluster orchestrator. Such an example is rke for Rancher and gkectl for GKE On-Prem. The public clouds have builtin container orchestrator and container management into their native CLIs, such as aws and gcloud . Client side tools normally rely on environment variables and user environment configuration files that store credentials, API endpoint locations and other security aspects.","title":"Key Attributes"},{"location":"learn/containers101/index.html#learning_resources_4","text":"Curated list of learning resources for Container Tooling. Reference: Use the Docker command line Docker CLI reference. Reference: The kubectl Cheat Sheet The kubectl cheat sheet. Utility: kustomize.io Kubernetes native configuration managment. Tutorial: The Ultimate Guide to Podman, Skopeo and Buildah An alternative container toolchain to Docker using Podman, Buildah and Skopeo.","title":"Learning Resources"},{"location":"learn/containers101/index.html#practical_exercises_4","text":"How to get hands-on experience of Container Tooling. Install Docker Desktop or just Docker if using Linux. Build a container image of an application you understand (docker build). Run the container image locally (docker run). Ship it to Docker Hub (docker push). Create an Amazon EKS cluster or equivalent. Retrieve the kubeconfig file. Run kubectl get nodes on your local machine. Start a Pod using the container image built in previous exercise.","title":"Practical Exercises"},{"location":"learn/containers101/index.html#container_storage","text":"Due to the ephemeral nature of a container, storage is predominantly served from the host the container is running on and is dependent on which container runtime is being used where data is stored. In the case of Docker, the overlay filesystems are under /var/lib/docker . If a certain path inside the container need to persist between upgrades, restarts on a different host or any other operation that will lose the locality of the data, the mount point needs to be replaced with a \"bind\" mount from the host. There are also container runtime technologies that are designed to persist the entire container, effectively treating the container more like a long-lived Virtual Machine. Examples are Canonical LXD, WeaveWorks Footloose and HPE BlueData. This is particularly important for applications that rely on its projected node info to remain static throughout its entire lifecycle. We can then begin to categorize containers into three main categories based on their lifecycle vs persistence needs. Stateless Containers No persistence needed across restarts/upgrades/rollbacks Stateful Containers Require certain mountpoints to persist across restarts/upgrades/rollbacks Persistent Containers Require static node identity information across restarts/upgrades/rollbacks Some modern Software-defined Storage solutions are offered to run alongside applications in a distributed fashion. Effectively enforcing multi-way replicas for reliability and eat into CPU and memory resources of the IaaS bill. This also introduces the dilemma of effectively locking the data into the container orchestrator and its compute nodes. Although it's convenient for developers to become self-entitled storage administrators. To stay in control of the data and remain mobile, storing data outside of the container orchestrator is preferable. Many container orchestrators provide plugins for external storage, some are built-in and some are supplied and supported by the storage vendor. Public clouds provide storage drivers for their IaaS storage services directly to the container orchestrator. This is widely popular pattern we're also seeing in BYO IaaS solutions such as VMware vSphere.","title":"Container Storage"},{"location":"learn/containers101/index.html#key_attributes_5","text":"These are some of the key elements of Container Storage. Ephemeral storage needs to be fast and expandable as environments scale with more diverse applications. Data for stateful containers is ideally stored outside of the container orchestrator, either the IaaS or external highly-available storage. Persistent containers require niche storage solution tightly coupled with the container runtime and the container orchestrator or scheduler. Most storage solutions provide an \"access mode\" often referred to as ReadWriteOnce (RWO) which only allow one Pod (in the Kubernetes case or containers from the same host access the volume. To allow multiple Pods and containers from multiple hosts, a distributed filesystem or an NFS server (widely adopted) is required to provide ReadWriteMany (RWX) access.","title":"Key Attributes"},{"location":"learn/containers101/index.html#learning_resources_5","text":"Curated list of learning resources for Container Storage. Talk: Kubernetes Storage Lingo 101 A talk that lays out the nomenclature for storage in Kubernetes in an understandable way. Reference: Docker: Volumes Fundamental reference on how to make mount points persist for containers. Reference: Kubernetes: Volumes Using volumes in Kubernetes Pods. Podcast: Kubernetes Storage with Saad Ali Essential listen understand the difference between high-availability and automatic recovery.","title":"Learning Resources"},{"location":"learn/containers101/index.html#practical_exercises_5","text":"How to get hands-on experience of Container Storage. Use Docker Desktop. Replace a mount point in an interactive container with a mount point from the host Deploy a Amazon EKS or equivalent cluster. Create a Persistent Volume Claim. Run kubectl get pv -o yaml and match the Persistent Volume against the IaaS block volumes.","title":"Practical Exercises"},{"location":"learn/containers101/index.html#devops","text":"There are many interpretations of what DevOps \"is\". A bird's eye view is that there are people, processes and tools that come together to drive business outcomes through value streams. There are many core principles that could ultimately drive the outcome and no cookie cutter solution for any given organization. Breaking down problems into small pieces and creating safe systems to work in and eliminate toil are some of those principles. Agile development and lean manufacturing are both predecessors and role models for driving DevOps principles.","title":"DevOps"},{"location":"learn/containers101/index.html#key_attributes_6","text":"These are some of the key elements of DevOps. Well-defined processes, safe and proportionally sized work units for each step in a value stream. Autonomy through tooling backed by well-defined processes. Operations, development and stakeholders unified behind common goals. Continuous improvement, robust feedback loops and problem \"swarming\" of value streams. All work in the value stream must be visible and measurable. From CEO and down buy-in to prevent failure of DevOps implementations. DevOps is essential to be successful when investing in a \"digital transformation\".","title":"Key Attributes"},{"location":"learn/containers101/index.html#learning_resources_6","text":"Curated list of learning resources for DevOps. Opinions: Define DevOps: What is DevOps Industry voices defining what DevOps is and means. Blog: Toil: Finally a Name For a Problem We've All Felt Broad definition of toil. Hardbacks: DevOps Books Author Gene Kim has written novels and \"cookbooks\" of DevOps. Reference: The DevOps Institute Focuses on the human side of successfully implementing DevOps. Talk: Bank on Open Source for DevOps Success - Capital One How a disruptive company differentiate with DevOps at its core.","title":"Learning Resources"},{"location":"learn/containers101/index.html#practical_exercises_6","text":"How to get hands-on experience of DevOps. Getting practical with DevOps requires an organization and a value stream. Listen to the The Phonenix Project for a glimpse into how to implement DevOps.","title":"Practical Exercises"},{"location":"learn/containers101/index.html#devops_tooling","text":"The tools in DevOps are centered around the processes and value streams that support the business. Said tools also promote visibility, openness and collaboration. Inherently following security patterns, audit trails and safety. No one person should be able to misuse one tool to cause major disturbance in a value stream without quick remediation plans. Many times CI/CD (Continuous Integration, Continuous Delivery and/or Deployment) is considered synonymous with DevOps. That is both right and wrong. If the value stream inherently contains software, yes.","title":"DevOps Tooling"},{"location":"learn/containers101/index.html#key_attributes_7","text":"These are some of the key elements of DevOps Tooling. Just the right amount of privileges for a particular task. Issue/project tracking, kanban, source code control, CI/CD, logging and reporting are essential. Visibility and traceability is a key element, no work should be hidden. By person or machine.","title":"Key Attributes"},{"location":"learn/containers101/index.html#learning_resources_7","text":"Curated list of learning resources for DevOps Tooling. Reference: Periodic Table of DevOps Tools The most comprehensive chart of current and popular DevOps tools. Blog: Continuous integration vs. continuous delivery vs. continuous deployment Distinguish the components of CI/CD and what each facet encompass. Reference: Kanban Understanding Kanban helps understanding flow of work to adjust tools to work for humans, not against them.","title":"Learning Resources"},{"location":"learn/containers101/index.html#practical_exercises_7","text":"How to get hands-on experience of DevOps Tooling. Study some of the tools available to perform automated tasks on complex systems. Jenkins Rundeck Ansible Tower Morpheus Data Delphix The common denominator across these platforms is the observability and the ability to limit scope of controls through Role-based Access Control (RBAC). Ensuring the tasks are well-defined, automated, scoped and safe to operate.","title":"Practical Exercises"},{"location":"learn/containers101/index.html#devops_storage","text":"There aren't any particular storage paradigms (file/block/object) that are associated with DevOps. It's the implementation of the application and how it consumes storage that we vaguely may associate with DevOps. It's more of the practice that the right security controls are in place and whomever needs storage resource are fully self serviced. Human or machine.","title":"DevOps Storage"},{"location":"learn/containers101/index.html#key_attributes_8","text":"These are some of the key elements of DevOps Storage. API driven through RBAC. Ensuring automation may put in place for the endpoint or person that needs access to the resource. Rich data management. If a value stream only needs a low performing read-only view of a certain dataset, resources supporting the value stream should only have read-only access with performance constrains. Agile and mobile. At will, data should be made available for a certain application or resource for its purpose. Whether it's in the public cloud, on-prem or as-a-service through safe and secure automation.","title":"Key Attributes"},{"location":"learn/containers101/index.html#learning_resources_8","text":"Curated list of learning resources for DevOps Storage. Blog: Is Your Storage Too Slow for DevOps? Characterization of DevOps Storage attributes.","title":"Learning Resources"},{"location":"learn/containers101/index.html#practical_exercises_8","text":"How to get hands-on experience of DevOps Storage. Familiarize yourself with a storage system's RESTful API and automation capabilities. Deploy an Ansible Tower trial. Write an Ansible playbook that creates a storage resource on said system. Create a job in Ansible Tower with the playbook and make it available to a restricted user.","title":"Practical Exercises"},{"location":"learn/containers101/index.html#summary","text":"If you have any suggestions or comments, head over to GitHub and file a PR or leave an issue.","title":"Summary"},{"location":"learn/introduction_to_containers/index.html","text":"Interactive learning path \u00b6 The Storage Education team at HPE has put together an interactive learning path to introduce field engineers, architects and account executives to Docker and Kubernetes. The course material has an angle to help understand the role of storage in the world of containers. It's a great starting point if you're new to containers. Course 2-4 contains interactive labs in an immersive environment with downloadable lab guides that can be used outside of the lab environment. It's recommended to take the courses in order. Audience Course name Duration (estimated) 1 AE and SA Containers and market opportunity 20 minutes 2 AE and SA Introduction to containers 30 minutes 3 Technical AE and SA Introduction to Docker 45 minutes 4 Technical AE and SA Introduction to Kubernetes 45 minutes Important All courses require a HPE Passport account, either partner or employee.","title":"For HPE partners:<br />&nbsp;&nbsp; Introduction to Containers"},{"location":"learn/introduction_to_containers/index.html#interactive_learning_path","text":"The Storage Education team at HPE has put together an interactive learning path to introduce field engineers, architects and account executives to Docker and Kubernetes. The course material has an angle to help understand the role of storage in the world of containers. It's a great starting point if you're new to containers. Course 2-4 contains interactive labs in an immersive environment with downloadable lab guides that can be used outside of the lab environment. It's recommended to take the courses in order. Audience Course name Duration (estimated) 1 AE and SA Containers and market opportunity 20 minutes 2 AE and SA Introduction to containers 30 minutes 3 Technical AE and SA Introduction to Docker 45 minutes 4 Technical AE and SA Introduction to Kubernetes 45 minutes Important All courses require a HPE Passport account, either partner or employee.","title":"Interactive learning path"},{"location":"learn/persistent_storage/index.html","text":"Overview \u00b6 This is a free learning resource from HPE which walks you through various exercises to get you familiar with Kubernetes and provisioning Persistent storage using HPE Nimble Storage, HPE Primera or HPE 3PAR storage systems. This guide is by no means a comprehensive overview of the capabilities of Kubernetes but rather a getting started guide for individuals who wants to learn how to use Kubernetes with persistent storage. Overview Kubernetes 101 The basics Nodes Master Kubernetes Cluster Persistent Volumes Containers Pods Namespaces Deployments Services Lab 1: Tour your cluster Overview of kubectl Syntax Getting to know your cluster: Lab 2: Install K8s dashboard Deploying the Dashboard UI Accessing the Dashboard UI Create the Admin Service Account Create ClusterRoleBinding Get Token Lab 3: Deploy your first pod Lab 4: Install the CSI driver Installing the chart Creating a Secret Creating a StorageClass Creating a PersistentVolumeClaim Lab 5: Deploying Wordpress Kubernetes 101 \u00b6 The basics \u00b6 The first thing we need to do is to understand the various components of Kubernetes. Nodes \u00b6 The nodes in a Kubernetes cluster are the machines (VMs, physical servers, etc) that run your applications and cloud workflows. The Kubernetes master controls each node; you\u2019ll rarely interact with nodes directly. Master \u00b6 The Kubernetes master is responsible for maintaining the desired state of your cluster. When you interact with Kubernetes, such as by using the kubectl command-line interface, you\u2019re communicating with your cluster\u2019s Kubernetes master nodes. Note \"Master\u201d refers to a collection of processes managing the cluster state. Typically all these processes run on a single node within the cluster, and this node is also referred to as the master. The master can be replicated for availability and redundancy. Kubernetes Cluster \u00b6 In Kubernetes, nodes pool together their resources (memory and CPU) to distribute workloads. A cluster is comprised of a control plane, master and worker nodes, and physical machines that allow you to run your container workloads on. Persistent Volumes \u00b6 Because programs running on your cluster aren\u2019t guaranteed to run on a specific node, data can\u2019t be saved to any arbitrary place in the file system. If a program tries to save data to a file for later, but is then relocated onto a new node, the file will no longer be where the program expects it to be. To store data permanently, Kubernetes uses Persistent Volumes. Local, external storage via SAN arrays, or cloud drives can be attached to the cluster as a Persistent Volume. Kubernetes Objects Containers \u00b6 Programs running on Kubernetes are packaged as containers which can run on Linux or Windows. A container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Pods \u00b6 A Pod is the basic execution unit of a Kubernetes application\u2013the smallest and simplest unit in the Kubernetes object model that you create or deploy. A Pod encapsulates an application\u2019s container (or, in some cases, multiple containers), storage resources, a unique network IP, and options that govern how the container(s) should run. Namespaces \u00b6 Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces. Namespaces are intended for use in environments with many users spread across multiple teams, or projects. Namespaces are a way to divide cluster resources between multiple users. Deployments \u00b6 A Deployment provides declarative updates for Pods. You declare a desired state for your pods in your Deployment and Kubernetes will manage it for you automatically. Services \u00b6 A Kubernetes Service object defines a policy for external clients to access an application within a cluster. By default, Docker uses host-private networking, so containers can talk to other containers only if they are on the same machine. In order for Docker containers to communicate across nodes, there must be allocated ports on the machine\u2019s own IP address, which are then forwarded or proxied to the containers. Coordinating port allocations is very difficult to do at scale, and exposes users to cluster-level issues outside of their control. Kubernetes assumes that pods can communicate with other pods, regardless of which host they land on. Kubernetes gives every pod its own cluster-private IP address, through a Kubernetes Service object, so you do not need to explicitly create links between pods or map container ports to host ports. This means that containers within a Pod can all reach each other\u2019s ports on localhost, and all pods in a cluster can see each other without NAT. Lab 1: Tour your cluster \u00b6 All of this information presented here is taken from the official documentation found on kubernetes.io/docs . Overview of kubectl \u00b6 The Kubernetes command-line tool, kubectl , allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs. For a complete list of kubectl operations, see Overview of kubectl on kubernetes.io. For more information on how to install and setup kubectl on Linux, Windows or MacOS, see Install and Set Up kubectl on kubernetes.io. Syntax \u00b6 Use the following syntax to run kubectl commands from your terminal window: kubectl [command] [TYPE] [NAME] [flags] where command , TYPE , NAME , and flags are: command : Specifies the operation that you want to perform on one or more resources, for example create, get, describe, delete. TYPE : Specifies the resource type. Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms. For example, the following commands produce the same output: kubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1 NAME : Specifies the name of the resource. Names are case-sensitive. If the name is omitted, details for all resources are displayed, for example kubectl get pods . Kubernetes Cheat Sheet Find more available commands at Kubernetes Cheat Sheet on kubernetes.io. Getting to know your cluster: \u00b6 Let's run through some simple kubectl commands to get familiar with your cluster. In order to communicate with the Kubernetes cluster, kubectl looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag. To view your config file: kubectl config view Check that kubectl and the config file are properly configured by getting the cluster state. kubectl cluster-info If you see a URL response, kubectl is correctly configured to access your cluster. The output is similar to this: $ kubectl cluster-info Kubernetes master is running at https://10.90.200.11:6443 coredns is running at https://10.90.200.11:6443/api/v1/namespaces/kube-system/services/coredns:dns/proxy kubernetes-dashboard is running at https://10.90.200.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Now let's look at the nodes within our cluster. kubectl get nodes You should see output similar to below. As you can see, each node has a role master or as worker nodes (<none>). $ kubectl get nodes NAME STATUS ROLES AGE VERSION kube-g1-master1 Ready master 37d v1.18.2 kube-g1-node1 Ready <none> 37d v1.18.2 kube-g1-node2 Ready <none> 37d v1.18.2 You can list pods. kubectl get pods Quiz Did you see any pods listed when you ran kubectl get pods ? Why? If you don't see any pods listed, it is because there are no pods deployed within the default namespace. Now run, kubectl get pods --all-namespaces . Does it look any different? Pay attention to the first column, NAMESPACES . In our case, we are working in the default namespace. Depending on the type of application and your user access level, applications can be deployed within one or more namespaces. If you don't see the object (deployment, pod, services, etc) you are looking for, double-check the namespace it was deployed under and use the -n <namespace> flag to view objects in other namespaces. Now that you have familiarized yourself with your cluster, let's configure the Kubernetes dashboard. Lab 2: Install K8s dashboard \u00b6 Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred. Please refer to Kubernetes Web UI (Dashboard) on kubernetes.io. Deploying the Dashboard UI \u00b6 The Dashboard UI is not deployed by default. To deploy it, run the following command. kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml Accessing the Dashboard UI \u00b6 You can access Dashboard using kubectl from your desktop. kubectl proxy Open a web browser, copy the following URL to access the Dashboard. http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ You should see something similar to the following: Note The Dashboard UI can only be accessed from the machine where the command is executed. See kubectl proxy --help for more options. Create the Admin Service Account \u00b6 To protect your cluster data, Dashboard deploys with a minimal RBAC configuration by default. Currently, Dashboard only supports logging in with a Bearer Token. To create a token for this demo, we will create an admin user. Warning The admin user created in the tutorial will have administrative privileges and is for educational purposes only. Open a second terminal, if you don't have one open already. The below YAML declarations are meant to be created with kubectl create . Either copy the content to a file on the host where kubectl is being executed, or copy & paste into the terminal, like this: kubectl create -f- < paste the YAML > ^D (CTRL + D) Step by step: kubectl create -f- Press Enter . Copy the code below into the terminal. apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system Press Enter and Ctrl-D . Create ClusterRoleBinding \u00b6 Let's create the ClusterRoleBinding for the new admin-user. We will apply the cluster-admin role to the admin-user . kubectl create -f- Press Enter . Copy the code below into the terminal. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system Press Enter and Ctrl-D . Get Token \u00b6 Now we are ready to get the token from the admin-user in order to log into the dashboard. Run the following command: kubectl -n kube-system get secret | grep admin-user It will return something similar to: admin-user-token-n7jx9 . Now run. kubectl -n kube-system describe secret admin-user-token-n7jx9 Copy the token value. Name: admin-user-token-n7jx9 Namespace: kube-system Labels: <none> Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 7e9a4b56-e692-496a-8767-965076a282a4 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: <your token will be shown here> ca.crt: 1025 bytes Switch back over to your browser and paste the token into the dashboard and Click - Sign In . From here, you can see the health of your cluster as well as inspect various objects (Pods, StorageClass, Persistent Volume Claims) and manage the cluster resources. You should see something similar to the following: Lab 3: Deploy your first pod \u00b6 A pod is a collection of containers sharing a network and mount namespace and is the basic unit of deployment in Kubernetes. All containers in a pod are scheduled on the same node. Let's create a simple nginx webserver. kubectl create -f- Press Enter . Copy and paste the following: apiVersion: apps/v1 kind: Deployment metadata: labels: run: nginx name: first-nginx-pod spec: replicas: 1 selector: matchLabels: run: nginx-first-pod template: metadata: labels: run: nginx-first-pod spec: containers: - image: nginx name: nginx Press Enter and Ctrl-D . We can now see the pod running. kubectl get pods NAME READY STATUS RESTARTS AGE first-nginx-pod-5bb4787f8d-7ndj6 1/1 Running 0 6m39s We can inspect the pod further using the kubectl describe command: Name: first-nginx-pod-5bb4787f8d-7ndj6 Namespace: default Priority: 0 Node: kube-g1-node1/10.90.200.184 Start Time: Mon, 02 Mar 2020 17:09:20 -0600 Labels: pod-template-hash=5bb4787f8d run=nginx-first-pod Annotations: <none> Status: Running IP: 10.233.82.7 IPs: IP: 10.233.82.7 Controlled By: ReplicaSet/first-nginx-pod-5bb4787f8d Containers: nginx: Container ID: docker://a0938f10d28cb0395b0c2c324ef0c74ecdcdc63e556863c53ee7a88d56d Image: nginx Image ID: docker-pullable://nginx@sha256:380eb808e2a3b0a15037efefcabc5b4e03d666d03 Port: <none> Host Port: <none> State: Running Started: Mon, 20 Aug 2020 17:09:32 -0600 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-m2vbl (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-m2vbl: Type: Secret (a volume populated by a Secret) SecretName: default-token-m2vbl Optional: false QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled <unknown> default-scheduler Successfully assigned default/first-nginx-pod Normal Pulling 54s kubelet, kube-g1-node1 Pulling image \"nginx\" Normal Pulled 46s kubelet, kube-g1-node1 Successfully pulled image \"nginx\" Normal Created 44s kubelet, kube-g1-node1 Created container nginx Normal Started 43s kubelet, kube-g1-node1 Started container nginx Let's find the IP address of the pod. kubectl get pod first-nginx-pod-5bb4787f8d-7ndj6 -o=jsonpath='{.status.podIP}' The output should be similar to the following. $ kubectl get pod first-nginx-pod-5bb4787f8d-7ndj6 -o=jsonpath='{.status.podIP}' 10.233.82.7 This IP address (10.233.82.7) is only accessible from within the cluster, so let's use port-forward to expose the pod port temporarily outside the cluster. kubectl port-forward first-nginx-pod-5bb4787f8d-7ndj6 80:80 Forwarding from 127.0.0.1:80 -> 8080 Forwarding from [::1]:80 -> 8080 Note If you have something already running locally on port 80, modify the port-forward to an unused port (i.e. 5000:80). port-forward is meant for temporarily exposing an application outside of a Kubernetes cluster. For a more permanent solution, look into Ingress Controllers. Finally, we can open a browser and go to http://127.0.0.1 and should see the following. You have successfully deployed your first Kubernetes pod. With the pod running, we can log in and explore the pod. If you don't already, open another shell and run: kubectl exec -it <pod_name> /bin/bash You can explore the pod and run various commands. Some commands might not be available within the pod. Why would that be? root@first-nginx-pod-5bb4787f8d-7ndj6:/# df -h Filesystem Size Used Avail Use% Mounted on overlay 46G 8.0G 38G 18% / tmpfs 64M 0 64M 0% /dev tmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup /dev/mapper/centos-root 46G 8.0G 38G 18% /etc/hosts shm 64M 0 64M 0% /dev/shm tmpfs 1.9G 12K 1.9G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 1.9G 0 1.9G 0% /proc/acpi tmpfs 1.9G 0 1.9G 0% /proc/scsi tmpfs 1.9G 0 1.9G 0% /sys/firmware Or modify the webpage: echo Hello from Kubernetes Storage > /usr/share/nginx/html/index.html Once done, press Ctrl-D to exit the pod. Use Ctrl+C to exit the port-forwarding. Lab 4: Install the CSI driver \u00b6 To get started with the deployment, the HPE CSI Driver is deployed using industry standard means, either a Helm chart or an Operator. For this tutorial, we will be using Helm to the deploy the HPE CSI driver. The official Helm chart for the HPE CSI Driver for Kubernetes is hosted on Artifact Hub . There, you will find the configuration and installation instructions for the chart. Installing the chart \u00b6 To install the chart with the name hpe-csi, add the HPE CSI Driver for Kubernetes helm repo. helm repo add hpe https://hpe-storage.github.io/co-deployments helm repo update Install the latest chart: helm install hpe-csi hpe/hpe-csi-driver --namespace kube-system Wait a few minutes as the deployment finishes. Verify that everything is up and running correctly with the listing out the pods. kubectl get pods --all-namespaces -l 'app in (nimble-csp, primera3par-csp, hpe-csi-node, hpe-csi-controller)' The output is similar to this: $ kubectl get pods --all-namespaces -l 'app in (nimble-csp, primera3par-csp, hpe-csi-node, hpe-csi-controller)' NAMESPACE NAME READY STATUS RESTARTS AGE kube-system csp-service-5df8679cf7-m4jcw 1/1 Running 0 5m kube-system hpe-csi-controller-84d8569476-9pk74 5/5 Running 0 5m kube-system hpe-csi-node-qt74m 2/2 Running 0 5m kube-system primera3par-csp-66f775b555-sfmnp 1/1 Running 0 5m If all of the components show in Running state, then the HPE CSI driver for Kubernetes and the corresponding Container Storage Providers have been successfully deployed. Creating a Secret \u00b6 When the HPE CSI Driver is deployed using the Helm chart or Operator, a Secret needs to be created based upon the backend type ( nimble or primera3par ), backend IP, and credentials. This Secret is used by the CSI sidecars in the StorageClass to authenticate to a specific backend for CSI operations. In order to add a new Secret or manage access to multiple backends, additional Secrets will need to be created per backend. Secret Requirements Each Secret name must be unique. servicePort should be set to 8080 . Create a new Secret , specify the name, Namespace , backend username, backend password, and the backend IP address to be used by the CSP. kubectl create -f- Copy and paste the following: HPE Nimble Storage apiVersion: v1 kind: Secret metadata: name: custom-secret namespace: kube-system stringData: serviceName: nimble-csp-svc servicePort: \"8080\" backend: 192.168.1.2 username: admin data: # echo -n \"admin\" | base64 password: YWRtaW4= HPE Primera apiVersion: v1 kind: Secret metadata: name: custom-secret namespace: kube-system stringData: serviceName: primera3par-csp-svc servicePort: \"8080\" backend: 10.10.0.2 username: 3paradm data: # echo -n \"3pardata\" | base64 password: M3BhcmRhdGE= Press Enter and Ctrl-D . Now let's look at the available StorageClasses. You should now see the Secret in the \"kube-system\" Namespace : kubectl -n kube-system get secret/custom-secret NAME TYPE DATA AGE custom-secret Opaque 5 1m Creating a StorageClass \u00b6 Now we will create a StorageClass that will be used in the following exercises. A StorageClass specifies the provisioner to use (in our case the HPE CSI Driver) and the volume parameters (such as Protection Templates, Performance Policies, CPG, etc.) of the volume that we want to create and can be used to differentiate between storage levels and usages. This concept is sometimes called \u201cprofiles\u201d in other storage systems. A cluster can have multiple StorageClasses allowing users to create storage claims tailored for their specific application requirements. Create an hpe-standard StorageClass based upon the CSP deployed. This StorageClass example will use the custom-secret we created in the previous step, if you used a different name make sure to modify the StorageClass accordingly. kubectl create -f- Copy and paste the following: HPE Nimble Storage apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-standard annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/provisioner-secret-name: custom-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: custom-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: custom-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: custom-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/controller-expand-secret-name: custom-secret csi.storage.k8s.io/controller-expand-secret-namespace: kube-system performancePolicy: \"SQL Server\" description: \"Volume from HPE CSI Driver\" accessProtocol: iscsi limitIops: \"76800\" allowOverrides: description,limitIops,performancePolicy allowVolumeExpansion: true HPE 3PAR and Primera apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-standard annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: ext4 csi.storage.k8s.io/provisioner-secret-name: custom-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: custom-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: custom-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: custom-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/controller-expand-secret-name: custom-secret csi.storage.k8s.io/controller-expand-secret-namespace: kube-system cpg: SSD_r6 provisioning_type: tpvv accessProtocol: iscsi allowOverrides: cpg,provisioning_type allowVolumeExpansion: true Press Enter and Ctrl-D . Now let's look at the available StorageClasses. $ kubectl get sc NAME PROVISIONER AGE hpe-standard (default) csi.hpe.com 2m Note We set hpe-standard StorageClass as default using the annotation storageclass.kubernetes.io/is-default-class: \"true\" . To learn more about configuring a default StorageClass , see Default StorageClass on kubernetes.io. Creating a PersistentVolumeClaim \u00b6 With a StorageClass available, we can create a PVC to request an amount of storage for our application. kubectl create -f- Copy and paste the following: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 50Gi Press Enter and Ctrl-D . Note We can use storageClassName to override the default StorageClass with another available StorageClass . We can see the my-pvc PersistentVolumeClaim created. kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-pvc Bound pvc-70d5caf8-7558-40e6-a8b7-77dfcf8ddcd8 50Gi RWO hpe-standard 72m We can inspect the PVC further for additional information. kubectl describe pvc my-pvc The output is similar to this: $ kubectl describe pvc my-pvc Name: my-pvc Namespace: default StorageClass: hpe-standard Status: Bound Volume: pvc-70d5caf8-7558-40e6-a8b7-77dfcf8ddcd8 Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: csi.hpe.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 50Gi Access Modes: RWO VolumeMode: Filesystem Mounted By: <none> Events: <none> We can also inspect the volume in a similar manner. kubectl describe pv <volume_name> The output is similar to this: $ kubectl describe pv pvc-70d5caf8-7558-40e6-a8b7-77dfcf8ddcd8 Name: pvc-70d5caf8-7558-40e6-a8b7-77dfcf8ddcd8 Labels: <none> Annotations: pv.kubernetes.io/provisioned-by: csi.hpe.com Finalizers: [kubernetes.io/pv-protection] StorageClass: hpe-standard Status: Bound Claim: default/my-pvc Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 50Gi Node Affinity: <none> Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: csi.hpe.com VolumeHandle: 063aba3d50ec99d866000000000000000000000001 ReadOnly: false VolumeAttributes: accessProtocol=iscsi allowOverrides=description,limitIops,performancePolicy description=Volume from HPE CSI Driver fsType=xfs limitIops=76800 performancePolicy=SQL Server storage.kubernetes.io/csiProvisionerIdentity=1583271972595-8081-csi.hpe.com volumeAccessMode=mount Events: <none> With the describe command, you can see the volume parameters applied to the volume. Let's recap what we have learned. We created a default StorageClass for our volumes. We created a PVC that created a volume from the storageClass. We can use kubectl get to list the StorageClass , PVC and PV . We can use kubectl describe to get details on the StorageClass , PVC or PV At this point, we have validated the deployment of the HPE CSI Driver and are ready to deploy an application with persistent storage. Lab 5: Deploying Wordpress \u00b6 To begin, we will be using the hpe-standard StorageClass we created previously. If you don't have hpe-standard available, please refer to StorageClass for instructions on creating a StorageClass . Create a PersistentVolumeClaim for MariaDB for use by Wordpress. This object creates a PersistentVolume as defined, make sure to reference the correct .spec.storageClassName . kind: PersistentVolumeClaim apiVersion: v1 metadata: name: data-my-wordpress-mariadb-0 spec: accessModes: - ReadWriteOnce resources: requests: storage: 50Gi storageClassName: hpe-standard Next let's make another for the Wordpress application. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-wordpress spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi storageClassName: hpe-standard Let's again verify the PersistentVolume were created successfully. kubectl get pv NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-my-wordpress-mariadb-0 Bound pvc-1abdb7d7-374e-45b3-8fa1-534131ec7ec6 50Gi RWO hpe-standard 1m my-wordpress Bound pvc-ff6dc8fd-2b14-4726-b608-be8b27485603 20Gi RWO hpe-standard 1m The above output means that the HPE CSI Driver successfully provisioned a new volume based upon the hpe-standard StorageClass . The volume is not attached to any node yet. It will only be attached to a node once a scheduled workload requests the PersistentVolumeClaim . Now, let's use Helm to deploy Wordpress using the PVC created previously. When Wordpress is deployed, the volumes will be attached, formatted and mounted. The first step is to add the Wordpress chart. helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm search repo bitnami/wordpress NAME CHART VERSION APP VERSION DESCRIPTION bitnami/wordpress 9.2.1 5.4.0 Web publishing platform for building blogs and ... Deploy Wordpress by setting persistence.existingClaim=<existing_PVC> to the PVC my-wordpress created in the previous step. helm install my-wordpress bitnami/wordpress --version 9.2.1 --set service.type=ClusterIP,wordpressUsername=admin,wordpressPassword=adminpassword,mariadb.mariadbRootPassword=secretpassword,persistence.existingClaim=my-wordpress,allowEmptyPassword=false Check to verify that Wordpress and MariaDB were deployed and are in the Running state. This may take a few minutes. kubectl get pods NAME READY STATUS RESTARTS AGE my-wordpress-69b7976c85-9mfjv 1/1 Running 0 2m my-wordpress-mariadb-0 1/1 Running 0 2m Finally let's take a look at the Wordpress site. You can use kubectl port-forward to access the Wordpress application from within the Kubernetes cluster to verify everything is working correctly. kubectl port-forward svc/my-wordpress 80:80 Forwarding from 127.0.0.1:80 -> 8080 Forwarding from [::1]:80 -> 8080 Note If you have something already running locally on port 80, modify the port-forward to an unused port (i.e. 5000:80). Open a browser on your workstation to http://127.0.0.1 and you should see, \"Hello World!\" . Access the admin console at: http://127.0.0.1/admin using the \"admin/adminpassword\" used to deploy the Helm Chart. Happy Blogging! This completes the tutorial of using the HPE CSI Driver with HPE storage to create Persistent Volumes within Kubernetes. This is just the beginning of the capabilities of the HPE Storage integrations within Kubernetes. We recommend exploring SCOD further and the specific HPE Storage CSP ( Nimble , Primera, and 3PAR ) to learn more.","title":"Persistent Storage for Kubernetes"},{"location":"learn/persistent_storage/index.html#overview","text":"This is a free learning resource from HPE which walks you through various exercises to get you familiar with Kubernetes and provisioning Persistent storage using HPE Nimble Storage, HPE Primera or HPE 3PAR storage systems. This guide is by no means a comprehensive overview of the capabilities of Kubernetes but rather a getting started guide for individuals who wants to learn how to use Kubernetes with persistent storage. Overview Kubernetes 101 The basics Nodes Master Kubernetes Cluster Persistent Volumes Containers Pods Namespaces Deployments Services Lab 1: Tour your cluster Overview of kubectl Syntax Getting to know your cluster: Lab 2: Install K8s dashboard Deploying the Dashboard UI Accessing the Dashboard UI Create the Admin Service Account Create ClusterRoleBinding Get Token Lab 3: Deploy your first pod Lab 4: Install the CSI driver Installing the chart Creating a Secret Creating a StorageClass Creating a PersistentVolumeClaim Lab 5: Deploying Wordpress","title":"Overview"},{"location":"learn/persistent_storage/index.html#kubernetes_101","text":"","title":"Kubernetes 101"},{"location":"learn/persistent_storage/index.html#the_basics","text":"The first thing we need to do is to understand the various components of Kubernetes.","title":"The basics"},{"location":"learn/persistent_storage/index.html#nodes","text":"The nodes in a Kubernetes cluster are the machines (VMs, physical servers, etc) that run your applications and cloud workflows. The Kubernetes master controls each node; you\u2019ll rarely interact with nodes directly.","title":"Nodes"},{"location":"learn/persistent_storage/index.html#master","text":"The Kubernetes master is responsible for maintaining the desired state of your cluster. When you interact with Kubernetes, such as by using the kubectl command-line interface, you\u2019re communicating with your cluster\u2019s Kubernetes master nodes. Note \"Master\u201d refers to a collection of processes managing the cluster state. Typically all these processes run on a single node within the cluster, and this node is also referred to as the master. The master can be replicated for availability and redundancy.","title":"Master"},{"location":"learn/persistent_storage/index.html#kubernetes_cluster","text":"In Kubernetes, nodes pool together their resources (memory and CPU) to distribute workloads. A cluster is comprised of a control plane, master and worker nodes, and physical machines that allow you to run your container workloads on.","title":"Kubernetes Cluster"},{"location":"learn/persistent_storage/index.html#persistent_volumes","text":"Because programs running on your cluster aren\u2019t guaranteed to run on a specific node, data can\u2019t be saved to any arbitrary place in the file system. If a program tries to save data to a file for later, but is then relocated onto a new node, the file will no longer be where the program expects it to be. To store data permanently, Kubernetes uses Persistent Volumes. Local, external storage via SAN arrays, or cloud drives can be attached to the cluster as a Persistent Volume.","title":"Persistent Volumes"},{"location":"learn/persistent_storage/index.html#containers","text":"Programs running on Kubernetes are packaged as containers which can run on Linux or Windows. A container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.","title":"Containers"},{"location":"learn/persistent_storage/index.html#pods","text":"A Pod is the basic execution unit of a Kubernetes application\u2013the smallest and simplest unit in the Kubernetes object model that you create or deploy. A Pod encapsulates an application\u2019s container (or, in some cases, multiple containers), storage resources, a unique network IP, and options that govern how the container(s) should run.","title":"Pods"},{"location":"learn/persistent_storage/index.html#namespaces","text":"Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces. Namespaces are intended for use in environments with many users spread across multiple teams, or projects. Namespaces are a way to divide cluster resources between multiple users.","title":"Namespaces"},{"location":"learn/persistent_storage/index.html#deployments","text":"A Deployment provides declarative updates for Pods. You declare a desired state for your pods in your Deployment and Kubernetes will manage it for you automatically.","title":"Deployments"},{"location":"learn/persistent_storage/index.html#services","text":"A Kubernetes Service object defines a policy for external clients to access an application within a cluster. By default, Docker uses host-private networking, so containers can talk to other containers only if they are on the same machine. In order for Docker containers to communicate across nodes, there must be allocated ports on the machine\u2019s own IP address, which are then forwarded or proxied to the containers. Coordinating port allocations is very difficult to do at scale, and exposes users to cluster-level issues outside of their control. Kubernetes assumes that pods can communicate with other pods, regardless of which host they land on. Kubernetes gives every pod its own cluster-private IP address, through a Kubernetes Service object, so you do not need to explicitly create links between pods or map container ports to host ports. This means that containers within a Pod can all reach each other\u2019s ports on localhost, and all pods in a cluster can see each other without NAT.","title":"Services"},{"location":"learn/persistent_storage/index.html#lab_1_tour_your_cluster","text":"All of this information presented here is taken from the official documentation found on kubernetes.io/docs .","title":"Lab 1: Tour your cluster"},{"location":"learn/persistent_storage/index.html#overview_of_kubectl","text":"The Kubernetes command-line tool, kubectl , allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs. For a complete list of kubectl operations, see Overview of kubectl on kubernetes.io. For more information on how to install and setup kubectl on Linux, Windows or MacOS, see Install and Set Up kubectl on kubernetes.io.","title":"Overview of kubectl"},{"location":"learn/persistent_storage/index.html#syntax","text":"Use the following syntax to run kubectl commands from your terminal window: kubectl [command] [TYPE] [NAME] [flags] where command , TYPE , NAME , and flags are: command : Specifies the operation that you want to perform on one or more resources, for example create, get, describe, delete. TYPE : Specifies the resource type. Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms. For example, the following commands produce the same output: kubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1 NAME : Specifies the name of the resource. Names are case-sensitive. If the name is omitted, details for all resources are displayed, for example kubectl get pods . Kubernetes Cheat Sheet Find more available commands at Kubernetes Cheat Sheet on kubernetes.io.","title":"Syntax"},{"location":"learn/persistent_storage/index.html#getting_to_know_your_cluster","text":"Let's run through some simple kubectl commands to get familiar with your cluster. In order to communicate with the Kubernetes cluster, kubectl looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag. To view your config file: kubectl config view Check that kubectl and the config file are properly configured by getting the cluster state. kubectl cluster-info If you see a URL response, kubectl is correctly configured to access your cluster. The output is similar to this: $ kubectl cluster-info Kubernetes master is running at https://10.90.200.11:6443 coredns is running at https://10.90.200.11:6443/api/v1/namespaces/kube-system/services/coredns:dns/proxy kubernetes-dashboard is running at https://10.90.200.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Now let's look at the nodes within our cluster. kubectl get nodes You should see output similar to below. As you can see, each node has a role master or as worker nodes (<none>). $ kubectl get nodes NAME STATUS ROLES AGE VERSION kube-g1-master1 Ready master 37d v1.18.2 kube-g1-node1 Ready <none> 37d v1.18.2 kube-g1-node2 Ready <none> 37d v1.18.2 You can list pods. kubectl get pods Quiz Did you see any pods listed when you ran kubectl get pods ? Why? If you don't see any pods listed, it is because there are no pods deployed within the default namespace. Now run, kubectl get pods --all-namespaces . Does it look any different? Pay attention to the first column, NAMESPACES . In our case, we are working in the default namespace. Depending on the type of application and your user access level, applications can be deployed within one or more namespaces. If you don't see the object (deployment, pod, services, etc) you are looking for, double-check the namespace it was deployed under and use the -n <namespace> flag to view objects in other namespaces. Now that you have familiarized yourself with your cluster, let's configure the Kubernetes dashboard.","title":"Getting to know your cluster:"},{"location":"learn/persistent_storage/index.html#lab_2_install_k8s_dashboard","text":"Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred. Please refer to Kubernetes Web UI (Dashboard) on kubernetes.io.","title":"Lab 2: Install K8s dashboard"},{"location":"learn/persistent_storage/index.html#deploying_the_dashboard_ui","text":"The Dashboard UI is not deployed by default. To deploy it, run the following command. kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml","title":"Deploying the Dashboard UI"},{"location":"learn/persistent_storage/index.html#accessing_the_dashboard_ui","text":"You can access Dashboard using kubectl from your desktop. kubectl proxy Open a web browser, copy the following URL to access the Dashboard. http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ You should see something similar to the following: Note The Dashboard UI can only be accessed from the machine where the command is executed. See kubectl proxy --help for more options.","title":"Accessing the Dashboard UI"},{"location":"learn/persistent_storage/index.html#create_the_admin_service_account","text":"To protect your cluster data, Dashboard deploys with a minimal RBAC configuration by default. Currently, Dashboard only supports logging in with a Bearer Token. To create a token for this demo, we will create an admin user. Warning The admin user created in the tutorial will have administrative privileges and is for educational purposes only. Open a second terminal, if you don't have one open already. The below YAML declarations are meant to be created with kubectl create . Either copy the content to a file on the host where kubectl is being executed, or copy & paste into the terminal, like this: kubectl create -f- < paste the YAML > ^D (CTRL + D) Step by step: kubectl create -f- Press Enter . Copy the code below into the terminal. apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system Press Enter and Ctrl-D .","title":"Create the Admin Service Account"},{"location":"learn/persistent_storage/index.html#create_clusterrolebinding","text":"Let's create the ClusterRoleBinding for the new admin-user. We will apply the cluster-admin role to the admin-user . kubectl create -f- Press Enter . Copy the code below into the terminal. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system Press Enter and Ctrl-D .","title":"Create ClusterRoleBinding"},{"location":"learn/persistent_storage/index.html#get_token","text":"Now we are ready to get the token from the admin-user in order to log into the dashboard. Run the following command: kubectl -n kube-system get secret | grep admin-user It will return something similar to: admin-user-token-n7jx9 . Now run. kubectl -n kube-system describe secret admin-user-token-n7jx9 Copy the token value. Name: admin-user-token-n7jx9 Namespace: kube-system Labels: <none> Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 7e9a4b56-e692-496a-8767-965076a282a4 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: <your token will be shown here> ca.crt: 1025 bytes Switch back over to your browser and paste the token into the dashboard and Click - Sign In . From here, you can see the health of your cluster as well as inspect various objects (Pods, StorageClass, Persistent Volume Claims) and manage the cluster resources. You should see something similar to the following:","title":"Get Token"},{"location":"learn/persistent_storage/index.html#lab_3_deploy_your_first_pod","text":"A pod is a collection of containers sharing a network and mount namespace and is the basic unit of deployment in Kubernetes. All containers in a pod are scheduled on the same node. Let's create a simple nginx webserver. kubectl create -f- Press Enter . Copy and paste the following: apiVersion: apps/v1 kind: Deployment metadata: labels: run: nginx name: first-nginx-pod spec: replicas: 1 selector: matchLabels: run: nginx-first-pod template: metadata: labels: run: nginx-first-pod spec: containers: - image: nginx name: nginx Press Enter and Ctrl-D . We can now see the pod running. kubectl get pods NAME READY STATUS RESTARTS AGE first-nginx-pod-5bb4787f8d-7ndj6 1/1 Running 0 6m39s We can inspect the pod further using the kubectl describe command: Name: first-nginx-pod-5bb4787f8d-7ndj6 Namespace: default Priority: 0 Node: kube-g1-node1/10.90.200.184 Start Time: Mon, 02 Mar 2020 17:09:20 -0600 Labels: pod-template-hash=5bb4787f8d run=nginx-first-pod Annotations: <none> Status: Running IP: 10.233.82.7 IPs: IP: 10.233.82.7 Controlled By: ReplicaSet/first-nginx-pod-5bb4787f8d Containers: nginx: Container ID: docker://a0938f10d28cb0395b0c2c324ef0c74ecdcdc63e556863c53ee7a88d56d Image: nginx Image ID: docker-pullable://nginx@sha256:380eb808e2a3b0a15037efefcabc5b4e03d666d03 Port: <none> Host Port: <none> State: Running Started: Mon, 20 Aug 2020 17:09:32 -0600 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-m2vbl (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-m2vbl: Type: Secret (a volume populated by a Secret) SecretName: default-token-m2vbl Optional: false QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled <unknown> default-scheduler Successfully assigned default/first-nginx-pod Normal Pulling 54s kubelet, kube-g1-node1 Pulling image \"nginx\" Normal Pulled 46s kubelet, kube-g1-node1 Successfully pulled image \"nginx\" Normal Created 44s kubelet, kube-g1-node1 Created container nginx Normal Started 43s kubelet, kube-g1-node1 Started container nginx Let's find the IP address of the pod. kubectl get pod first-nginx-pod-5bb4787f8d-7ndj6 -o=jsonpath='{.status.podIP}' The output should be similar to the following. $ kubectl get pod first-nginx-pod-5bb4787f8d-7ndj6 -o=jsonpath='{.status.podIP}' 10.233.82.7 This IP address (10.233.82.7) is only accessible from within the cluster, so let's use port-forward to expose the pod port temporarily outside the cluster. kubectl port-forward first-nginx-pod-5bb4787f8d-7ndj6 80:80 Forwarding from 127.0.0.1:80 -> 8080 Forwarding from [::1]:80 -> 8080 Note If you have something already running locally on port 80, modify the port-forward to an unused port (i.e. 5000:80). port-forward is meant for temporarily exposing an application outside of a Kubernetes cluster. For a more permanent solution, look into Ingress Controllers. Finally, we can open a browser and go to http://127.0.0.1 and should see the following. You have successfully deployed your first Kubernetes pod. With the pod running, we can log in and explore the pod. If you don't already, open another shell and run: kubectl exec -it <pod_name> /bin/bash You can explore the pod and run various commands. Some commands might not be available within the pod. Why would that be? root@first-nginx-pod-5bb4787f8d-7ndj6:/# df -h Filesystem Size Used Avail Use% Mounted on overlay 46G 8.0G 38G 18% / tmpfs 64M 0 64M 0% /dev tmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup /dev/mapper/centos-root 46G 8.0G 38G 18% /etc/hosts shm 64M 0 64M 0% /dev/shm tmpfs 1.9G 12K 1.9G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 1.9G 0 1.9G 0% /proc/acpi tmpfs 1.9G 0 1.9G 0% /proc/scsi tmpfs 1.9G 0 1.9G 0% /sys/firmware Or modify the webpage: echo Hello from Kubernetes Storage > /usr/share/nginx/html/index.html Once done, press Ctrl-D to exit the pod. Use Ctrl+C to exit the port-forwarding.","title":"Lab 3: Deploy your first pod"},{"location":"learn/persistent_storage/index.html#lab_4_install_the_csi_driver","text":"To get started with the deployment, the HPE CSI Driver is deployed using industry standard means, either a Helm chart or an Operator. For this tutorial, we will be using Helm to the deploy the HPE CSI driver. The official Helm chart for the HPE CSI Driver for Kubernetes is hosted on Artifact Hub . There, you will find the configuration and installation instructions for the chart.","title":"Lab 4: Install the CSI driver"},{"location":"learn/persistent_storage/index.html#installing_the_chart","text":"To install the chart with the name hpe-csi, add the HPE CSI Driver for Kubernetes helm repo. helm repo add hpe https://hpe-storage.github.io/co-deployments helm repo update Install the latest chart: helm install hpe-csi hpe/hpe-csi-driver --namespace kube-system Wait a few minutes as the deployment finishes. Verify that everything is up and running correctly with the listing out the pods. kubectl get pods --all-namespaces -l 'app in (nimble-csp, primera3par-csp, hpe-csi-node, hpe-csi-controller)' The output is similar to this: $ kubectl get pods --all-namespaces -l 'app in (nimble-csp, primera3par-csp, hpe-csi-node, hpe-csi-controller)' NAMESPACE NAME READY STATUS RESTARTS AGE kube-system csp-service-5df8679cf7-m4jcw 1/1 Running 0 5m kube-system hpe-csi-controller-84d8569476-9pk74 5/5 Running 0 5m kube-system hpe-csi-node-qt74m 2/2 Running 0 5m kube-system primera3par-csp-66f775b555-sfmnp 1/1 Running 0 5m If all of the components show in Running state, then the HPE CSI driver for Kubernetes and the corresponding Container Storage Providers have been successfully deployed.","title":"Installing the chart"},{"location":"learn/persistent_storage/index.html#creating_a_secret","text":"When the HPE CSI Driver is deployed using the Helm chart or Operator, a Secret needs to be created based upon the backend type ( nimble or primera3par ), backend IP, and credentials. This Secret is used by the CSI sidecars in the StorageClass to authenticate to a specific backend for CSI operations. In order to add a new Secret or manage access to multiple backends, additional Secrets will need to be created per backend. Secret Requirements Each Secret name must be unique. servicePort should be set to 8080 . Create a new Secret , specify the name, Namespace , backend username, backend password, and the backend IP address to be used by the CSP. kubectl create -f- Copy and paste the following: HPE Nimble Storage apiVersion: v1 kind: Secret metadata: name: custom-secret namespace: kube-system stringData: serviceName: nimble-csp-svc servicePort: \"8080\" backend: 192.168.1.2 username: admin data: # echo -n \"admin\" | base64 password: YWRtaW4= HPE Primera apiVersion: v1 kind: Secret metadata: name: custom-secret namespace: kube-system stringData: serviceName: primera3par-csp-svc servicePort: \"8080\" backend: 10.10.0.2 username: 3paradm data: # echo -n \"3pardata\" | base64 password: M3BhcmRhdGE= Press Enter and Ctrl-D . Now let's look at the available StorageClasses. You should now see the Secret in the \"kube-system\" Namespace : kubectl -n kube-system get secret/custom-secret NAME TYPE DATA AGE custom-secret Opaque 5 1m","title":"Creating a Secret"},{"location":"learn/persistent_storage/index.html#creating_a_storageclass","text":"Now we will create a StorageClass that will be used in the following exercises. A StorageClass specifies the provisioner to use (in our case the HPE CSI Driver) and the volume parameters (such as Protection Templates, Performance Policies, CPG, etc.) of the volume that we want to create and can be used to differentiate between storage levels and usages. This concept is sometimes called \u201cprofiles\u201d in other storage systems. A cluster can have multiple StorageClasses allowing users to create storage claims tailored for their specific application requirements. Create an hpe-standard StorageClass based upon the CSP deployed. This StorageClass example will use the custom-secret we created in the previous step, if you used a different name make sure to modify the StorageClass accordingly. kubectl create -f- Copy and paste the following: HPE Nimble Storage apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-standard annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: xfs csi.storage.k8s.io/provisioner-secret-name: custom-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: custom-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: custom-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: custom-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/controller-expand-secret-name: custom-secret csi.storage.k8s.io/controller-expand-secret-namespace: kube-system performancePolicy: \"SQL Server\" description: \"Volume from HPE CSI Driver\" accessProtocol: iscsi limitIops: \"76800\" allowOverrides: description,limitIops,performancePolicy allowVolumeExpansion: true HPE 3PAR and Primera apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: hpe-standard annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: csi.hpe.com parameters: csi.storage.k8s.io/fstype: ext4 csi.storage.k8s.io/provisioner-secret-name: custom-secret csi.storage.k8s.io/provisioner-secret-namespace: kube-system csi.storage.k8s.io/controller-publish-secret-name: custom-secret csi.storage.k8s.io/controller-publish-secret-namespace: kube-system csi.storage.k8s.io/node-stage-secret-name: custom-secret csi.storage.k8s.io/node-stage-secret-namespace: kube-system csi.storage.k8s.io/node-publish-secret-name: custom-secret csi.storage.k8s.io/node-publish-secret-namespace: kube-system csi.storage.k8s.io/controller-expand-secret-name: custom-secret csi.storage.k8s.io/controller-expand-secret-namespace: kube-system cpg: SSD_r6 provisioning_type: tpvv accessProtocol: iscsi allowOverrides: cpg,provisioning_type allowVolumeExpansion: true Press Enter and Ctrl-D . Now let's look at the available StorageClasses. $ kubectl get sc NAME PROVISIONER AGE hpe-standard (default) csi.hpe.com 2m Note We set hpe-standard StorageClass as default using the annotation storageclass.kubernetes.io/is-default-class: \"true\" . To learn more about configuring a default StorageClass , see Default StorageClass on kubernetes.io.","title":"Creating a StorageClass"},{"location":"learn/persistent_storage/index.html#creating_a_persistentvolumeclaim","text":"With a StorageClass available, we can create a PVC to request an amount of storage for our application. kubectl create -f- Copy and paste the following: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 50Gi Press Enter and Ctrl-D . Note We can use storageClassName to override the default StorageClass with another available StorageClass . We can see the my-pvc PersistentVolumeClaim created. kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-pvc Bound pvc-70d5caf8-7558-40e6-a8b7-77dfcf8ddcd8 50Gi RWO hpe-standard 72m We can inspect the PVC further for additional information. kubectl describe pvc my-pvc The output is similar to this: $ kubectl describe pvc my-pvc Name: my-pvc Namespace: default StorageClass: hpe-standard Status: Bound Volume: pvc-70d5caf8-7558-40e6-a8b7-77dfcf8ddcd8 Labels: <none> Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes volume.beta.kubernetes.io/storage-provisioner: csi.hpe.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 50Gi Access Modes: RWO VolumeMode: Filesystem Mounted By: <none> Events: <none> We can also inspect the volume in a similar manner. kubectl describe pv <volume_name> The output is similar to this: $ kubectl describe pv pvc-70d5caf8-7558-40e6-a8b7-77dfcf8ddcd8 Name: pvc-70d5caf8-7558-40e6-a8b7-77dfcf8ddcd8 Labels: <none> Annotations: pv.kubernetes.io/provisioned-by: csi.hpe.com Finalizers: [kubernetes.io/pv-protection] StorageClass: hpe-standard Status: Bound Claim: default/my-pvc Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 50Gi Node Affinity: <none> Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: csi.hpe.com VolumeHandle: 063aba3d50ec99d866000000000000000000000001 ReadOnly: false VolumeAttributes: accessProtocol=iscsi allowOverrides=description,limitIops,performancePolicy description=Volume from HPE CSI Driver fsType=xfs limitIops=76800 performancePolicy=SQL Server storage.kubernetes.io/csiProvisionerIdentity=1583271972595-8081-csi.hpe.com volumeAccessMode=mount Events: <none> With the describe command, you can see the volume parameters applied to the volume. Let's recap what we have learned. We created a default StorageClass for our volumes. We created a PVC that created a volume from the storageClass. We can use kubectl get to list the StorageClass , PVC and PV . We can use kubectl describe to get details on the StorageClass , PVC or PV At this point, we have validated the deployment of the HPE CSI Driver and are ready to deploy an application with persistent storage.","title":"Creating a PersistentVolumeClaim"},{"location":"learn/persistent_storage/index.html#lab_5_deploying_wordpress","text":"To begin, we will be using the hpe-standard StorageClass we created previously. If you don't have hpe-standard available, please refer to StorageClass for instructions on creating a StorageClass . Create a PersistentVolumeClaim for MariaDB for use by Wordpress. This object creates a PersistentVolume as defined, make sure to reference the correct .spec.storageClassName . kind: PersistentVolumeClaim apiVersion: v1 metadata: name: data-my-wordpress-mariadb-0 spec: accessModes: - ReadWriteOnce resources: requests: storage: 50Gi storageClassName: hpe-standard Next let's make another for the Wordpress application. apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-wordpress spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi storageClassName: hpe-standard Let's again verify the PersistentVolume were created successfully. kubectl get pv NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-my-wordpress-mariadb-0 Bound pvc-1abdb7d7-374e-45b3-8fa1-534131ec7ec6 50Gi RWO hpe-standard 1m my-wordpress Bound pvc-ff6dc8fd-2b14-4726-b608-be8b27485603 20Gi RWO hpe-standard 1m The above output means that the HPE CSI Driver successfully provisioned a new volume based upon the hpe-standard StorageClass . The volume is not attached to any node yet. It will only be attached to a node once a scheduled workload requests the PersistentVolumeClaim . Now, let's use Helm to deploy Wordpress using the PVC created previously. When Wordpress is deployed, the volumes will be attached, formatted and mounted. The first step is to add the Wordpress chart. helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm search repo bitnami/wordpress NAME CHART VERSION APP VERSION DESCRIPTION bitnami/wordpress 9.2.1 5.4.0 Web publishing platform for building blogs and ... Deploy Wordpress by setting persistence.existingClaim=<existing_PVC> to the PVC my-wordpress created in the previous step. helm install my-wordpress bitnami/wordpress --version 9.2.1 --set service.type=ClusterIP,wordpressUsername=admin,wordpressPassword=adminpassword,mariadb.mariadbRootPassword=secretpassword,persistence.existingClaim=my-wordpress,allowEmptyPassword=false Check to verify that Wordpress and MariaDB were deployed and are in the Running state. This may take a few minutes. kubectl get pods NAME READY STATUS RESTARTS AGE my-wordpress-69b7976c85-9mfjv 1/1 Running 0 2m my-wordpress-mariadb-0 1/1 Running 0 2m Finally let's take a look at the Wordpress site. You can use kubectl port-forward to access the Wordpress application from within the Kubernetes cluster to verify everything is working correctly. kubectl port-forward svc/my-wordpress 80:80 Forwarding from 127.0.0.1:80 -> 8080 Forwarding from [::1]:80 -> 8080 Note If you have something already running locally on port 80, modify the port-forward to an unused port (i.e. 5000:80). Open a browser on your workstation to http://127.0.0.1 and you should see, \"Hello World!\" . Access the admin console at: http://127.0.0.1/admin using the \"admin/adminpassword\" used to deploy the Helm Chart. Happy Blogging! This completes the tutorial of using the HPE CSI Driver with HPE storage to create Persistent Volumes within Kubernetes. This is just the beginning of the capabilities of the HPE Storage integrations within Kubernetes. We recommend exploring SCOD further and the specific HPE Storage CSP ( Nimble , Primera, and 3PAR ) to learn more.","title":"Lab 5: Deploying Wordpress"},{"location":"learn/video_gallery/index.html","text":"Overview \u00b6 Welcome to the Video Gallery. This is a collection of current YouTube assets that pertains to supported HPE primary storage container technologies. Overview CSI driver management Managing multiple HPE primary storage backends using the HPE CSI Driver Container Storage Providers Using the HPE CSI Driver with HPE Primera Using the HPE CSI Driver with HPE Nimble Storage Provisioning Dynamic Provisioning of Persistent Storage on Kubernetes HPE DEV Hack Shack Workshop: Using the Container Storage Interface Using the HPE CSI Driver to create CSI snapshots and clones Partner Ecosystems Install the HPE CSI Operator for Kubernetes on Red Hat OpenShift Watch more CSI driver management \u00b6 How to manage the components that surrounds driver deployment. Managing multiple HPE primary storage backends using the HPE CSI Driver \u00b6 This tutorial talks about managing multiple Secrets and StorageClasses to distinguish different backends. Watch on YouTube Container Storage Providers \u00b6 Each CSP has its own features and perks, learn about the different platforms right here. Using the HPE CSI Driver with HPE Primera \u00b6 This tutorial showcases a few of the HPE Primera specific features with the HPE CSI Driver. Watch on YouTube Using the HPE CSI Driver with HPE Nimble Storage \u00b6 This tutorial showcases a few of the HPE Nimble Storage specific features with the HPE CSI Driver. Watch on YouTube Provisioning \u00b6 The provisioning topic covers provisioning of storage resources on container orchestrators, such as volumes, snapshots and clones. Dynamic Provisioning of Persistent Storage on Kubernetes \u00b6 Learn the fundamentals of storage provisioning on Kubernetes. Watch on YouTube HPE DEV Hack Shack Workshop: Using the Container Storage Interface \u00b6 An interactive CSI workshop from HPE Discover Virtual Experience. It explains key provisioning concepts, including CSI snapshots and clones, ephemeral inline volumes, raw block volumes and how to use the NFS server provisioner. Watch on Vimeo Using the HPE CSI Driver to create CSI snapshots and clones \u00b6 Learn how to use CSI snapshots and clones with the HPE CSI Driver. Watch on YouTube Partner Ecosystems \u00b6 Joint solutions with our revered ecosystem partners. Install the HPE CSI Operator for Kubernetes on Red Hat OpenShift \u00b6 This tutorial goes through the steps of installing the HPE CSI Operator on Red Hat OpenShift. Watch on YouTube Watch more \u00b6 A curated playlist of content related to HPE primary storage and containers is available on YouTube .","title":"Video Gallery"},{"location":"learn/video_gallery/index.html#overview","text":"Welcome to the Video Gallery. This is a collection of current YouTube assets that pertains to supported HPE primary storage container technologies. Overview CSI driver management Managing multiple HPE primary storage backends using the HPE CSI Driver Container Storage Providers Using the HPE CSI Driver with HPE Primera Using the HPE CSI Driver with HPE Nimble Storage Provisioning Dynamic Provisioning of Persistent Storage on Kubernetes HPE DEV Hack Shack Workshop: Using the Container Storage Interface Using the HPE CSI Driver to create CSI snapshots and clones Partner Ecosystems Install the HPE CSI Operator for Kubernetes on Red Hat OpenShift Watch more","title":"Overview"},{"location":"learn/video_gallery/index.html#csi_driver_management","text":"How to manage the components that surrounds driver deployment.","title":"CSI driver management"},{"location":"learn/video_gallery/index.html#managing_multiple_hpe_primary_storage_backends_using_the_hpe_csi_driver","text":"This tutorial talks about managing multiple Secrets and StorageClasses to distinguish different backends. Watch on YouTube","title":"Managing multiple HPE primary storage backends using the HPE CSI Driver"},{"location":"learn/video_gallery/index.html#container_storage_providers","text":"Each CSP has its own features and perks, learn about the different platforms right here.","title":"Container Storage Providers"},{"location":"learn/video_gallery/index.html#using_the_hpe_csi_driver_with_hpe_primera","text":"This tutorial showcases a few of the HPE Primera specific features with the HPE CSI Driver. Watch on YouTube","title":"Using the HPE CSI Driver with HPE Primera"},{"location":"learn/video_gallery/index.html#using_the_hpe_csi_driver_with_hpe_nimble_storage","text":"This tutorial showcases a few of the HPE Nimble Storage specific features with the HPE CSI Driver. Watch on YouTube","title":"Using the HPE CSI Driver with HPE Nimble Storage"},{"location":"learn/video_gallery/index.html#provisioning","text":"The provisioning topic covers provisioning of storage resources on container orchestrators, such as volumes, snapshots and clones.","title":"Provisioning"},{"location":"learn/video_gallery/index.html#dynamic_provisioning_of_persistent_storage_on_kubernetes","text":"Learn the fundamentals of storage provisioning on Kubernetes. Watch on YouTube","title":"Dynamic Provisioning of Persistent Storage on Kubernetes"},{"location":"learn/video_gallery/index.html#hpe_dev_hack_shack_workshop_using_the_container_storage_interface","text":"An interactive CSI workshop from HPE Discover Virtual Experience. It explains key provisioning concepts, including CSI snapshots and clones, ephemeral inline volumes, raw block volumes and how to use the NFS server provisioner. Watch on Vimeo","title":"HPE DEV Hack Shack Workshop: Using the Container Storage Interface"},{"location":"learn/video_gallery/index.html#using_the_hpe_csi_driver_to_create_csi_snapshots_and_clones","text":"Learn how to use CSI snapshots and clones with the HPE CSI Driver. Watch on YouTube","title":"Using the HPE CSI Driver to create CSI snapshots and clones"},{"location":"learn/video_gallery/index.html#partner_ecosystems","text":"Joint solutions with our revered ecosystem partners.","title":"Partner Ecosystems"},{"location":"learn/video_gallery/index.html#install_the_hpe_csi_operator_for_kubernetes_on_red_hat_openshift","text":"This tutorial goes through the steps of installing the HPE CSI Operator on Red Hat OpenShift. Watch on YouTube","title":"Install the HPE CSI Operator for Kubernetes on Red Hat OpenShift"},{"location":"learn/video_gallery/index.html#watch_more","text":"A curated playlist of content related to HPE primary storage and containers is available on YouTube .","title":"Watch more"},{"location":"legal/contributing/index.html","text":"Introduction \u00b6 We welcome and encourage community contributions to SCOD. Where to start? \u00b6 The best way to directly collaborate with the project contributors is through GitHub: https://github.com/hpe-storage/scod If you want to contribute to our documentation by either fixing a typo or creating a page, please open a GitHub pull request . If you want to raise an issue such as a defect, an enhancement request or a general issue, please open a GitHub issue . Before you start writing, we recommend discussing your plans through a GitHub issue, especially for more ambitious contributions. This gives other contributors a chance to point you in the right direction, give you feedback on your contribution, and help you find out if someone else is working on the same thing. Note that all submissions from all contributors get reviewed. After a pull request is made, other contributors will offer feedback. If the patch passes review, a maintainer will accept it with a comment. When a pull request fails review, the author is expected to update the pull request to address the issue until it passes review and the pull request merges successfully. At least one review from a maintainer is required for all patches. Developer's Certificate of Origin \u00b6 All contributions must include acceptance of the DCO: Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 660 York Street, Suite 102, San Francisco, CA 94110 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. Sign your work \u00b6 To accept the DCO, simply add this line to each commit message with your name and email address ( git commit -s will do this for you): Signed-off-by: Jane Example <jane@example.com> For legal reasons, no anonymous or pseudonymous contributions are accepted. Submitting Pull Requests \u00b6 We encourage and support contributions from the community. No fix is too small. We strive to process all pull requests as soon as possible and with constructive feedback. If your pull request is not accepted at first, please try again after addressing the feedback you received. To make a pull request you will need a GitHub account. For help, see GitHub's documentation on forking and pull requests.","title":"Contributing"},{"location":"legal/contributing/index.html#introduction","text":"We welcome and encourage community contributions to SCOD.","title":"Introduction"},{"location":"legal/contributing/index.html#where_to_start","text":"The best way to directly collaborate with the project contributors is through GitHub: https://github.com/hpe-storage/scod If you want to contribute to our documentation by either fixing a typo or creating a page, please open a GitHub pull request . If you want to raise an issue such as a defect, an enhancement request or a general issue, please open a GitHub issue . Before you start writing, we recommend discussing your plans through a GitHub issue, especially for more ambitious contributions. This gives other contributors a chance to point you in the right direction, give you feedback on your contribution, and help you find out if someone else is working on the same thing. Note that all submissions from all contributors get reviewed. After a pull request is made, other contributors will offer feedback. If the patch passes review, a maintainer will accept it with a comment. When a pull request fails review, the author is expected to update the pull request to address the issue until it passes review and the pull request merges successfully. At least one review from a maintainer is required for all patches.","title":"Where to start?"},{"location":"legal/contributing/index.html#developers_certificate_of_origin","text":"All contributions must include acceptance of the DCO: Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 660 York Street, Suite 102, San Francisco, CA 94110 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.","title":"Developer's Certificate of Origin"},{"location":"legal/contributing/index.html#sign_your_work","text":"To accept the DCO, simply add this line to each commit message with your name and email address ( git commit -s will do this for you): Signed-off-by: Jane Example <jane@example.com> For legal reasons, no anonymous or pseudonymous contributions are accepted.","title":"Sign your work"},{"location":"legal/contributing/index.html#submitting_pull_requests","text":"We encourage and support contributions from the community. No fix is too small. We strive to process all pull requests as soon as possible and with constructive feedback. If your pull request is not accepted at first, please try again after addressing the feedback you received. To make a pull request you will need a GitHub account. For help, see GitHub's documentation on forking and pull requests.","title":"Submitting Pull Requests"},{"location":"legal/license/index.html","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"legal/support/index.html","text":"Statement \u00b6 Software components documented on SCOD are generally covered with valid support contract on the HPE product being used. Terms and conditions may be found in the support contract. Please reach out to your official HPE representative or HPE partner for any uncertainties. HPE 3PAR and Primera Container Storage Provider support \u00b6 Limited to the HPE 3PAR and Primera Container Storage Provider only, best effort support is available for HPE 3PAR and HPE Primera controllers supported by the HPE 3PAR and Primera Container Storage Provider and bundled with All Inclusive software with an active HPE Pointnext support agreement. Since the HPE Pointnext support for the HPE 3PAR and Primera Container Storage Provider is best effort only, any other support levels like Warranty, Foundation Care, Proactive Care, Proactive Care Advanced and Datacenter Care or other support levels do not apply to the HPE 3PAR and Primera Container Storage Provider. Best effort response times are based on local standard business days and working hours. If your location is outside the customary service zone, response time may be longer. 3PAR/Primera Hardware Contract Type Phone Number Warranty and Foundation Care 800-633-3600 Proactive Care (PC) 866-211-5211 Datacenter Care (DC) 888-751-2149","title":"Support"},{"location":"legal/support/index.html#statement","text":"Software components documented on SCOD are generally covered with valid support contract on the HPE product being used. Terms and conditions may be found in the support contract. Please reach out to your official HPE representative or HPE partner for any uncertainties.","title":"Statement"},{"location":"legal/support/index.html#hpe_3par_and_primera_container_storage_provider_support","text":"Limited to the HPE 3PAR and Primera Container Storage Provider only, best effort support is available for HPE 3PAR and HPE Primera controllers supported by the HPE 3PAR and Primera Container Storage Provider and bundled with All Inclusive software with an active HPE Pointnext support agreement. Since the HPE Pointnext support for the HPE 3PAR and Primera Container Storage Provider is best effort only, any other support levels like Warranty, Foundation Care, Proactive Care, Proactive Care Advanced and Datacenter Care or other support levels do not apply to the HPE 3PAR and Primera Container Storage Provider. Best effort response times are based on local standard business days and working hours. If your location is outside the customary service zone, response time may be longer. 3PAR/Primera Hardware Contract Type Phone Number Warranty and Foundation Care 800-633-3600 Proactive Care (PC) 866-211-5211 Datacenter Care (DC) 888-751-2149","title":"HPE 3PAR and Primera Container Storage Provider support"},{"location":"partners/index.html","text":"Partner Ecosystems \u00b6 Rancher Labs Red Hat OpenShift","title":"Partner Ecosystems"},{"location":"partners/index.html#partner_ecosystems","text":"Rancher Labs Red Hat OpenShift","title":"Partner Ecosystems"},{"location":"partners/rancher_labs/index.html","text":"Overview \u00b6 Rancher Labs provides a platform to deploy Kubernetes-as-a-service everywhere. HPE partners with Rancher Labs to provide effortless management of the CSI and FlexVolume driver on managed Kubernetes clusters. This allows our joint customers and channel partners to enable hybrid cloud stateful workloads on Kubernetes. Overview Deployment considerations Rancher 2.4 HPE CSI Driver for Kubernetes Rancher CLI install Web UI install Post install steps HPE Volume Driver for Kubernetes FlexVolume plugin Deployment considerations \u00b6 Rancher is capable of managing Kubernetes across a broad spectrum of managed and BYO clusters. It's important to understand that the HPE CSI Driver for Kubernetes does not support the same amount of combinations Rancher does. Consult the support matrix on the CSI driver overview page for the supported combinations of the HPE CSI Driver, Kubernetes and supported node Operating Systems. Rancher 2.4 \u00b6 Rancher uses Helm to deploy and manage partner software. The concept of a Helm repository in Rancher is organized as a \"catalog\". The HPE CSI Driver for Kubernetes and the HPE Volume Driver for Kubernetes FlexVolume plugin are both partner solutions present in the official Rancher Catalog. Tip Learn more about Catalogs, Helm Charts and Apps in the Rancher documentation . HPE CSI Driver for Kubernetes \u00b6 The HPE CSI Driver is part of the official Helm v3 library in Rancher. The CSI driver is deployed on managed Kubernetes clusters like any ordinary \"App\" in Rancher. You may use either the Rancher CLI or the web UI to deploy the CSI driver. Rancher CLI install \u00b6 Switch to the project you want to install the CSI driver. For this example, the default project on a managed cluster is being used. $ rancher context current Cluster:torta Project:Default Steps to install the CSI driver. $ rancher app install hpe-csi-driver hpe-csi-driver --no-prompt run \"app show-notes hpe-csi-driver\" to view app notes once app is ready $ rancher app ID NAME STATE CATALOG TEMPLATE VERSION p-k28xd:hpe-csi-driver hpe-csi-driver active helm3-library hpe-csi-driver 1.3.1 Note This is installs the driver with the default parameters which is the most common deployment option. Please see the official Helm chart documentation for supported parameters. Web UI install \u00b6 The web UI install is straight forward. Illustrated here for completeness. Login to RKE and click the cluster tab Select which managed cluster and project the driver should be installed to Select \"Apps\" Select \"Launch\" Pick \"hpe-csi-driver\" Scroll past the manifest unless any parameters needs changing, hit \"Launch\" After a few moments the driver should be installed. The number of workloads depends on the cluster node count. Note This is installs the driver with the default parameters which is the most common deployment option. Please see the official Helm chart documentation for supported parameters. Post install steps \u00b6 For Rancher Apps to make use of persistent storage from HPE, a supported backend needs to be configured. This procedure is generic regardless of Kubernetes distribution being used. Go ahead and add a HPE storage backend HPE Volume Driver for Kubernetes FlexVolume plugin \u00b6 Only use the FlexVolume driver for Kubernetes 1.12 and below or with HPE Cloud Volumes up to Kubernetes 1.17. The FlexVolume driver is provided as a Helm v2 chart in the official Rancher Catalog. Parameters are very specific to the environment to where the driver is being installed to. Please follow the steps in the FlexVolume Helm chart documentation for further guidance. Also understand that the FlexVolume driver only supports HPE Nimble Storage and HPE Cloud Volumes. Caution The FlexVolume driver is being deprecated. Reach out to your HPE representative if you think deploying the FlexVolume driver on your Rancher managed Kubernetes cluster is the correct course of action.","title":"Rancher Labs"},{"location":"partners/rancher_labs/index.html#overview","text":"Rancher Labs provides a platform to deploy Kubernetes-as-a-service everywhere. HPE partners with Rancher Labs to provide effortless management of the CSI and FlexVolume driver on managed Kubernetes clusters. This allows our joint customers and channel partners to enable hybrid cloud stateful workloads on Kubernetes. Overview Deployment considerations Rancher 2.4 HPE CSI Driver for Kubernetes Rancher CLI install Web UI install Post install steps HPE Volume Driver for Kubernetes FlexVolume plugin","title":"Overview"},{"location":"partners/rancher_labs/index.html#deployment_considerations","text":"Rancher is capable of managing Kubernetes across a broad spectrum of managed and BYO clusters. It's important to understand that the HPE CSI Driver for Kubernetes does not support the same amount of combinations Rancher does. Consult the support matrix on the CSI driver overview page for the supported combinations of the HPE CSI Driver, Kubernetes and supported node Operating Systems.","title":"Deployment considerations"},{"location":"partners/rancher_labs/index.html#rancher_24","text":"Rancher uses Helm to deploy and manage partner software. The concept of a Helm repository in Rancher is organized as a \"catalog\". The HPE CSI Driver for Kubernetes and the HPE Volume Driver for Kubernetes FlexVolume plugin are both partner solutions present in the official Rancher Catalog. Tip Learn more about Catalogs, Helm Charts and Apps in the Rancher documentation .","title":"Rancher 2.4"},{"location":"partners/rancher_labs/index.html#hpe_csi_driver_for_kubernetes","text":"The HPE CSI Driver is part of the official Helm v3 library in Rancher. The CSI driver is deployed on managed Kubernetes clusters like any ordinary \"App\" in Rancher. You may use either the Rancher CLI or the web UI to deploy the CSI driver.","title":"HPE CSI Driver for Kubernetes"},{"location":"partners/rancher_labs/index.html#rancher_cli_install","text":"Switch to the project you want to install the CSI driver. For this example, the default project on a managed cluster is being used. $ rancher context current Cluster:torta Project:Default Steps to install the CSI driver. $ rancher app install hpe-csi-driver hpe-csi-driver --no-prompt run \"app show-notes hpe-csi-driver\" to view app notes once app is ready $ rancher app ID NAME STATE CATALOG TEMPLATE VERSION p-k28xd:hpe-csi-driver hpe-csi-driver active helm3-library hpe-csi-driver 1.3.1 Note This is installs the driver with the default parameters which is the most common deployment option. Please see the official Helm chart documentation for supported parameters.","title":"Rancher CLI install"},{"location":"partners/rancher_labs/index.html#web_ui_install","text":"The web UI install is straight forward. Illustrated here for completeness. Login to RKE and click the cluster tab Select which managed cluster and project the driver should be installed to Select \"Apps\" Select \"Launch\" Pick \"hpe-csi-driver\" Scroll past the manifest unless any parameters needs changing, hit \"Launch\" After a few moments the driver should be installed. The number of workloads depends on the cluster node count. Note This is installs the driver with the default parameters which is the most common deployment option. Please see the official Helm chart documentation for supported parameters.","title":"Web UI install"},{"location":"partners/rancher_labs/index.html#post_install_steps","text":"For Rancher Apps to make use of persistent storage from HPE, a supported backend needs to be configured. This procedure is generic regardless of Kubernetes distribution being used. Go ahead and add a HPE storage backend","title":"Post install steps"},{"location":"partners/rancher_labs/index.html#hpe_volume_driver_for_kubernetes_flexvolume_plugin","text":"Only use the FlexVolume driver for Kubernetes 1.12 and below or with HPE Cloud Volumes up to Kubernetes 1.17. The FlexVolume driver is provided as a Helm v2 chart in the official Rancher Catalog. Parameters are very specific to the environment to where the driver is being installed to. Please follow the steps in the FlexVolume Helm chart documentation for further guidance. Also understand that the FlexVolume driver only supports HPE Nimble Storage and HPE Cloud Volumes. Caution The FlexVolume driver is being deprecated. Reach out to your HPE representative if you think deploying the FlexVolume driver on your Rancher managed Kubernetes cluster is the correct course of action.","title":"HPE Volume Driver for Kubernetes FlexVolume plugin"},{"location":"partners/redhat_openshift/index.html","text":"Overview \u00b6 HPE and Red Hat have a long standing partnership to provide jointly supported software, platform and services with the absolute best customer experience in the industry. Red Hat OpenShift uses open source Kubernetes and various other components to deliver a PaaS experience that benefits both developers and operations. This packaged experience differs slightly on how you would deploy and use the HPE volume drivers and this page serves as the authoritative source for all things HPE primary storage and Red Hat OpenShift. Overview OpenShift 4 Certified combinations Deployment Prerequisites Caveats OpenShift CLI OpenShift web console Additional information OpenShift 3 OpenShift 4 \u00b6 Software deployed on OpenShift 4 follows the Operator pattern . CSI drivers are no exception. Certified combinations \u00b6 Software delivered through the HPE and Red Hat partnership follows a rigorous certification process and only qualify what's listed in the following table. Status Red Hat OpenShift HPE CSI Operator Container Storage Providers Certified 4.2 1.2.0 Nimble, 3PAR and Primera Certified 4.3 1.2.0 Nimble, 3PAR and Primera Certified 4.3, 4.4 1.3.0 Nimble, 3PAR and Primera Check this table periodically for future releases. Pointers Other combinations may work but will not be supported. Both Red Hat Enterprise Linux and Red Hat CoreOS worker nodes are supported. Deployment \u00b6 The HPE CSI Operator for Kubernetes needs to be installed through the interfaces provided by Red Hat. Do not follow the instructions found on OperatorHub.io. Tip There's a tutorial available on YouTube accessible throught the Video Gallery on how to install and use the HPE CSI Operator on Red Hat OpenShift. Prerequisites \u00b6 The HPE CSI Driver needs to run in privileged mode and needs access to host ports, host network and should be able to mount hostPath volumes. Hence, before deploying HPE CSI Operator on OpenShift, please create the following SecurityContextConstraints (SCC) to allow the CSI driver to be running with these privileges. Download the SCC to where you have access to oc and the OpenShift cluster: curl -sL https://raw.githubusercontent.com/hpe-storage/co-deployments/master/operators/hpe-csi-operator/deploy/scc.yaml > hpe-csi-scc.yaml Change my-hpe-csi-operator to the name of the project (e.g. hpe-csi-driver below) where the CSI Operator is being deployed. oc new-project hpe-csi-driver --display-name=\"HPE CSI Driver for Kubernetes\" sed -i 's/my-hpe-csi-operator/hpe-csi-driver/g' hpe-csi-scc.yaml Deploy the SCC: oc create -f hpe-csi-scc.yaml securitycontextconstraints.security.openshift.io/hpe-csi-scc created Important Make note of the project name as it's needed for the Operator deployment in the next steps. Caveats \u00b6 At this time of writing (CSI Operator 1.2.0) the default StorageClass being shipped with the CSI driver is not very useful for OpenShift as it doesn't allow applications to write in the PersistentVolumes . Make sure to deploy a new StorageClass with .parameters.fsMode set to \"0770\" . This caveat will be removed in subsequent releases. Learn how to create a base StorageClass in using the CSI driver . OpenShift CLI \u00b6 This provides an example Operator deployment using oc . If you want to use the web console, proceed to the next section . It's assumed the SCC has been applied to the project and have kube:admin privileges. As an example, we'll deploy to the hpe-csi-driver project as described in previous steps. First, an OperatorGroup needs to be created. apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: hpe-csi-driver-for-kubernetes namespace: hpe-csi-driver spec: targetNamespaces: - hpe-csi-driver Next, create a Subscription to the Operator. apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: hpe-csi-operator namespace: hpe-csi-driver spec: channel: stable name: hpe-csi-operator source: certified-operators sourceNamespace: openshift-marketplace The Operator will now be installed on the OpenShift cluster. Before instantiating a CSI driver, watch the roll-out of the Operator. oc rollout status deploy/hpe-csi-operator -n hpe-csi-driver Waiting for deployment \"hpe-csi-operator\" rollout to finish: 0 of 1 updated replicas are available... deployment \"hpe-csi-operator\" successfully rolled out The next step is to create a HPECSIDriver object. It's unique per backend CSP. HPE Nimble Storage apiVersion: storage.hpe.com/v1 kind: HPECSIDriver metadata: name: csi-driver namespace: hpe-csi-driver spec: backendType: nimble imagePullPolicy: Always logLevel: info secret: backend: 192.168.1.1 create: true password: admin servicePort: '8080' username: admin storageClass: allowVolumeExpansion: true create: true defaultClass: false name: hpe-standard parameters: accessProtocol: iscsi fsType: xfs volumeDescription: Volume created by the HPE CSI Driver for Kubernetes HPE 3PAR and Primera apiVersion: storage.hpe.com/v1 kind: HPECSIDriver metadata: name: csi-driver namespace: hpe-csi-driver spec: backendType: primera3par imagePullPolicy: Always logLevel: info secret: backend: 10.10.10.1 create: true password: 3pardata servicePort: '8080' username: 3paradm storageClass: allowVolumeExpansion: true create: true defaultClass: false name: hpe-standard parameters: accessProtocol: iscsi fsType: xfs volumeDescription: Volume created by the HPE CSI Driver for Kubernetes Note As noted in the caveats , the installed StorageClass is not very useful for OpenShift. Create a new base StorageClass by following the steps in using the CSI driver . OpenShift web console \u00b6 Once the SCC has been applied to the project, login to the OpenShift web console as kube:admin and navigate to Operators -> OperatorHub . Search for 'HPE' in the search field. Select the HPE CSI Operator and click 'Install'. In the next pane, click 'Subscribe'. The HPE CSI Operator is now installed. Click the HPE CSI Operator, in the next pane, click 'Create Instance'. Configure the instance with the desired values ! . Values The required parameters are .spec.backendType , .spec.secret.backend and the credentials for the backend ( .spec.secret.username and .spec.secret.password ). By navigating to the Developer view, it should now be possible to inspect the CSI driver and Operator topology. Note As noted in the caveats , the installed StorageClass is not very useful for OpenShift. Create a new base StorageClass by following the steps in using the CSI driver . Additional information \u00b6 At this point the CSI driver is managed like any other Operator on Kubernetes and the life-cycle management capabilities may be explored further in the official Red Hat OpenShift documentation . OpenShift 3 \u00b6 Customers still using OpenShift 3 may use any of the legacy FlexVolume drivers for managing persistent storage.","title":"Red Hat OpenShift"},{"location":"partners/redhat_openshift/index.html#overview","text":"HPE and Red Hat have a long standing partnership to provide jointly supported software, platform and services with the absolute best customer experience in the industry. Red Hat OpenShift uses open source Kubernetes and various other components to deliver a PaaS experience that benefits both developers and operations. This packaged experience differs slightly on how you would deploy and use the HPE volume drivers and this page serves as the authoritative source for all things HPE primary storage and Red Hat OpenShift. Overview OpenShift 4 Certified combinations Deployment Prerequisites Caveats OpenShift CLI OpenShift web console Additional information OpenShift 3","title":"Overview"},{"location":"partners/redhat_openshift/index.html#openshift_4","text":"Software deployed on OpenShift 4 follows the Operator pattern . CSI drivers are no exception.","title":"OpenShift 4"},{"location":"partners/redhat_openshift/index.html#certified_combinations","text":"Software delivered through the HPE and Red Hat partnership follows a rigorous certification process and only qualify what's listed in the following table. Status Red Hat OpenShift HPE CSI Operator Container Storage Providers Certified 4.2 1.2.0 Nimble, 3PAR and Primera Certified 4.3 1.2.0 Nimble, 3PAR and Primera Certified 4.3, 4.4 1.3.0 Nimble, 3PAR and Primera Check this table periodically for future releases. Pointers Other combinations may work but will not be supported. Both Red Hat Enterprise Linux and Red Hat CoreOS worker nodes are supported.","title":"Certified combinations"},{"location":"partners/redhat_openshift/index.html#deployment","text":"The HPE CSI Operator for Kubernetes needs to be installed through the interfaces provided by Red Hat. Do not follow the instructions found on OperatorHub.io. Tip There's a tutorial available on YouTube accessible throught the Video Gallery on how to install and use the HPE CSI Operator on Red Hat OpenShift.","title":"Deployment"},{"location":"partners/redhat_openshift/index.html#prerequisites","text":"The HPE CSI Driver needs to run in privileged mode and needs access to host ports, host network and should be able to mount hostPath volumes. Hence, before deploying HPE CSI Operator on OpenShift, please create the following SecurityContextConstraints (SCC) to allow the CSI driver to be running with these privileges. Download the SCC to where you have access to oc and the OpenShift cluster: curl -sL https://raw.githubusercontent.com/hpe-storage/co-deployments/master/operators/hpe-csi-operator/deploy/scc.yaml > hpe-csi-scc.yaml Change my-hpe-csi-operator to the name of the project (e.g. hpe-csi-driver below) where the CSI Operator is being deployed. oc new-project hpe-csi-driver --display-name=\"HPE CSI Driver for Kubernetes\" sed -i 's/my-hpe-csi-operator/hpe-csi-driver/g' hpe-csi-scc.yaml Deploy the SCC: oc create -f hpe-csi-scc.yaml securitycontextconstraints.security.openshift.io/hpe-csi-scc created Important Make note of the project name as it's needed for the Operator deployment in the next steps.","title":"Prerequisites"},{"location":"partners/redhat_openshift/index.html#caveats","text":"At this time of writing (CSI Operator 1.2.0) the default StorageClass being shipped with the CSI driver is not very useful for OpenShift as it doesn't allow applications to write in the PersistentVolumes . Make sure to deploy a new StorageClass with .parameters.fsMode set to \"0770\" . This caveat will be removed in subsequent releases. Learn how to create a base StorageClass in using the CSI driver .","title":"Caveats"},{"location":"partners/redhat_openshift/index.html#openshift_cli","text":"This provides an example Operator deployment using oc . If you want to use the web console, proceed to the next section . It's assumed the SCC has been applied to the project and have kube:admin privileges. As an example, we'll deploy to the hpe-csi-driver project as described in previous steps. First, an OperatorGroup needs to be created. apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: hpe-csi-driver-for-kubernetes namespace: hpe-csi-driver spec: targetNamespaces: - hpe-csi-driver Next, create a Subscription to the Operator. apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: hpe-csi-operator namespace: hpe-csi-driver spec: channel: stable name: hpe-csi-operator source: certified-operators sourceNamespace: openshift-marketplace The Operator will now be installed on the OpenShift cluster. Before instantiating a CSI driver, watch the roll-out of the Operator. oc rollout status deploy/hpe-csi-operator -n hpe-csi-driver Waiting for deployment \"hpe-csi-operator\" rollout to finish: 0 of 1 updated replicas are available... deployment \"hpe-csi-operator\" successfully rolled out The next step is to create a HPECSIDriver object. It's unique per backend CSP. HPE Nimble Storage apiVersion: storage.hpe.com/v1 kind: HPECSIDriver metadata: name: csi-driver namespace: hpe-csi-driver spec: backendType: nimble imagePullPolicy: Always logLevel: info secret: backend: 192.168.1.1 create: true password: admin servicePort: '8080' username: admin storageClass: allowVolumeExpansion: true create: true defaultClass: false name: hpe-standard parameters: accessProtocol: iscsi fsType: xfs volumeDescription: Volume created by the HPE CSI Driver for Kubernetes HPE 3PAR and Primera apiVersion: storage.hpe.com/v1 kind: HPECSIDriver metadata: name: csi-driver namespace: hpe-csi-driver spec: backendType: primera3par imagePullPolicy: Always logLevel: info secret: backend: 10.10.10.1 create: true password: 3pardata servicePort: '8080' username: 3paradm storageClass: allowVolumeExpansion: true create: true defaultClass: false name: hpe-standard parameters: accessProtocol: iscsi fsType: xfs volumeDescription: Volume created by the HPE CSI Driver for Kubernetes Note As noted in the caveats , the installed StorageClass is not very useful for OpenShift. Create a new base StorageClass by following the steps in using the CSI driver .","title":"OpenShift CLI"},{"location":"partners/redhat_openshift/index.html#openshift_web_console","text":"Once the SCC has been applied to the project, login to the OpenShift web console as kube:admin and navigate to Operators -> OperatorHub . Search for 'HPE' in the search field. Select the HPE CSI Operator and click 'Install'. In the next pane, click 'Subscribe'. The HPE CSI Operator is now installed. Click the HPE CSI Operator, in the next pane, click 'Create Instance'. Configure the instance with the desired values ! . Values The required parameters are .spec.backendType , .spec.secret.backend and the credentials for the backend ( .spec.secret.username and .spec.secret.password ). By navigating to the Developer view, it should now be possible to inspect the CSI driver and Operator topology. Note As noted in the caveats , the installed StorageClass is not very useful for OpenShift. Create a new base StorageClass by following the steps in using the CSI driver .","title":"OpenShift web console"},{"location":"partners/redhat_openshift/index.html#additional_information","text":"At this point the CSI driver is managed like any other Operator on Kubernetes and the life-cycle management capabilities may be explored further in the official Red Hat OpenShift documentation .","title":"Additional information"},{"location":"partners/redhat_openshift/index.html#openshift_3","text":"Customers still using OpenShift 3 may use any of the legacy FlexVolume drivers for managing persistent storage.","title":"OpenShift 3"},{"location":"welcome/index.html","text":"Choose your platform \u00b6 HPE provides a broad portfolio of products that integrate with Docker, Kubernetes and neighboring ecosystems. The following table provides an overview of integrations available for each primary storage platform. Ecosystem HPE 3PAR / Primera HPE Nimble Storage HPE Cloud Volumes K8s 1.13+ CSI driver CSI driver FlexVolume driver k8s < 1.13 FlexVolume driver FlexVolume driver FlexVolume driver Docker Docker Volume Plugin Docker Volume Plugin Docker Volume Plugin Help me choose \u00b6 Interested in acquiring a persistent storage solution for your Kubernetes or Docker project? Criteria HPE 3PAR and Primera HPE Nimble Storage HPE Cloud Volumes Availability 100% 99.9999% Cloud dependent SLA/SLO Workloads Tier0 Tier1 and general purpose Tier2, backups, DR Market Large Enterprise Enterprise/SMB Flexible Cloud Private Private and Hybrid Public Learn more hpe.com/storage/primera hpe.com/storage/3par hpe.com/storage/nimble cloudvolumes.hpe.com Other HPE storage platforms \u00b6 Can't find what you're looking for? Check out hpe.com/storage for additional HPE storage platforms.","title":"Get started!"},{"location":"welcome/index.html#choose_your_platform","text":"HPE provides a broad portfolio of products that integrate with Docker, Kubernetes and neighboring ecosystems. The following table provides an overview of integrations available for each primary storage platform. Ecosystem HPE 3PAR / Primera HPE Nimble Storage HPE Cloud Volumes K8s 1.13+ CSI driver CSI driver FlexVolume driver k8s < 1.13 FlexVolume driver FlexVolume driver FlexVolume driver Docker Docker Volume Plugin Docker Volume Plugin Docker Volume Plugin","title":"Choose your platform"},{"location":"welcome/index.html#help_me_choose","text":"Interested in acquiring a persistent storage solution for your Kubernetes or Docker project? Criteria HPE 3PAR and Primera HPE Nimble Storage HPE Cloud Volumes Availability 100% 99.9999% Cloud dependent SLA/SLO Workloads Tier0 Tier1 and general purpose Tier2, backups, DR Market Large Enterprise Enterprise/SMB Flexible Cloud Private Private and Hybrid Public Learn more hpe.com/storage/primera hpe.com/storage/3par hpe.com/storage/nimble cloudvolumes.hpe.com","title":"Help me choose"},{"location":"welcome/index.html#other_hpe_storage_platforms","text":"Can't find what you're looking for? Check out hpe.com/storage for additional HPE storage platforms.","title":"Other HPE storage platforms"}]}